shared_params:
  results_dir: "results/awa2_incomplete/"
  trials: 5
  model_selection_trials: 3
  dataset_config:
    dataset: "awa2"
    num_workers: 8
    num_load_workers: 4
    batch_size: 512
    root_dir: "/path/to/awa2/" # REPLACE ME
    sampling_percent: 0.1
    sampling_groups: false
    test_subsampling: 1
    weight_loss: false
  intervention_config:
    competence_levels: [1]
    intervention_freq: 1
    intervention_batch_size: 1024
    val_intervention_policies:
      - policy: "random"
        group_level: true
        use_prior: false
    intervention_policies:
      - policy: "random"
        group_level: true
        use_prior: false
      # Remove the following if you are only interested in random interventions
      - policy: "random"
        group_level: true
        use_prior: true
        include_run_names: [".*(IntCEM).*"]
      - policy: "coop"
        group_level: true
        use_prior: False
      - policy: "behavioural_cloning"
        group_level: true
        use_prior: False
      - policy: "optimal_greedy"
        group_level: true
        use_prior: False
      - policy: "global_val_error"
        group_level: true
        use_prior: False
      - policy: "global_val_improvement"
        group_level: true
        use_prior: False
      ##########################################################################
  eval_config:
    additional_metrics:
      - name: "mixcem_sel"
        include_list: [".*(MixCEM).*"]
    additional_test_sets:
      - name: "OOD_sap_0.1"
        update_previous: true
        dataset_config:
          test_transformation_config:
            post_generation: false
            name: "salt_and_pepper"
            amount: 0.1
            s_vs_p: 0.5

  skip_repr_evaluation: true
  num_load_workers: 4
  max_epochs: 150
  top_k_accuracy: null
  save_model: true
  patience: 2
  lr_scheduler_patience: 5
  check_val_every_n_epoch: 5
  emb_size: 16
  extra_dims: 0
  concept_loss_weight: [1, 5, 10]
  learning_rate: 0.01
  weight_decay: 4.0e-06
  weight_loss: true
  c_extractor_arch: "resnet18"
  optimizer: "sgd"
  bool: false
  early_stopping_monitor: "val_loss"
  early_stopping_mode: "min"
  early_stopping_delta: 0.0
  momentum: 0.9
  sigmoidal_prob: false
  training_intervention_prob: 0
runs:

  - architecture: "ConceptBottleneckModel"
    run_name: "DNN"
    extra_dims: 100
    concept_loss_weight: 0
    sigmoidal_prob: true
    embedding_activation: "leakyrelu"

  - architecture: "ConceptBottleneckModel"
    run_name: "Joint CBM"
    concept_loss_weight: 1
    sigmoidal_prob: true
    embedding_activation: "leakyrelu"
    sigmoidal_extra_capacity: false

  - architecture: "ConceptBottleneckModel"
    run_name: "Joint Logit CBM"
    concept_loss_weight: 10
    embedding_activation: "leakyrelu"
    sigmoidal_extra_capacity: false

  - architecture: "SequentialConceptBottleneckModel"
    run_name: "Sequential CBM"
    concept_loss_weight: 1
    sigmoidal_prob: true
    embedding_activation: "leakyrelu"
    sigmoidal_extra_capacity: false

  - architecture: "IndependentConceptBottleneckModel"
    run_name: "Independent CBM"
    concept_loss_weight: 1
    sigmoidal_prob: true
    embedding_activation: "leakyrelu"
    sigmoidal_extra_capacity: false

  - architecture: "ConceptBottleneckModel"
    run_name: "Hybrid-CBM"
    extra_dims: 50
    concept_loss_weight: 10
    sigmoidal_prob: true
    embedding_activation: "leakyrelu"

  - architecture: "ConceptEmbeddingModel"
    run_name: "CEM"
    concept_loss_weight: 5
    sigmoidal_prob: true
    training_intervention_prob: 0.25
    embedding_activation: "leakyrelu"

  - architecture: "IntAwareConceptEmbeddingModel"
    run_name: "IntCEM"
    emb_size: 32
    concept_loss_weight: 1
    training_intervention_prob: 0.25
    intervention_weight: 0.1
    intervention_task_discount: 1.1
    use_concept_groups: true
    int_model_use_bn: true
    int_model_layers: [128, 128, 64, 64]
    embedding_activation: "leakyrelu"
    max_horizon: 6
    horizon_rate: 1.005
    gradient_clip_val: 100

  - architecture: "ProbabilisticConceptBottleneckModel"
    run_name: "ProbCBM"
    concept_loss_weight: 1
    learning_rate: 0.001
    weight_decay: 0
    weight_loss: false
    optimizer: "adam"
    n_samples_inference: 50
    use_neg_concept: true
    pred_class: true
    init_negative_scale: 5
    init_shift: 5
    pretrained: true
    hidden_dim: 16
    class_hidden_dim: 64
    intervention_prob: 0.5
    gradient_clip_val: 2.0
    max_concept_epochs: 70
    warmup_epochs: 5
    max_task_epochs: 75
    vib_beta: 5.0e-05
    lr_ratio: 10

  - architecture: "PosthocConceptBottleneckModel"
    run_name: "Posthoc CBM"
    emb_size: null
    residual: false
    reg_strength: 0.001
    l1_ratio: 0.99
    freeze_pretrained_model: true
    freeze_concept_embeddings: true
    svd_penalty: 1
    active_top_percentile: 95
    active_bottom_percentile: 5
    blackbox_model_config:
      name: "resnet18"
      imagenet_pretrained: true
      add_linear_layers: [112]
    blackbox_training_epochs: 150
    max_residual_epochs: 150
    blackbox_path: "PCBM_BlackBox_Model_resnet18_pre_True_fold_2_layers_[112]"

  - architecture: "PosthocConceptBottleneckModel"
    run_name: "Posthoc Hybrid CBM"
    emb_size: null
    residual: true
    reg_strength: 0.1
    l1_ratio: 0.99
    freeze_pretrained_model: true
    freeze_concept_embeddings: true
    svd_penalty: 1
    blackbox_model_config:
      name: "resnet18"
      imagenet_pretrained: true
      add_linear_layers: [112]
    blackbox_training_epochs: 150
    max_residual_epochs: 150
    blackbox_path: "PCBM_BlackBox_Model_resnet18_pre_True_fold_2_layers_[112]"

  - architecture: "MixCEM"
    run_name: "MixCEM"
    concept_loss_weight: 5
    training_intervention_prob: 0.25
    load_path_name: "MixCEM_r_1_g_ood_0.1_cwl_5_ce_0"
    all_intervened_loss_weight: 1
    ood_dropout_prob: 0.1
    calibration_epochs: 30
