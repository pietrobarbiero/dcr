trials: 3
model_selection_trials: 1
results_dir: /anfs/bigdisc/me466/mixcem_results/cub_complete/

num_load_workers: 0

model_selection_groups:
  - ["^(DNN).*$", "DNN (Baseline)"]
  - ["^(CBM_Sigmoid_Baseline_).*$", "Joint CBM (Baseline)"]
  - ["^(CBM_Logit_Baseline_).*$", "Joint Logit CBM (Baseline)"]
  - ["^(Hybrid-CBM_).*(_Baseline_).*$", "Hybrid-CBM (Baseline)"]
  - ["^(CEM_Baseline_).*$", "CEM (Baseline)"]
  - ["^(IntCEM_).*(_Baseline).*$", "IntCEM (Baseline)"]
  - ["^(ProbCBM_).*$", "ProbCBM (Baseline)"]
  - ["^(PCBM_).*$", "Posthoc CBM (Baseline)"]
  - ["^(HybridPCBM_).*$", "Posthoc Hybrid CBM (Baseline)"]
  - ["^(Linear_CMCMixIntCEM).*(_ce_0_).*$", "Linear CMCMixIntCEM No Calibration (Baseline)"]
  - ["^(Linear_CMCMixIntCEM).*(_ce_30_).*$", "Linear CMCMixIntCEM (Baseline)"]
  - ["^(Linear_CMCMixCEM).*(_ce_0_).*$", "Linear CMCMixCEM No Calibration (Baseline)"]
  - ["^(Linear_CMCMixCEM).*(_ce_30_).*$", "Linear CMCMixCEM (Baseline)"]

  - ["^(Entropy_CMCMixIntCEM).*(r_1_).*(_ce_0_).*((g_ood_0.1_)|(g_ood_0.5_)).*$", "Entropy CMCMixIntCEM No Calibration (Baseline)"]
  - ["^(Entropy_CMCMixIntCEM).*(r_1_).*(_ce_30_).*((g_ood_0.1_)|(g_ood_0.5_)).*$", "Entropy CMCMixIntCEM (Baseline)"]
  - ["^(Entropy_CMCMixCEM).*(r_1_).*(_ce_0_).*((g_ood_0.1_)|(g_ood_0.5_)).*$", "Entropy CMCMixCEM No Calibration (Baseline)"]
  - ["^(Entropy_CMCMixCEM).*(r_1_).*(_ce_30_).*((g_ood_0.1_)|(g_ood_0.5_)).*$", "Entropy CMCMixCEM (Baseline)"]

  - ["^(Entropy_CMCMixIntCEM).*(r_0_).*(_ce_0_).*$", "Entropy CMCMixIntCEM (r=0) No Calibration (Baseline)"]
  - ["^(Entropy_CMCMixIntCEM).*(r_0_).*(_ce_30_).*$", "Entropy CMCMixIntCEM (r=0) (Baseline)"]
  - ["^(Entropy_CMCMixCEM).*(r_0_).*(_ce_0_).*$", "Entropy CMCMixCEM (r=0) No Calibration (Baseline)"]
  - ["^(Entropy_CMCMixCEM).*(r_0_).*(_ce_30_).*$", "Entropy CMCMixCEM (r=0) (Baseline)"]

  - ["^(Entropy_CMCMixIntCEM).*(_ce_0_).*(g_ood_0_).*$", "Entropy CMCMixIntCEM (drop=0) No Calibration (Baseline)"]
  - ["^(Entropy_CMCMixIntCEM).*(_ce_30_).*(g_ood_0_).*$", "Entropy CMCMixIntCEM (drop=0) (Baseline)"]
  - ["^(Entropy_CMCMixCEM).*(_ce_0_).*(g_ood_0_).*$", "Entropy CMCMixCEM (drop=0) No Calibration (Baseline)"]
  - ["^(Entropy_CMCMixCEM).*(_ce_30_).*(g_ood_0_).*$", "Entropy CMCMixCEM (drop=0) (Baseline)"]

model_selection_metrics:
  - val_acc_y_random_group_level_True_use_prior_False_int_auc
  - val_acc_y

shared_params:
  # Dataset Configuration
  dataset_config:
    dataset: "cub"
    num_workers: 8
    batch_size: 64

    # DATASET VARIABLES
    root_dir: /homes/me466/data/CUB200/
    sampling_percent: 1
    sampling_groups: True
    test_subsampling: 1
    weight_loss: True

  # Intervention Parameters
  intervention_config:
    competence_levels: [1]
    intervention_freq: 1
    intervention_batch_size: 1024 #256
    val_intervention_policies:
      - policy: "random"
        group_level: True
        use_prior: False
    intervention_policies:
      - policy: "random"
        group_level: True
        use_prior: False

  # Representation metrics
  # Change to False if you want representation metrics to be included in the
  # evaluation (may significantly increase experiment times)
  skip_repr_evaluation: True

  max_epochs: 150
  top_k_accuracy: null
  save_model: True
  patience: 15
  emb_size: 16
  extra_dims: 0
  concept_loss_weight: [1, 5, 10]
  learning_rate: 0.01
  weight_decay: 0.000004
  weight_loss: True
  c_extractor_arch: resnet18
  optimizer: sgd
  bool: False
  early_stopping_monitor: val_loss
  early_stopping_mode: min
  early_stopping_delta: 0.0
  momentum: 0.9
  sigmoidal_prob: False
  training_intervention_prob: 0
  grid_variables:
    - concept_loss_weight
  grid_search_mode: exhaustive

  # Evaluation configuration
  eval_config:
    additional_test_sets:

      - name: "OOD_sap_0.01"
        update_previous: True
        dataset_config:
         test_transformation_config:
           post_generation: False
           name: salt_and_pepper
           amount: 0.01
           s_vs_p: 0.5

      - name: "OOD_sap_0.05"
        update_previous: True
        dataset_config:
         test_transformation_config:
           post_generation: False
           name: salt_and_pepper
           amount: 0.05
           s_vs_p: 0.5

      - name: "OOD_sap_0.1"
        update_previous: True
        dataset_config:
          test_transformation_config:
            post_generation: False
            name: salt_and_pepper
            amount: 0.1
            s_vs_p: 0.5

      - name: "OOD_sap_0.25"
        update_previous: True
        dataset_config:
          test_transformation_config:
            post_generation: False
            name: salt_and_pepper
            amount: 0.25
            s_vs_p: 0.5

      - name: "OOD_sap_0.5"
        update_previous: True
        dataset_config:
          test_transformation_config:
            post_generation: False
            name: salt_and_pepper
            amount: 0.5
            s_vs_p: 0.5

      - name: "OOD_sap_0.75"
        update_previous: True
        dataset_config:
          test_transformation_config:
            post_generation: False
            name: salt_and_pepper
            amount: 0.75
            s_vs_p: 0.5

      - name: "OOD_sap_0.9"
        update_previous: True
        dataset_config:
          test_transformation_config:
            post_generation: False
            name: salt_and_pepper
            amount: 0.9
            s_vs_p: 0.5

      - name: "OOD_sap_0.99"
        update_previous: True
        dataset_config:
          test_transformation_config:
            post_generation: False
            name: salt_and_pepper
            amount: 0.99
            s_vs_p: 0.5

      - name: "OOD"
        update_previous: True
        dataset_config:
          test_transformation_config:
            name: random_noise
            low_noise_level: 1
            noise_level: 0.5


runs:
  - architecture: 'ProbabilisticConceptBottleneckModel'
    run_name: "ProbCBM_cwl_class_hidden_dim_{class_hidden_dim}_hidden_dim_{hidden_dim}_n_samples_inference_{n_samples_inference}_max_concept_epochs_{max_concept_epochs}_max_task_epochs_{max_task_epochs}"
    n_samples_inference: 50
    use_neg_concept: True
    pred_class: True
    init_negative_scale: 5
    init_shift: 5
    pretrained: True
    hidden_dim: [16, 32]
    class_hidden_dim: [64, 128]
    intervention_prob: 0.5
    gradient_clip_val: 2.0
    max_concept_epochs: 70
    warmup_epochs: 5
    max_task_epochs: 75
    vib_beta: 0.00005
    concept_loss_weight: 1
    learning_rate: 0.001
    lr_ratio: 10
    weight_decay: 0
    weight_loss: False
    optimizer: adam
    grid_variables:
      - class_hidden_dim
      - hidden_dim
    grid_search_mode: exhaustive

  - architecture: "IntAwareConceptEmbeddingModel"
    run_name: "IntCEM_emb_size_{emb_size}_intervention_weight_{intervention_weight}_intervention_task_discount_{intervention_task_discount}_Baseline_cwl_{concept_loss_weight}"
    training_intervention_prob: 0.25
    intervention_weight: [0.1, 1] #, 5]
    intervention_task_discount: [1.1, 1.5]
    use_concept_groups: True
    int_model_use_bn: True
    int_model_layers: [128, 128, 64, 64]
    embedding_activation: "leakyrelu"
    max_horizon: 6
    horizon_rate: 1.005
    gradient_clip_val: 100
    emb_size: [16, 32]
    grid_variables:
        - concept_loss_weight
        - intervention_task_discount
        - intervention_weight
        - emb_size
    grid_search_mode: exhaustive

  - architecture: 'ConceptEmbeddingModel'
    run_name: "CEM_Baseline_cwl_{concept_loss_weight}"
    sigmoidal_prob: True
    training_intervention_prob: 0.25
    embedding_activation: "leakyrelu"

  - architecture: 'ConceptBottleneckModel'
    run_name: "DNN_extra_dims_{extra_dims}"
    extra_dims: [0, 50, 100, 200]
    training_intervention_prob: 0
    embedding_activation: "leakyrelu"
    sigmoidal_prob: True
    concept_loss_weight: 0
    grid_variables:
      - extra_dims
    grid_search_mode: exhaustive

  # - architecture: 'MixingConceptEmbeddingModel'
  #   run_name: "MixCEM_n_extra_{n_discovered_concepts}_entr_{discovered_probs_entropy}_dis_{intervention_task_discount}_ip_{training_intervention_prob}_dip_{dyn_training_intervention_prob}_cl_{contrastive_loss_weight}_mix_{mix_ground_truth_embs}_shared_{shared_emb_generator}_emb_size_{emb_size}_ml_{intermediate_task_concept_loss}_Baseline_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [256, 64, 32]
  #   embedding_activation: null
  #   n_discovered_concepts: [50, 25]
  #   concept_loss_weight: [10, 1]
  #   contrastive_loss_weight: [0]
  #   mix_ground_truth_embs: True
  #   shared_emb_generator: [True]
  #   normalize_embs: False
  #   sample_probs: False
  #   cond_discovery: False
  #   intermediate_task_concept_loss: [0]
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.01, 1.05] #, 1.5]
  #   discovered_probs_entropy: 0
  #   dyn_training_intervention_prob: [0.1, 0.25]
  #   grid_variables:
  #     - concept_loss_weight
  #     - n_discovered_concepts
  #     - contrastive_loss_weight
  #     - shared_emb_generator
  #     - emb_size
  #     - intermediate_task_concept_loss
  #     - intervention_task_discount
  #     - training_intervention_prob
  #     - dyn_training_intervention_prob
  #   grid_search_mode: exhaustive


  # - architecture: 'MixingConceptEmbeddingModel'
  #   run_name: "MixCEM_n_extra_{n_discovered_concepts}_entr_{discovered_probs_entropy}_dis_{intervention_task_discount}_ip_{training_intervention_prob}_dip_{dyn_training_intervention_prob}_cl_{contrastive_loss_weight}_mix_{mix_ground_truth_embs}_shared_{shared_emb_generator}_emb_size_{emb_size}_ml_{intermediate_task_concept_loss}_Baseline_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [512]
  #   embedding_activation: null
  #   n_discovered_concepts: [10, 5]
  #   concept_loss_weight: [1, 5] #[10, 1]
  #   contrastive_loss_weight: [0]
  #   mix_ground_truth_embs: True
  #   shared_emb_generator: [True]
  #   normalize_embs: False
  #   sample_probs: False
  #   cond_discovery: False
  #   intermediate_task_concept_loss: [0]
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.01] #, 1.05] #, 1.5]
  #   discovered_probs_entropy: 0
  #   dyn_training_intervention_prob: [0.1, 0.25]
  #   grid_variables:
  #     - concept_loss_weight
  #     - n_discovered_concepts
  #     - contrastive_loss_weight
  #     - shared_emb_generator
  #     - emb_size
  #     - intermediate_task_concept_loss
  #     - intervention_task_discount
  #     - training_intervention_prob
  #     - dyn_training_intervention_prob
  #   grid_search_mode: exhaustive

  # - architecture: 'MixingConceptEmbeddingModel'
  #   run_name: "MixCEM_n_extra_{n_discovered_concepts}_entr_{discovered_probs_entropy}_dis_{intervention_task_discount}_ip_{training_intervention_prob}_dip_{dyn_training_intervention_prob}_cl_{contrastive_loss_weight}_mix_{mix_ground_truth_embs}_shared_{shared_emb_generator}_emb_size_{emb_size}_ml_{intermediate_task_concept_loss}_Baseline_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [1024, 512, 256, 128]
  #   embedding_activation: null
  #   n_discovered_concepts: [0]
  #   concept_loss_weight: [10, 5, 1]
  #   contrastive_loss_weight: [0]
  #   mix_ground_truth_embs: True
  #   shared_emb_generator: [True]
  #   normalize_embs: False
  #   sample_probs: False
  #   cond_discovery: False
  #   intermediate_task_concept_loss: [0]
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.01, 1.05] #, 1.5]
  #   discovered_probs_entropy: 0
  #   dyn_training_intervention_prob: [0.1] #, 0.25]
  #   grid_variables:
  #     - concept_loss_weight
  #     - n_discovered_concepts
  #     - contrastive_loss_weight
  #     - shared_emb_generator
  #     - emb_size
  #     - intermediate_task_concept_loss
  #     - intervention_task_discount
  #     - training_intervention_prob
  #     - dyn_training_intervention_prob
  #   grid_search_mode: exhaustive

  # - architecture: 'ConceptEmbeddingModel'
  #   run_name: "CEM_clip_Baseline_emb_size_{emb_size}_cwl_{concept_loss_weight}"
  #   sigmoidal_prob: True
  #   emb_size: 16
  #   training_intervention_prob: 0.25
  #   embedding_activation: "leakyrelu"

  #   dataset_config:
  #     dataset: "cub"
  #     num_workers: 8
  #     batch_size: 64 # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #     # from_clip_embedding: True # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #     clip_model: 'ViT-B/32' # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #     # input_features: 512 # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #     zero_shot_clip_attrs: True # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

  #     # DATASET VARIABLES
  #     root_dir: /homes/me466/data/CUB200/
  #     sampling_percent: 1
  #     sampling_groups: True
  #     test_subsampling: 1
  #     weight_loss: False # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


  # - architecture: 'MixingConceptEmbeddingModel'
  #   run_name: "MixCEM_clip_fe_{fixed_embeddings}_i_{initial_concept_embeddings}_n_{normalize_embs}_cos_{use_cosine_similarity}_e_{use_linear_emb_layer}_s_{fixed_scale}_n_extra_{n_discovered_concepts}_entr_{discovered_probs_entropy}_dis_{intervention_task_discount}_ip_{training_intervention_prob}_dip_{dyn_training_intervention_prob}_cl_{contrastive_loss_weight}_emb_size_{emb_size}_ml_{intermediate_task_concept_loss}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [512]
  #   embedding_activation: null
  #   n_discovered_concepts: [25, 0]
  #   concept_loss_weight: [10]
  #   contrastive_loss_weight: [0]
  #   mix_ground_truth_embs: True
  #   shared_emb_generator: [True]
  #   normalize_embs: False
  #   sample_probs: False
  #   cond_discovery: False
  #   intermediate_task_concept_loss: [0]
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.01, 1.05] #, 1.5]
  #   discovered_probs_entropy: 0
  #   dyn_training_intervention_prob: [0.1] #, 0.25]
  #   initial_concept_embeddings: [null]  # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   fixed_embeddings: [False]  # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   use_cosine_similarity: [False]
  #   # c_extractor_arch: "identity" # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   # c_extractor_arch: "mlp" # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   # c_extractor_arch_layers: [512, 512] # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   use_linear_emb_layer: [False] #, True]  # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   fixed_scale: [null] # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   # learning_rate: 0.001 # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   grid_variables:
  #     - concept_loss_weight
  #     - n_discovered_concepts
  #     - contrastive_loss_weight
  #     - shared_emb_generator
  #     - emb_size
  #     - intermediate_task_concept_loss
  #     - intervention_task_discount
  #     - training_intervention_prob
  #     - dyn_training_intervention_prob
  #     - fixed_embeddings
  #     - use_cosine_similarity
  #     - use_linear_emb_layer
  #     - fixed_scale
  #     - initial_concept_embeddings
  #   grid_search_mode: exhaustive

  #   dataset_config:
  #     dataset: "cub"
  #     num_workers: 8
  #     batch_size: 64 # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #     # from_clip_embedding: True # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #     clip_model: 'ViT-B/32' # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #     # input_features: 512 # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #     zero_shot_clip_attrs: True # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

  #     # DATASET VARIABLES
  #     root_dir: /homes/me466/data/CUB200/
  #     sampling_percent: 1
  #     sampling_groups: True
  #     test_subsampling: 1
  #     weight_loss: False # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

  # - architecture: 'MixingConceptEmbeddingModel'
  #   run_name: "MixCEM_esup_fe_{fixed_embeddings}_i_{initial_concept_embeddings}_n_{normalize_embs}_cos_{use_cosine_similarity}_e_{use_linear_emb_layer}_s_{fixed_scale}_n_extra_{n_discovered_concepts}_entr_{discovered_probs_entropy}_dis_{intervention_task_discount}_ip_{training_intervention_prob}_dip_{dyn_training_intervention_prob}_cl_{contrastive_loss_weight}_emb_size_{emb_size}_ml_{intermediate_task_concept_loss}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [512]
  #   embedding_activation: null
  #   n_discovered_concepts: [0]
  #   concept_loss_weight: [10]
  #   contrastive_loss_weight: [0]
  #   mix_ground_truth_embs: True
  #   shared_emb_generator: [True]
  #   normalize_embs: True
  #   sample_probs: False
  #   cond_discovery: False
  #   intermediate_task_concept_loss: [0]
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.01, 1.05] #, 1.5]
  #   discovered_probs_entropy: 0
  #   dyn_training_intervention_prob: [0.1] #, 0.25]
  #   initial_concept_embeddings: [True]  # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   fixed_embeddings: [True] #, False]  # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   use_cosine_similarity: [False]
  #   use_linear_emb_layer: [True] #, True]  # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   fixed_scale: [null] #, 100] # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   grid_variables:
  #     - concept_loss_weight
  #     - n_discovered_concepts
  #     - contrastive_loss_weight
  #     - shared_emb_generator
  #     - emb_size
  #     - intermediate_task_concept_loss
  #     - intervention_task_discount
  #     - training_intervention_prob
  #     - dyn_training_intervention_prob
  #     - fixed_embeddings
  #     - use_cosine_similarity
  #     - use_linear_emb_layer
  #     - fixed_scale
  #     - initial_concept_embeddings
  #   grid_search_mode: exhaustive


  - architecture: 'ConceptBottleneckModel'
    run_name: "Hybrid-CBM_Sigmoid_extra_dims_{extra_dims}_Baseline_cwl_{concept_loss_weight}"
    extra_dims: [50, 100, 200]
    training_intervention_prob: 0
    embedding_activation: "leakyrelu"
    sigmoidal_prob: True
    grid_variables:
      - concept_loss_weight
      - extra_dims
    grid_search_mode: exhaustive

  - architecture: 'ConceptBottleneckModel'
    run_name: "CBM_Sigmoid_Baseline_cwl_{concept_loss_weight}"
    training_intervention_prob: 0
    embedding_activation: "leakyrelu"
    bool: False
    extra_dims: 0
    sigmoidal_extra_capacity: False
    sigmoidal_prob: True

  - architecture: 'ConceptBottleneckModel'
    run_name: "CBM_Logit_Baseline_cwl_{concept_loss_weight}"
    embedding_activation: "leakyrelu"
    bool: False
    extra_dims: 0
    sigmoidal_extra_capacity: False
    sigmoidal_prob: False

  - architecture: 'SequentialConceptBottleneckModel'
    run_name: "CBM_Seq"
    embedding_activation: "leakyrelu"
    bool: False
    extra_dims: 0
    concept_loss_weight: [1]
    sigmoidal_extra_capacity: False
    sigmoidal_prob: True

  - architecture: 'IndependentConceptBottleneckModel'
    run_name: "CBM_Ind"
    embedding_activation: "leakyrelu"
    bool: False
    extra_dims: 0
    concept_loss_weight: [1]
    sigmoidal_extra_capacity: False
    sigmoidal_prob: True

  - architecture: 'PosthocConceptBottleneckModel'
    run_name: "PCBM_reg_{reg_strength}_l1_{l1_ratio}_penalty_{svd_penalty}"
    residual: False
    reg_strength: [0.000001, 0.001, 0.1]
    l1_ratio: [0.99]
    freeze_pretrained_model: True
    freeze_concept_embeddings: True
    emb_size: null

    svd_penalty: [1]
    active_top_percentile: 95
    active_bottom_percentile: 5
    blackbox_model_config:
      name: resnet18
      imagenet_pretrained: True
      add_linear_layers: [112]  # Add an intermediate layer with the same size as the number of concepts so that the evaluation is fair
    blackbox_training_epochs: 150
    max_residual_epochs: 150
    max_epochs: 150
    blackbox_path: 'PCBM_BlackBox_Model_{blackbox_model_config[name]}_pre_{blackbox_model_config[imagenet_pretrained]}_fold_{split}_layers_{blackbox_model_config[add_linear_layers]}'


    grid_variables:
      - reg_strength
      - l1_ratio
      - svd_penalty
    grid_search_mode: exhaustive

  - architecture: 'PosthocConceptBottleneckModel'
    run_name: "HybridPCBM_reg_{reg_strength}_l1_{l1_ratio}_penalty_{svd_penalty}"
    residual: True  # TURN ON THE RESIDUAL MODEL!
    reg_strength: [0.000001, 0.001, 0.1]
    l1_ratio: [0.99]
    freeze_pretrained_model: True
    freeze_concept_embeddings: True
    emb_size: null

    svd_penalty: [1]
    blackbox_model_config:
      name: resnet18
      imagenet_pretrained: True
      add_linear_layers: [112]  # Add an intermediate layer with the same size as the number of concepts so that the evaluation is fair
    blackbox_training_epochs: 150
    max_residual_epochs: 150
    max_epochs: 150
    blackbox_path: 'PCBM_BlackBox_Model_{blackbox_model_config[name]}_pre_{blackbox_model_config[imagenet_pretrained]}_fold_{split}_layers_{blackbox_model_config[add_linear_layers]}'

    grid_variables:
      - reg_strength
      - l1_ratio
      - svd_penalty
    grid_search_mode: exhaustive


  # - architecture: 'MCIntCEM'
  #   run_name: "CMCMixIntCEM_freeze_all_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "CMCMixIntCEM_freeze_all_r_{all_intervened_loss_weight}_{pooling_mode}_None_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   global_model_path: "g_CMCMixIntCEM_freeze_all_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1]
  #   emb_size: [16]
  #   shared_emb_generator: True
  #   montecarlo_train_tries: [1]
  #   montecarlo_test_tries: [50]
  #   ood_dropout_prob: [0.1]
  #   pooling_mode: ['concat']
  #   calibration_epochs: [0, 30]
  #   finetune_with_val: [True]
  #   learnable_temps: False
  #   class_wise_temperature: False
  #   global_epochs: [0]
  #   global_mixed_probs_coeff: 0
  #   freeze_global_embeddings: False
  #   max_epochs: 150
  #   uncerainty_threshold_percentile: [null]
  #   inference_threshold: False
  #   mixed_probs_coeff: [1]
  #   all_intervened_loss_weight: [0.01, 0.1, 1]
  #   calibrate_concept_probs: True
  #   dynamic_confidence_scaling: True

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]
  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - ood_dropout_prob
  #     - emb_size
  #     - finetune_with_val
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - global_epochs
  #     - montecarlo_train_tries
  #     - montecarlo_test_tries
  #     - mixed_probs_coeff
  #     - all_intervened_loss_weight
  #     - calibration_epochs
  #     - uncerainty_threshold_percentile
  #   grid_search_mode: exhaustive


  - architecture: 'MCIntCEM'
    run_name: "Linear_CMCMixIntCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    load_path_name: "Linear_CMCMixIntCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_None_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    global_model_path: "g_Linear_CMCMixIntCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

    concept_loss_weight: [1]
    emb_size: [16]
    shared_emb_generator: True
    montecarlo_train_tries: [1]
    montecarlo_test_tries: [50]
    ood_dropout_prob: [0.1]
    pooling_mode: ['concat']
    calibration_epochs: [0, 30]
    finetune_with_val: [True]
    learnable_temps: False
    class_wise_temperature: False
    global_epochs: [0]
    global_mixed_probs_coeff: 0
    freeze_global_embeddings: False
    max_epochs: 150
    uncerainty_threshold_percentile: [null]
    inference_threshold: False
    mixed_probs_coeff: [1]
    all_intervened_loss_weight: [0.01, 0.1, 1]
    calibrate_concept_probs: True
    dynamic_confidence_scaling: True
    temperature: [1, 2, 0]  # Difference!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    scale_fn: linear  # Difference!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    ###############################################################################################

    # IntCEM stuff
    embedding_activation: null
    training_intervention_prob: [0.25]
    intervention_task_discount: [1.1]
    use_concept_groups: True
    int_model_use_bn: True
    int_model_layers: [128, 128, 64, 64]
    max_horizon: 6
    horizon_rate: 1.005
    intervention_weight: [0.1]
    grid_variables:
      - training_intervention_prob
      - intervention_task_discount
      - ood_dropout_prob
      - emb_size
      - finetune_with_val
      - concept_loss_weight
      - pooling_mode
      - intervention_weight
      - global_epochs
      - montecarlo_train_tries
      - montecarlo_test_tries
      - mixed_probs_coeff
      - all_intervened_loss_weight
      - temperature
      - calibration_epochs
      - uncerainty_threshold_percentile
    grid_search_mode: exhaustive


  - architecture: 'MCIntCEM'
    run_name: "Linear_CMCMixCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    load_path_name: "Linear_CMCMixCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_None_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    global_model_path: "g_Linear_CMCMixCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

    concept_loss_weight: [1]
    emb_size: [16]
    shared_emb_generator: True
    montecarlo_train_tries: [1]
    montecarlo_test_tries: [50]
    ood_dropout_prob: [0.1]
    pooling_mode: ['concat']
    calibration_epochs: [0, 30]
    finetune_with_val: [True]
    learnable_temps: False
    class_wise_temperature: False
    global_epochs: [0]
    global_mixed_probs_coeff: 0
    freeze_global_embeddings: False
    max_epochs: 150
    uncerainty_threshold_percentile: [null]
    inference_threshold: False
    mixed_probs_coeff: [1]
    all_intervened_loss_weight: [0.01, 0.1, 1]
    calibrate_concept_probs: True
    dynamic_confidence_scaling: True
    temperature: [1, 2, 0]  # Difference!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    scale_fn: linear  # Difference!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    ###############################################################################################

    # IntCEM stuff
    embedding_activation: null
    training_intervention_prob: [0.25]
    intervention_task_discount: [1]
    use_concept_groups: True
    int_model_use_bn: False
    int_model_layers: []
    max_horizon: 1
    horizon_rate: 1
    intervention_weight: [0]
    grid_variables:
      - training_intervention_prob
      - intervention_task_discount
      - ood_dropout_prob
      - emb_size
      - finetune_with_val
      - concept_loss_weight
      - pooling_mode
      - intervention_weight
      - global_epochs
      - montecarlo_train_tries
      - montecarlo_test_tries
      - mixed_probs_coeff
      - all_intervened_loss_weight
      - temperature
      - calibration_epochs
      - uncerainty_threshold_percentile
    grid_search_mode: exhaustive



  - architecture: 'MCIntCEM'
    run_name: "Entropy_CMCMixIntCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    load_path_name: "Entropy_CMCMixIntCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_None_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_50_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    global_model_path: "g_Entropy_CMCMixIntCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_50_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

    concept_loss_weight: [1]
    emb_size: [16]
    shared_emb_generator: True
    montecarlo_train_tries: [1]
    montecarlo_test_tries: [50, 1]
    ood_dropout_prob: [0.1, 0.5]
    pooling_mode: ['concat']
    calibration_epochs: [0, 30]
    finetune_with_val: [True]
    learnable_temps: False
    class_wise_temperature: False
    global_epochs: [0]
    global_mixed_probs_coeff: 0
    freeze_global_embeddings: False
    max_epochs: 150
    uncerainty_threshold_percentile: [null]
    inference_threshold: False
    mixed_probs_coeff: [1]
    all_intervened_loss_weight: [1, 0.1]
    calibrate_concept_probs: True
    dynamic_confidence_scaling: True
    temperature: [1]
    scale_fn: entropy

    ###############################################################################################

    # IntCEM stuff
    embedding_activation: null
    training_intervention_prob: [0.25]
    intervention_task_discount: [1.1]
    use_concept_groups: True
    int_model_use_bn: True
    int_model_layers: [128, 128, 64, 64]
    max_horizon: 6
    horizon_rate: 1.005
    intervention_weight: [0.1]
    grid_variables:
      - training_intervention_prob
      - intervention_task_discount
      - ood_dropout_prob
      - emb_size
      - finetune_with_val
      - concept_loss_weight
      - pooling_mode
      - intervention_weight
      - global_epochs
      - montecarlo_train_tries
      - montecarlo_test_tries
      - mixed_probs_coeff
      - all_intervened_loss_weight
      - temperature
      - calibration_epochs
      - uncerainty_threshold_percentile
    grid_search_mode: exhaustive


  - architecture: 'MCIntCEM'
    run_name: "Entropy_CMCMixCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    load_path_name: "Entropy_CMCMixCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_None_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_50_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    global_model_path: "g_Entropy_CMCMixCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_50_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

    concept_loss_weight: [1]
    emb_size: [16]
    shared_emb_generator: True
    montecarlo_train_tries: [1]
    montecarlo_test_tries: [50, 1]
    ood_dropout_prob: [0.1, 0.5]
    pooling_mode: ['concat']
    calibration_epochs: [0, 30]
    finetune_with_val: [True]
    learnable_temps: False
    class_wise_temperature: False
    global_epochs: [0]
    global_mixed_probs_coeff: 0
    freeze_global_embeddings: False
    max_epochs: 150
    uncerainty_threshold_percentile: [null]
    inference_threshold: False
    mixed_probs_coeff: [1]
    all_intervened_loss_weight: [1, 0.1]
    calibrate_concept_probs: True
    dynamic_confidence_scaling: True
    temperature: [1]
    scale_fn: entropy

    ###############################################################################################

    # IntCEM stuff
    embedding_activation: null
    training_intervention_prob: [0.25]
    intervention_task_discount: [1]
    use_concept_groups: True
    int_model_use_bn: False
    int_model_layers: []
    max_horizon: 1
    horizon_rate: 1
    intervention_weight: [0]
    grid_variables:
      - training_intervention_prob
      - intervention_task_discount
      - ood_dropout_prob
      - emb_size
      - finetune_with_val
      - concept_loss_weight
      - pooling_mode
      - intervention_weight
      - global_epochs
      - montecarlo_train_tries
      - montecarlo_test_tries
      - mixed_probs_coeff
      - all_intervened_loss_weight
      - temperature
      - calibration_epochs
      - uncerainty_threshold_percentile
    grid_search_mode: exhaustive



  # - architecture: 'CertificateConceptEmbeddingModel'
  #   run_name: "ConceptMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "ConceptMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1]
  #   emb_size: [16]
  #   temperature: [1]
  #   max_temperature: 1
  #   shared_emb_generator: True

  #   # OOD stuff
  #   certificate_loss_weight: [0]
  #   ood_dropout_prob: [0]
  #   global_ood_prob: [0.5]
  #   entire_global_prob: [0]
  #   pooling_mode: ['iss']
  #   selection_mode: ['max_concept_confidence']
  #   hard_eval_selection: [null, 0.5]
  #   selection_sample: [False]
  #   calibration_epochs: [0, 30]
  #   mixed_probs: True
  #   contrastive_reg: 0
  #   calibrate_dynamic_logits: True
  #   calibrate_global_logits: True
  #   finetune_with_val: [True]
  #   init_dyn_temps: 1
  #   init_global_temps: 1
  #   global_temp_reg: 0
  #   inference_dyn_prob: False
  #   learnable_temps: False
  #   positive_calibration: False
  #   class_wise_temperature: False
  #   separate_calibration: False
  #   freeze_global_components: [False]
  #   global_epochs: [5, 0]
  #   print_eval_only: True
  #   counter_limit: 0

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]

  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - certificate_loss_weight
  #     - ood_dropout_prob
  #     - selection_mode
  #     - hard_eval_selection
  #     - selection_sample
  #     - global_ood_prob
  #     - entire_global_prob
  #     - temperature
  #     - finetune_with_val
  #     - calibration_epochs
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - freeze_global_components
  #     - global_epochs
  #   grid_search_mode: exhaustive


  # # Mixing approach for vanilla CEMs
  # - architecture: 'CertificateConceptEmbeddingModel'
  #   run_name: "ConceptMixCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "ConceptMixCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1, 10]
  #   emb_size: [16]
  #   temperature: [1]
  #   max_temperature: 1
  #   shared_emb_generator: True

  #   # OOD stuff
  #   certificate_loss_weight: [0]
  #   ood_dropout_prob: [0]
  #   global_ood_prob: [0.5]
  #   entire_global_prob: [0]
  #   pooling_mode: ['iss']
  #   selection_mode: ['max_concept_confidence']
  #   hard_eval_selection: [null]
  #   selection_sample: [False]
  #   calibration_epochs: [0, 30]
  #   mixed_probs: True
  #   contrastive_reg: 0
  #   calibrate_dynamic_logits: True
  #   calibrate_global_logits: True
  #   finetune_with_val: [True]
  #   init_dyn_temps: 1
  #   init_global_temps: 1
  #   global_temp_reg: 0
  #   inference_dyn_prob: False
  #   learnable_temps: False
  #   positive_calibration: False
  #   class_wise_temperature: False
  #   separate_calibration: False
  #   freeze_global_components: [False]
  #   global_epochs: [5, 0]
  #   print_eval_only: True
  #   counter_limit: 0

  #   ############################################################################

  #   # IntCEM stuff cancelled so that it behaves as a CEM
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1]
  #   use_concept_groups: True
  #   int_model_use_bn: False
  #   int_model_layers: []
  #   max_horizon: 1
  #   horizon_rate: 1
  #   intervention_weight: [0]

  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - certificate_loss_weight
  #     - ood_dropout_prob
  #     - selection_mode
  #     - hard_eval_selection
  #     - selection_sample
  #     - global_ood_prob
  #     - entire_global_prob
  #     - temperature
  #     - finetune_with_val
  #     - calibration_epochs
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - freeze_global_components
  #     - global_epochs
  #   grid_search_mode: exhaustive