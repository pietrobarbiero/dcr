trials: 3
model_selection_trials: 1

results_dir: /anfs/bigdisc/me466/mixcem_results/cub_incomplete/

model_selection_groups:
  - ["^(DNN).*$", "DNN (Baseline)"]
  - ["^(CBM_Sigmoid_Baseline_).*$", "Joint CBM (Baseline)"]
  - ["^(Hybrid-CBM_).*(_Baseline_).*$", "Hybrid-CBM (Baseline)"]
  - ["^(CEM_Baseline_).*$", "CEM (Baseline)"]
  - ["^(IntCEM_).*(_Baseline).*$", "IntCEM (Baseline)"]
  - ["^(MixCEM_).*(_Baseline).*$", "MixCEM (Baseline)"]
  - ["^(ProbCBM_).*$", "ProbCBM (Baseline)"]
  - ["^(PCBM_).*$", "Posthoc CBM (Baseline)"]
  - ["^(HybridPCBM_).*$", "Hybrid Posthoc CBM (Baseline)"]
  - ["^(BankMixCEM_).*$", "BankMixCEM (Baseline)"]
  - ["^(CertificateCEM).*", "CertificateCEM (Baseline)"]
  - ["^(MixIntCEM).*", "MixIntCEM (Baseline)"]
  - ["^(FinalMixIntCEM).*", "LabelMixIntCEM (Baseline)"]
  - ["^(ConceptMixIntCEM).*(_ce_0_).*$", "ConceptMixIntCEM No Calibration (Baseline)"]
  - ["^(ConceptMixIntCEM_0).*(_ce_30_).*$", "ConceptMixIntCEM No Pretaining (Baseline)"]
  - ["^(ConceptMixIntCEM_5).*(_ce_30_).*$", "ConceptMixIntCEM Pretrained (Baseline)"]
  - ["^(ConceptMixCEM).*(_ce_0_).*$", "ConceptMixCEM No Calibration (Baseline)"]
  - ["^(ConceptMixCEM_0).*(_ce_30_).*$", "ConceptMixCEM No Pretaining (Baseline)"]
  - ["^(ConceptMixCEM_5).*(_ce_30_).*$", "ConceptMixCEM Pretrained (Baseline)"]
  - ["^(CMCMixIntCEM).*(_ce_0_).*$", "CMCMixIntCEM No Calibration (Baseline)"]
  - ["^(CMCMixIntCEM).*(_ce_30_).*$", "CMCMixIntCEM (Baseline)"]
  - ["^(CMCMixCEM).*(_ce_0_).*$", "CMCMixCEM No Calibration (Baseline)"]
  - ["^(CMCMixCEM).*(_ce_30_).*$", "CMCMixCEM (Baseline)"]
  - ["^(Linear_CMCMixIntCEM).*(_ce_0_).*$", "Linear CMCMixIntCEM No Calibration (Baseline)"]
  - ["^(Linear_CMCMixIntCEM).*(_ce_30_).*$", "Linear CMCMixIntCEM (Baseline)"]
  - ["^(Linear_CMCMixCEM).*(_ce_0_).*$", "Linear CMCMixCEM No Calibration (Baseline)"]
  - ["^(Linear_CMCMixCEM).*(_ce_30_).*$", "Linear CMCMixCEM (Baseline)"]

  - ["^(Entropy_CMCMixIntCEM).*((r_1_)|(r_0.1_)).*(_ce_0_).*((g_ood_0.1_)|(g_ood_0.5_)).*$", "Entropy CMCMixIntCEM No Calibration (Baseline)"]
  - ["^(Entropy_CMCMixIntCEM).*((r_1_)|(r_0.1_)).*(_ce_30_).*((g_ood_0.1_)|(g_ood_0.5_)).*$", "Entropy CMCMixIntCEM (Baseline)"]
  - ["^(Entropy_CMCMixCEM).*((r_1_)|(r_0.1_)).*(_ce_0_).*((g_ood_0.1_)|(g_ood_0.5_)).*$", "Entropy CMCMixCEM No Calibration (Baseline)"]
  - ["^(Entropy_CMCMixCEM).*((r_1_)|(r_0.1_)).*(_ce_30_).*((g_ood_0.1_)|(g_ood_0.5_)).*$", "Entropy CMCMixCEM (Baseline)"]

  - ["^(Entropy_CMCMixIntCEM).*(r_0_).*(_ce_0_).*$", "Entropy CMCMixIntCEM (r=0) No Calibration (Baseline)"]
  - ["^(Entropy_CMCMixIntCEM).*(r_0_).*(_ce_30_).*$", "Entropy CMCMixIntCEM (r=0) (Baseline)"]
  - ["^(Entropy_CMCMixCEM).*(r_0_).*(_ce_0_).*$", "Entropy CMCMixCEM (r=0) No Calibration (Baseline)"]
  - ["^(Entropy_CMCMixCEM).*(r_0_).*(_ce_30_).*$", "Entropy CMCMixCEM (r=0) (Baseline)"]

  - ["^(Entropy_CMCMixIntCEM).*(_ce_0_).*(g_ood_0_).*$", "Entropy CMCMixIntCEM (drop=0) No Calibration (Baseline)"]
  - ["^(Entropy_CMCMixIntCEM).*(_ce_30_).*(g_ood_0_).*$", "Entropy CMCMixIntCEM (drop=0) (Baseline)"]
  - ["^(Entropy_CMCMixCEM).*(_ce_0_).*(g_ood_0_).*$", "Entropy CMCMixCEM (drop=0) No Calibration (Baseline)"]
  - ["^(Entropy_CMCMixCEM).*(_ce_30_).*(g_ood_0_).*$", "Entropy CMCMixCEM (drop=0) (Baseline)"]

model_selection_metrics:
  - val_acc_y_random_group_level_True_use_prior_False_int_auc
  - val_acc_y

shared_params:
  # Dataset Configuration
  dataset_config:
    dataset: "cub"
    num_workers: 8
    batch_size: 64

    # DATASET VARIABLES
    root_dir: /homes/me466/data/CUB200/
    sampling_percent: 0.25  # [IMPORTANT] Select only a quarter of all concepts!
    sampling_groups: True
    test_subsampling: 1
    weight_loss: True

  # Intervention Parameters
  intervention_config:
    competence_levels: [1]
    intervention_freq: 1
    intervention_batch_size: 256
    val_intervention_policies:
      - policy: "random"
        group_level: True
        use_prior: False
    intervention_policies:
      - policy: "random"
        group_level: True
        use_prior: False

  # Representation metrics
  # Change to False if you want representation metrics to be included in the
  # evaluation (may significantly increase experiment times)
  skip_repr_evaluation: True

  max_epochs: 150
  top_k_accuracy: null
  save_model: True
  patience: 15
  emb_size: 16
  extra_dims: 0
  concept_loss_weight: [1, 5, 10]
  learning_rate: 0.01
  weight_decay: 0.000004
  weight_loss: True
  c_extractor_arch: resnet18
  optimizer: sgd
  bool: False
  early_stopping_monitor: val_loss
  early_stopping_mode: min
  early_stopping_delta: 0.0
  momentum: 0.9
  sigmoidal_prob: False
  training_intervention_prob: 0
  grid_variables:
    - concept_loss_weight
  grid_search_mode: exhaustive

  # Evaluation configuration
  eval_config:
    additional_test_sets:

      # - name: "OOD_sap_0.001"
      #   update_previous: True
      #   dataset_config:
      #     test_transformation_config:
      #       post_generation: False
      #       name: salt_and_pepper
      #       amount: 0.001
      #       s_vs_p: 0.5

      #- name: "OOD_sap_0.01"
      #  update_previous: True
      #  dataset_config:
      #    test_transformation_config:
      #      post_generation: False
      #      name: salt_and_pepper
      #      amount: 0.01
      #      s_vs_p: 0.5

      #- name: "OOD_sap_0.05"
      #  update_previous: True
      #  dataset_config:
      #    test_transformation_config:
      #      post_generation: False
      #      name: salt_and_pepper
      #      amount: 0.05
      #      s_vs_p: 0.5

      - name: "OOD_sap_0.1"
        update_previous: True
        dataset_config:
          test_transformation_config:
            post_generation: False
            name: salt_and_pepper
            amount: 0.1
            s_vs_p: 0.5

      - name: "OOD_sap_0.25"
        update_previous: True
        dataset_config:
          test_transformation_config:
            post_generation: False
            name: salt_and_pepper
            amount: 0.25
            s_vs_p: 0.5

      - name: "OOD_sap_0.5"
        update_previous: True
        dataset_config:
          test_transformation_config:
            post_generation: False
            name: salt_and_pepper
            amount: 0.5
            s_vs_p: 0.5

      - name: "OOD_sap_0.75"
        update_previous: True
        dataset_config:
          test_transformation_config:
            post_generation: False
            name: salt_and_pepper
            amount: 0.75
            s_vs_p: 0.5

      - name: "OOD_sap_0.9"
        update_previous: True
        dataset_config:
          test_transformation_config:
            post_generation: False
            name: salt_and_pepper
            amount: 0.9
            s_vs_p: 0.5

      - name: "OOD_sap_0.99"
        update_previous: True
        dataset_config:
          test_transformation_config:
            post_generation: False
            name: salt_and_pepper
            amount: 0.99
            s_vs_p: 0.5

      - name: "OOD"
        update_previous: True
        dataset_config:
          test_transformation_config:
            name: random_noise
            low_noise_level: 1
            noise_level: 0.5

runs:

  - architecture: 'ConceptEmbeddingModel'
    run_name: "CEM_Baseline_cwl_{concept_loss_weight}"
    sigmoidal_prob: True
    training_intervention_prob: 0.25
    embedding_activation: "leakyrelu"

  - architecture: 'PosthocConceptBottleneckModel'
    run_name: "PCBM_reg_{reg_strength}_l1_{l1_ratio}_penalty_{svd_penalty}"
    residual: False
    reg_strength: [0.000001, 0.001, 0.1]
    l1_ratio: [0.99]
    freeze_pretrained_model: True
    freeze_concept_embeddings: True
    emb_size: null

    svd_penalty: [1]
    active_top_percentile: 95
    active_bottom_percentile: 5
    blackbox_model_config:
      name: resnet18
      imagenet_pretrained: True
      add_linear_layers: [112]  # Add an intermediate layer with the same size as the number of concepts so that the evaluation is fair
    blackbox_training_epochs: 150
    max_residual_epochs: 150
    max_epochs: 150
    blackbox_path: 'PCBM_BlackBox_Model_{blackbox_model_config[name]}_pre_{blackbox_model_config[imagenet_pretrained]}_fold_{split}_layers_{blackbox_model_config[add_linear_layers]}'


    grid_variables:
      - reg_strength
      - l1_ratio
      - svd_penalty
    grid_search_mode: exhaustive

  - architecture: 'PosthocConceptBottleneckModel'
    run_name: "HybridPCBM_reg_{reg_strength}_l1_{l1_ratio}_penalty_{svd_penalty}"
    residual: True  # TURN ON THE RESIDUAL MODEL!
    reg_strength: [0.000001, 0.001, 0.1]
    l1_ratio: [0.99]
    freeze_pretrained_model: True
    freeze_concept_embeddings: True
    emb_size: null

    svd_penalty: [1]
    blackbox_model_config:
      name: resnet18
      imagenet_pretrained: True
      add_linear_layers: [112]  # Add an intermediate layer with the same size as the number of concepts so that the evaluation is fair
    blackbox_training_epochs: 150
    max_residual_epochs: 150
    max_epochs: 150
    blackbox_path: 'PCBM_BlackBox_Model_{blackbox_model_config[name]}_pre_{blackbox_model_config[imagenet_pretrained]}_fold_{split}_layers_{blackbox_model_config[add_linear_layers]}'

    grid_variables:
      - reg_strength
      - l1_ratio
      - svd_penalty
    grid_search_mode: exhaustive

  - architecture: 'ProbabilisticConceptBottleneckModel'
    run_name: "ProbCBM_cwl_class_hidden_dim_{class_hidden_dim}_hidden_dim_{hidden_dim}_n_samples_inference_{n_samples_inference}_max_concept_epochs_{max_concept_epochs}_max_task_epochs_{max_task_epochs}"
    n_samples_inference: 50
    use_neg_concept: True
    pred_class: True
    init_negative_scale: 5
    init_shift: 5
    pretrained: True
    hidden_dim: [16, 32]
    class_hidden_dim: [64, 128]
    intervention_prob: 0.5
    gradient_clip_val: 2.0
    max_concept_epochs: 70
    warmup_epochs: 5
    max_task_epochs: 75
    vib_beta: 0.00005
    concept_loss_weight: 1
    learning_rate: 0.001
    lr_ratio: 10
    weight_decay: 0
    weight_loss: False
    optimizer: adam
    grid_variables:
      - class_hidden_dim
      - hidden_dim
    grid_search_mode: exhaustive

  - architecture: "IntAwareConceptEmbeddingModel"
    run_name: "IntCEM_emb_size_{emb_size}_intervention_weight_{intervention_weight}_intervention_task_discount_{intervention_task_discount}_Baseline_cwl_{concept_loss_weight}"
    training_intervention_prob: 0.25
    intervention_weight: [0.1, 1] #, 5]
    intervention_task_discount: [1.1, 1.5]
    use_concept_groups: True
    int_model_use_bn: True
    int_model_layers: [128, 128, 64, 64]
    embedding_activation: "leakyrelu"
    max_horizon: 6
    horizon_rate: 1.005
    gradient_clip_val: 100
    emb_size: [16, 32]
    grid_variables:
        - concept_loss_weight
        - intervention_task_discount
        - intervention_weight
        - emb_size
    grid_search_mode: exhaustive

  - architecture: 'ConceptBottleneckModel'
    run_name: "DNN_extra_dims_{extra_dims}"
    extra_dims: [0, 50, 100, 200]
    training_intervention_prob: 0
    embedding_activation: "leakyrelu"
    sigmoidal_prob: True
    concept_loss_weight: 0
    grid_variables:
      - extra_dims
    grid_search_mode: exhaustive

  - architecture: 'ConceptBottleneckModel'
    run_name: "Hybrid-CBM_Sigmoid_extra_dims_{extra_dims}_Baseline_cwl_{concept_loss_weight}"
    extra_dims: [50, 100, 200]
    training_intervention_prob: 0
    embedding_activation: "leakyrelu"
    sigmoidal_prob: True
    grid_variables:
      - concept_loss_weight
      - extra_dims
    grid_search_mode: exhaustive

  - architecture: 'ConceptBottleneckModel'
    run_name: "CBM_Sigmoid_Baseline_cwl_{concept_loss_weight}"
    training_intervention_prob: 0
    embedding_activation: "leakyrelu"
    bool: False
    extra_dims: 0
    sigmoidal_extra_capacity: False
    sigmoidal_prob: True

  - architecture: 'ConceptBottleneckModel'
    run_name: "CBM_Logit_Baseline_cwl_{concept_loss_weight}"
    embedding_activation: "leakyrelu"
    bool: False
    extra_dims: 0
    sigmoidal_extra_capacity: False
    sigmoidal_prob: False


  # - architecture: 'MCIntCEM'
  #   run_name: "Linear_CMCMixIntCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "Linear_CMCMixIntCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_None_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   global_model_path: "g_LineaCMCMixIntCEM_r_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1] #, 10]
  #   emb_size: [16]
  #   shared_emb_generator: True
  #   montecarlo_train_tries: [1]
  #   montecarlo_test_tries: [50]
  #   ood_dropout_prob: [0.1] #, 0.1, 0.5]
  #   pooling_mode: ['concat']
  #   calibration_epochs: [0, 30]
  #   finetune_with_val: [True]
  #   learnable_temps: False
  #   class_wise_temperature: False
  #   global_epochs: [0] #, 50, 5]
  #   global_mixed_probs_coeff: 0
  #   freeze_global_embeddings: False
  #   max_epochs: 150
  #   uncerainty_threshold_percentile: [null]
  #   inference_threshold: False
  #   mixed_probs_coeff: [1]
  #   all_intervened_loss_weight: [0.1, 1]
  #   calibrate_concept_probs: True
  #   dynamic_confidence_scaling: True
  #   temperature: [1, 2, 0]  # Difference!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   scale_fn: linear  # Difference!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]
  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - ood_dropout_prob
  #     - emb_size
  #     - finetune_with_val
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - global_epochs
  #     - montecarlo_train_tries
  #     - montecarlo_test_tries
  #     - mixed_probs_coeff
  #     - all_intervened_loss_weight
  #     - temperature
  #     - calibration_epochs
  #     - uncerainty_threshold_percentile
  #   grid_search_mode: exhaustive


  # # Same as above but just with the normal CEM architecture
  # - architecture: 'MCIntCEM'
  #   run_name: "Linear_CMCMixCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "Linear_CMCMixCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_None_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   global_model_path: "g_LineaCMCMixCEM_r_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1]
  #   emb_size: [16]
  #   shared_emb_generator: True
  #   montecarlo_train_tries: [1]
  #   montecarlo_test_tries: [50]
  #   ood_dropout_prob: [0.1]
  #   pooling_mode: ['concat']
  #   calibration_epochs: [0, 30]
  #   finetune_with_val: [True]
  #   learnable_temps: False
  #   class_wise_temperature: False
  #   global_epochs: [0]
  #   global_mixed_probs_coeff: 0
  #   freeze_global_embeddings: False
  #   max_epochs: 150
  #   uncerainty_threshold_percentile: [null]
  #   inference_threshold: False
  #   mixed_probs_coeff: [1]
  #   all_intervened_loss_weight: [0.1, 1]
  #   calibrate_concept_probs: True
  #   dynamic_confidence_scaling: True
  #   temperature: [1, 2, 0]  # Difference!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   scale_fn: linear  # Difference!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1]
  #   use_concept_groups: True
  #   int_model_use_bn: False
  #   int_model_layers: []
  #   max_horizon: 1
  #   horizon_rate: 1
  #   intervention_weight: [0]
  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - ood_dropout_prob
  #     - emb_size
  #     - finetune_with_val
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - global_epochs
  #     - montecarlo_train_tries
  #     - montecarlo_test_tries
  #     - mixed_probs_coeff
  #     - all_intervened_loss_weight
  #     - temperature
  #     - calibration_epochs
  #     - uncerainty_threshold_percentile
  #   grid_search_mode: exhaustive



  # Same as above but using the entropy mixing function rather than the
  # linear function
  - architecture: 'MCIntCEM'
    run_name: "Entropy_CMCMixIntCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    load_path_name: "Entropy_CMCMixIntCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_None_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    global_model_path: "g_LineaCMCMixIntCEM_r_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

    concept_loss_weight: [1] #, 10]
    emb_size: [16]
    shared_emb_generator: True
    montecarlo_train_tries: [1]
    montecarlo_test_tries: [50, 1]
    ood_dropout_prob: [0.1, 0.5]
    pooling_mode: ['concat']
    calibration_epochs: [0, 30]
    finetune_with_val: [True]
    learnable_temps: False
    class_wise_temperature: False
    global_epochs: [0]
    global_mixed_probs_coeff: 0
    freeze_global_embeddings: False
    max_epochs: 150
    uncerainty_threshold_percentile: [null]
    inference_threshold: False
    mixed_probs_coeff: [1]
    all_intervened_loss_weight: [0.1, 1]
    calibrate_concept_probs: True
    dynamic_confidence_scaling: True
    temperature: [1]  # Difference!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    scale_fn: entropy  # Difference!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    ignore_task_acc_in_calibration: False

    ###############################################################################################

    # IntCEM stuff
    embedding_activation: null
    training_intervention_prob: [0.25]
    intervention_task_discount: [1.1]
    use_concept_groups: True
    int_model_use_bn: True
    int_model_layers: [128, 128, 64, 64]
    max_horizon: 6
    horizon_rate: 1.005
    intervention_weight: [0.1]
    grid_variables:
      - training_intervention_prob
      - intervention_task_discount
      - ood_dropout_prob
      - emb_size
      - finetune_with_val
      - concept_loss_weight
      - pooling_mode
      - intervention_weight
      - global_epochs
      - montecarlo_train_tries
      - montecarlo_test_tries
      - mixed_probs_coeff
      - all_intervened_loss_weight
      - temperature
      - calibration_epochs
      - uncerainty_threshold_percentile
    grid_search_mode: exhaustive


  # No all intervention loss
  - architecture: 'MCIntCEM'
    run_name: "Entropy_CMCMixIntCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    load_path_name: "Entropy_CMCMixIntCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_None_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    global_model_path: "g_LineaCMCMixIntCEM_r_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

    concept_loss_weight: [1]
    emb_size: [16]
    shared_emb_generator: True
    montecarlo_train_tries: [1]
    montecarlo_test_tries: [50, 1]
    ood_dropout_prob: [0.5]
    pooling_mode: ['concat']
    calibration_epochs: [0, 30]
    finetune_with_val: [True]
    learnable_temps: False
    class_wise_temperature: False
    global_epochs: [0]
    global_mixed_probs_coeff: 0
    freeze_global_embeddings: False
    uncerainty_threshold_percentile: [null]
    inference_threshold: False
    mixed_probs_coeff: [1]
    all_intervened_loss_weight: [0] # Difference!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    calibrate_concept_probs: True
    dynamic_confidence_scaling: True
    temperature: [1]
    scale_fn: entropy
    ignore_task_acc_in_calibration: False

    ###############################################################################################

    # IntCEM stuff
    embedding_activation: null
    training_intervention_prob: [0.25]
    intervention_task_discount: [1.1]
    use_concept_groups: True
    int_model_use_bn: True
    int_model_layers: [128, 128, 64, 64]
    max_horizon: 6
    horizon_rate: 1.005
    intervention_weight: [0.1]
    grid_variables:
      - training_intervention_prob
      - intervention_task_discount
      - ood_dropout_prob
      - emb_size
      - finetune_with_val
      - concept_loss_weight
      - pooling_mode
      - intervention_weight
      - global_epochs
      - montecarlo_train_tries
      - montecarlo_test_tries
      - mixed_probs_coeff
      - all_intervened_loss_weight
      - temperature
      - calibration_epochs
      - uncerainty_threshold_percentile
    grid_search_mode: exhaustive

  # No random residual dropout
  - architecture: 'MCIntCEM'
    run_name: "Entropy_CMCMixIntCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    load_path_name: "Entropy_CMCMixIntCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_None_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    global_model_path: "g_LineaCMCMixIntCEM_r_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

    concept_loss_weight: [1]
    emb_size: [16]
    shared_emb_generator: True
    montecarlo_train_tries: [1]
    montecarlo_test_tries: [1]
    ood_dropout_prob: [0] # Difference!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    pooling_mode: ['concat']
    calibration_epochs: [0, 30]
    finetune_with_val: [True]
    learnable_temps: False
    class_wise_temperature: False
    global_epochs: [0]
    global_mixed_probs_coeff: 0
    freeze_global_embeddings: False
    uncerainty_threshold_percentile: [null]
    inference_threshold: False
    mixed_probs_coeff: [1]
    all_intervened_loss_weight: [1, 0.1]
    calibrate_concept_probs: True
    dynamic_confidence_scaling: True
    temperature: [1]
    scale_fn: entropy
    ignore_task_acc_in_calibration: False

    ###############################################################################################

    # IntCEM stuff
    embedding_activation: null
    training_intervention_prob: [0.25]
    intervention_task_discount: [1.1]
    use_concept_groups: True
    int_model_use_bn: True
    int_model_layers: [128, 128, 64, 64]
    max_horizon: 6
    horizon_rate: 1.005
    intervention_weight: [0.1]
    grid_variables:
      - training_intervention_prob
      - intervention_task_discount
      - ood_dropout_prob
      - emb_size
      - finetune_with_val
      - concept_loss_weight
      - pooling_mode
      - intervention_weight
      - global_epochs
      - montecarlo_train_tries
      - montecarlo_test_tries
      - mixed_probs_coeff
      - all_intervened_loss_weight
      - temperature
      - calibration_epochs
      - uncerainty_threshold_percentile
    grid_search_mode: exhaustive


  # Same as above but just with the normal CEM architecture
  - architecture: 'MCIntCEM'
    run_name: "Entropy_CMCMixCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    load_path_name: "Entropy_CMCMixCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_None_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    global_model_path: "g_LineaCMCMixCEM_r_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

    concept_loss_weight: [1]
    emb_size: [16]
    shared_emb_generator: True
    montecarlo_train_tries: [1]
    montecarlo_test_tries: [50, 1]
    ood_dropout_prob: [0.1, 0.5]
    pooling_mode: ['concat']
    calibration_epochs: [0, 30]
    finetune_with_val: [True]
    learnable_temps: False
    class_wise_temperature: False
    global_epochs: [0]
    global_mixed_probs_coeff: 0
    freeze_global_embeddings: False
    max_epochs: 150
    uncerainty_threshold_percentile: [null]
    inference_threshold: False
    mixed_probs_coeff: [1]
    all_intervened_loss_weight: [0.1, 1]
    calibrate_concept_probs: True
    dynamic_confidence_scaling: True
    temperature: [1]
    scale_fn: entropy  # Difference!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    ignore_task_acc_in_calibration: False

    ###############################################################################################

    # IntCEM stuff
    embedding_activation: null
    training_intervention_prob: [0.25]
    intervention_task_discount: [1]
    use_concept_groups: True
    int_model_use_bn: False
    int_model_layers: []
    max_horizon: 1
    horizon_rate: 1
    intervention_weight: [0]
    grid_variables:
      - training_intervention_prob
      - intervention_task_discount
      - ood_dropout_prob
      - emb_size
      - finetune_with_val
      - concept_loss_weight
      - pooling_mode
      - intervention_weight
      - global_epochs
      - montecarlo_train_tries
      - montecarlo_test_tries
      - mixed_probs_coeff
      - all_intervened_loss_weight
      - temperature
      - calibration_epochs
      - uncerainty_threshold_percentile
    grid_search_mode: exhaustive

  # No all intervention loss
  - architecture: 'MCIntCEM'
    run_name: "Entropy_CMCMixCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    load_path_name: "Entropy_CMCMixCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_None_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    global_model_path: "g_LineaCMCMixCEM_r_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

    concept_loss_weight: [1]
    emb_size: [16]
    shared_emb_generator: True
    montecarlo_train_tries: [1]
    montecarlo_test_tries: [50, 1]
    ood_dropout_prob: [0.5]
    pooling_mode: ['concat']
    calibration_epochs: [0, 30]
    finetune_with_val: [True]
    learnable_temps: False
    class_wise_temperature: False
    global_epochs: [0]
    global_mixed_probs_coeff: 0
    freeze_global_embeddings: False
    uncerainty_threshold_percentile: [null]
    inference_threshold: False
    mixed_probs_coeff: [1]
    all_intervened_loss_weight: [0] # Difference!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    calibrate_concept_probs: True
    dynamic_confidence_scaling: True
    temperature: [1]
    scale_fn: entropy
    ignore_task_acc_in_calibration: False

    ###############################################################################################

    # IntCEM stuff
    embedding_activation: null
    training_intervention_prob: [0.25]
    intervention_task_discount: [1]
    use_concept_groups: True
    int_model_use_bn: False
    int_model_layers: []
    max_horizon: 1
    horizon_rate: 1
    intervention_weight: [0]
    grid_variables:
      - training_intervention_prob
      - intervention_task_discount
      - ood_dropout_prob
      - emb_size
      - finetune_with_val
      - concept_loss_weight
      - pooling_mode
      - intervention_weight
      - global_epochs
      - montecarlo_train_tries
      - montecarlo_test_tries
      - mixed_probs_coeff
      - all_intervened_loss_weight
      - temperature
      - calibration_epochs
      - uncerainty_threshold_percentile
    grid_search_mode: exhaustive

  # No random dropout
  - architecture: 'MCIntCEM'
    run_name: "Entropy_CMCMixCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    load_path_name: "Entropy_CMCMixCEM_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_None_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
    global_model_path: "g_LineaCMCMixCEM_r_t_{temperature}_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

    concept_loss_weight: [1]
    emb_size: [16]
    shared_emb_generator: True
    montecarlo_train_tries: [1]
    montecarlo_test_tries: [1]
    ood_dropout_prob: [0] # Difference!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    pooling_mode: ['concat']
    calibration_epochs: [0, 30]
    finetune_with_val: [True]
    learnable_temps: False
    class_wise_temperature: False
    global_epochs: [0]
    global_mixed_probs_coeff: 0
    freeze_global_embeddings: False
    uncerainty_threshold_percentile: [null]
    inference_threshold: False
    mixed_probs_coeff: [1]
    all_intervened_loss_weight: [1, 0.1]
    calibrate_concept_probs: True
    dynamic_confidence_scaling: True
    temperature: [1]
    scale_fn: entropy
    ignore_task_acc_in_calibration: False

    ###############################################################################################

    # IntCEM stuff
    embedding_activation: null
    training_intervention_prob: [0.25]
    intervention_task_discount: [1]
    use_concept_groups: True
    int_model_use_bn: False
    int_model_layers: []
    max_horizon: 1
    horizon_rate: 1
    intervention_weight: [0]
    grid_variables:
      - training_intervention_prob
      - intervention_task_discount
      - ood_dropout_prob
      - emb_size
      - finetune_with_val
      - concept_loss_weight
      - pooling_mode
      - intervention_weight
      - global_epochs
      - montecarlo_train_tries
      - montecarlo_test_tries
      - mixed_probs_coeff
      - all_intervened_loss_weight
      - temperature
      - calibration_epochs
      - uncerainty_threshold_percentile
    grid_search_mode: exhaustive



  # - architecture: 'MixingConceptEmbeddingModel'
  #   run_name: "MixCEM_n_extra_{n_discovered_concepts}_entr_{discovered_probs_entropy}_dis_{intervention_task_discount}_ip_{training_intervention_prob}_dip_{dyn_training_intervention_prob}_cl_{contrastive_loss_weight}_mix_{mix_ground_truth_embs}_shared_{shared_emb_generator}_emb_size_{emb_size}_ml_{intermediate_task_concept_loss}_Baseline_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [64] #, 32]
  #   embedding_activation: null
  #   n_discovered_concepts: [50] #, 25]
  #   concept_loss_weight: [10, 5, 1]
  #   contrastive_loss_weight: [0]
  #   mix_ground_truth_embs: True
  #   shared_emb_generator: [True]
  #   normalize_embs: False
  #   sample_probs: False
  #   cond_discovery: False
  #   intermediate_task_concept_loss: [0]
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.01, 1.05] #, 1.1, 1.5]
  #   discovered_probs_entropy: 0
  #   dyn_training_intervention_prob: [0.1, 0.25]
  #   grid_variables:
  #     - concept_loss_weight
  #     - n_discovered_concepts
  #     - contrastive_loss_weight
  #     - shared_emb_generator
  #     - emb_size
  #     - intermediate_task_concept_loss
  #     - intervention_task_discount
  #     - training_intervention_prob
  #     - dyn_training_intervention_prob
  #   grid_search_mode: exhaustive

  # - architecture: 'MixingConceptEmbeddingModel'
  #   run_name: "MixCEM_n_extra_{n_discovered_concepts}_entr_{discovered_probs_entropy}_dis_{intervention_task_discount}_ip_{training_intervention_prob}_dip_{dyn_training_intervention_prob}_cl_{contrastive_loss_weight}_mix_{mix_ground_truth_embs}_shared_{shared_emb_generator}_emb_size_{emb_size}_ml_{intermediate_task_concept_loss}_Baseline_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [1024, 512, 256] #, 32]
  #   embedding_activation: null
  #   n_discovered_concepts: [25, 10] #, 25]
  #   concept_loss_weight: [0.1, 1, 10]
  #   contrastive_loss_weight: [0]
  #   mix_ground_truth_embs: True
  #   shared_emb_generator: [True]
  #   normalize_embs: False
  #   sample_probs: False
  #   cond_discovery: False
  #   intermediate_task_concept_loss: [0]
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.01] #, 1.05] #, 1.1, 1.5]
  #   discovered_probs_entropy: 0
  #   dyn_training_intervention_prob: [0.1, 0.25]
  #   grid_variables:
  #     - concept_loss_weight
  #     - n_discovered_concepts
  #     - contrastive_loss_weight
  #     - shared_emb_generator
  #     - emb_size
  #     - intermediate_task_concept_loss
  #     - intervention_task_discount
  #     - training_intervention_prob
  #     - dyn_training_intervention_prob
  #   grid_search_mode: exhaustive


  # - architecture: 'MCIntCEM'
  #   run_name: "CMCMixIntCEM_freeze_all_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "CMCMixIntCEM_freeze_all_r_{all_intervened_loss_weight}_{pooling_mode}_None_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   global_model_path: "g_CMCMixIntCEM_freeze_all_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1] #, 10]
  #   emb_size: [16]
  #   shared_emb_generator: True
  #   montecarlo_train_tries: [1]
  #   montecarlo_test_tries: [50, 1]
  #   ood_dropout_prob: [0.1, 0.5, 0.9] #, 0.1, 0.5]
  #   pooling_mode: ['concat']
  #   calibration_epochs: [0, 30]
  #   finetune_with_val: [True]
  #   learnable_temps: False
  #   class_wise_temperature: False
  #   global_epochs: [0] #, 50, 5]
  #   global_mixed_probs_coeff: 0
  #   freeze_global_embeddings: False
  #   max_epochs: 150
  #   uncerainty_threshold_percentile: [null]
  #   inference_threshold: False
  #   mixed_probs_coeff: [1]
  #   all_intervened_loss_weight: [0.01, 0.1, 1]
  #   calibrate_concept_probs: True
  #   dynamic_confidence_scaling: True

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]
  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - ood_dropout_prob
  #     - emb_size
  #     - finetune_with_val
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - global_epochs
  #     - montecarlo_train_tries
  #     - montecarlo_test_tries
  #     - mixed_probs_coeff
  #     - all_intervened_loss_weight
  #     - calibration_epochs
  #     - uncerainty_threshold_percentile
  #   grid_search_mode: exhaustive


  # # Same as above but just with the normal CEM architecture
  # - architecture: 'MCIntCEM'
  #   run_name: "CMCMixCEM_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "CMCMixCEM_r_{all_intervened_loss_weight}_{pooling_mode}_None_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   global_model_path: "g_CMCMixCEM_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_50_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1]
  #   emb_size: [16]
  #   shared_emb_generator: True
  #   montecarlo_train_tries: [1]
  #   montecarlo_test_tries: [50, 1]
  #   ood_dropout_prob: [0.1]
  #   pooling_mode: ['concat']
  #   calibration_epochs: [0, 30]
  #   finetune_with_val: [True]
  #   learnable_temps: False
  #   class_wise_temperature: False
  #   global_epochs: [0]
  #   global_mixed_probs_coeff: 0
  #   freeze_global_embeddings: False
  #   max_epochs: 150
  #   uncerainty_threshold_percentile: [null]
  #   inference_threshold: False
  #   mixed_probs_coeff: [1]
  #   all_intervened_loss_weight: [0.01, 0.1, 1]
  #   calibrate_concept_probs: True
  #   dynamic_confidence_scaling: True

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1] # CHANGE
  #   use_concept_groups: True
  #   int_model_use_bn: False  # CHANGE
  #   int_model_layers: []  # CHANGE
  #   max_horizon: 1  # CHANGE
  #   horizon_rate: 1  # CHANGE
  #   intervention_weight: [0] # CHANGE
  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - ood_dropout_prob
  #     - emb_size
  #     - finetune_with_val
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - global_epochs
  #     - montecarlo_train_tries
  #     - montecarlo_test_tries
  #     - mixed_probs_coeff
  #     - all_intervened_loss_weight
  #     - calibration_epochs
  #     - uncerainty_threshold_percentile
  #   grid_search_mode: exhaustive


  # - architecture: 'MCIntCEM'
  #   run_name: "MCMixIntCEM_freeze_all_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "MCMixIntCEM_freeze_all_r_{all_intervened_loss_weight}_{pooling_mode}_1_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   global_model_path: "g_MCMixIntCEM_freeze_all_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1] #, 10]
  #   emb_size: [16]
  #   shared_emb_generator: True
  #   montecarlo_train_tries: [1]
  #   montecarlo_test_tries: [50]
  #   ood_dropout_prob: [0.01, 0.1, 0.5, 0.25] #, 0.1, 0.5]
  #   pooling_mode: ['concat']
  #   calibration_epochs: [0]
  #   finetune_with_val: [True]
  #   learnable_temps: False
  #   class_wise_temperature: False
  #   global_epochs: [0]
  #   global_mixed_probs_coeff: 0 # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   freeze_global_embeddings: False  # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   max_epochs: 150
  #   uncerainty_threshold_percentile: [1, 99]
  #   mixed_probs_coeff: [1]
  #   inference_threshold: True
  #   all_intervened_loss_weight: [0.01, 0.1, 1, 10] #, 1]
  #   # lr_scheduler_patience: 15 # 10 CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   # patience: 10  # 5 CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   global_cem_only_pass: True # CHANGE FOR GLOBAL!

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]
  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - ood_dropout_prob
  #     - emb_size
  #     - finetune_with_val
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - global_epochs
  #     - montecarlo_train_tries
  #     - montecarlo_test_tries
  #     - mixed_probs_coeff
  #     - all_intervened_loss_weight
  #     - calibration_epochs
  #     - uncerainty_threshold_percentile
  #   grid_search_mode: exhaustive

  # - architecture: 'MCIntCEM'
  #   run_name: "MCMixIntCEM_freeze_all_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "MCMixIntCEM_freeze_all_r_{all_intervened_loss_weight}_{pooling_mode}_50_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   global_model_path: "g_MCMixIntCEM_freeze_all_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1] #, 10]
  #   emb_size: [16]
  #   shared_emb_generator: True
  #   montecarlo_train_tries: [10]
  #   montecarlo_test_tries: [50]
  #   ood_dropout_prob: [0.1, 0.5]
  #   pooling_mode: ['concat']
  #   calibration_epochs: [0]
  #   finetune_with_val: [True]
  #   learnable_temps: False
  #   class_wise_temperature: False
  #   global_epochs: [75]
  #   global_mixed_probs_coeff: 0 # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   freeze_global_embeddings: False  # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   max_epochs: 150
  #   uncerainty_threshold_percentile: [50, 5, 95]
  #   mixed_probs_coeff: [1]
  #   inference_threshold: True
  #   all_intervened_loss_weight: [0.1, 1]
  #   # lr_scheduler_patience: 15 # 10 CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   # patience: 10  # 5 CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   global_cem_only_pass: True # CHANGE FOR GLOBAL!

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]
  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - ood_dropout_prob
  #     - emb_size
  #     - finetune_with_val
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - global_epochs
  #     - montecarlo_train_tries
  #     - montecarlo_test_tries
  #     - mixed_probs_coeff
  #     - all_intervened_loss_weight
  #     - calibration_epochs
  #     - uncerainty_threshold_percentile
  #   grid_search_mode: exhaustive

  # - architecture: 'MCIntCEM'
  #   run_name: "MCMixIntCEM_all_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "MCMixIntCEM_all_r_{all_intervened_loss_weight}_{pooling_mode}_50_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1] #, 10]
  #   emb_size: [16]
  #   shared_emb_generator: True
  #   montecarlo_train_tries: [10]
  #   montecarlo_test_tries: [50]
  #   ood_dropout_prob: [0.1, 0.5]
  #   pooling_mode: ['concat']
  #   calibration_epochs: [0]
  #   finetune_with_val: [True]
  #   learnable_temps: False
  #   class_wise_temperature: False
  #   global_epochs: [0]
  #   max_epochs: 150
  #   uncerainty_threshold_percentile: [50, 5, 95]
  #   mixed_probs_coeff: [0.5, 0, 0.1]
  #   inference_threshold: True
  #   all_intervened_loss_weight: [1, 0.1]
  #   # lr_scheduler_patience: 15 # 10 CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   # patience: 10  # 5 CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   global_cem_only_pass: True # CHANGE FOR GLOBAL!

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]
  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - ood_dropout_prob
  #     - emb_size
  #     - finetune_with_val
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - global_epochs
  #     - montecarlo_train_tries
  #     - montecarlo_test_tries
  #     - mixed_probs_coeff
  #     - all_intervened_loss_weight
  #     - calibration_epochs
  #     - uncerainty_threshold_percentile
  #   grid_search_mode: exhaustive


  # - architecture: 'MCIntCEM'
  #   run_name: "AMCMixIntCEM_ar_{anneal_rate}_{min_rate}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "AMCMixIntCEM_ar_{anneal_rate}_{min_rate}_{pooling_mode}_50_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1] #, 10]
  #   emb_size: [16] #, 32]
  #   shared_emb_generator: True
  #   montecarlo_train_tries: [10]
  #   montecarlo_test_tries: [50]
  #   ood_dropout_prob: [1] # Initial dropout val when annealing
  #   mixed_probs_coeff: [0]  # Initial mixed probability val when annealing
  #   pooling_mode: ['concat']
  #   calibration_epochs: [0]
  #   finetune_with_val: [True]
  #   learnable_temps: False
  #   class_wise_temperature: False
  #   global_epochs: [0]
  #   max_epochs: 150
  #   uncerainty_threshold_percentile: [50, 5, 95]
  #   anneal_rate: [0.001]
  #   min_rate: [0.1]
  #   inference_threshold: True
  #   global_cem_only_pass: True # CHANGE FOR GLOBAL!

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]

  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - ood_dropout_prob
  #     - finetune_with_val
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - global_epochs
  #     - montecarlo_train_tries
  #     - montecarlo_test_tries
  #     - mixed_probs_coeff
  #     - anneal_rate
  #     - min_rate
  #     - calibration_epochs
  #     - uncerainty_threshold_percentile
  #   grid_search_mode: exhaustive

  # - architecture: 'MCIntCEM'
  #   run_name: "MCMixIntCEM_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "MCMixIntCEM_{pooling_mode}_50_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1] #, 10]
  #   emb_size: [16] #, 32]
  #   shared_emb_generator: True
  #   montecarlo_train_tries: [10]
  #   montecarlo_test_tries: [50]
  #   ood_dropout_prob: [0.5, 0.75, 0.25]
  #   pooling_mode: ['concat']
  #   calibration_epochs: [0]
  #   finetune_with_val: [True]
  #   learnable_temps: False
  #   class_wise_temperature: False
  #   global_epochs: [75, 0]
  #   max_epochs: 75
  #   uncerainty_threshold_percentile: [50, 5, 95]
  #   mixed_probs_coeff: [0, 0.5]
  #   inference_threshold: True
  #   global_cem_only_pass: True # CHANGE FOR GLOBAL!

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]

  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - ood_dropout_prob
  #     - finetune_with_val
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - global_epochs
  #     - montecarlo_train_tries
  #     - montecarlo_test_tries
  #     - mixed_probs_coeff
  #     - calibration_epochs
  #     - uncerainty_threshold_percentile
  #   grid_search_mode: exhaustive



  # # Attempting to have a stable model with some pre-training of the global
  # # configs
  # - architecture: 'CertificateConceptEmbeddingModel'
  #   run_name: "FinalMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "FinalMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1]
  #   emb_size: [16]
  #   temperature: [10, 1]
  #   max_temperature: 10
  #   shared_emb_generator: True

  #   # OOD stuff
  #   certificate_loss_weight: [0]
  #   ood_dropout_prob: [0]
  #   global_ood_prob: [0.5]
  #   entire_global_prob: [0]
  #   pooling_mode: ['iss']
  #   selection_mode: ['max_class_confidence']
  #   hard_eval_selection: [null, 0.5]
  #   selection_sample: [False]
  #   calibration_epochs: [0, 30]
  #   mixed_probs: True
  #   contrastive_reg: 0
  #   calibrate_dynamic_logits: True
  #   calibrate_global_logits: True
  #   finetune_with_val: [True, False]
  #   init_dyn_temps: 1.5
  #   init_global_temps: 0.5
  #   global_temp_reg: 0
  #   inference_dyn_prob: False
  #   learnable_temps: False
  #   positive_calibration: False
  #   class_wise_temperature: False
  #   separate_calibration: False
  #   freeze_global_components: [False]
  #   global_epochs: [1, 5]
  #   print_eval_only: True
  #   counter_limit: 0

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]

  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - certificate_loss_weight
  #     - ood_dropout_prob
  #     - selection_mode
  #     - hard_eval_selection
  #     - selection_sample
  #     - global_ood_prob
  #     - entire_global_prob
  #     - temperature
  #     - finetune_with_val
  #     - calibration_epochs
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - freeze_global_components
  #     - global_epochs
  #   grid_search_mode: exhaustive

  # # Same as above but symmetric temperatures
  # - architecture: 'CertificateConceptEmbeddingModel'
  #   run_name: "FinalMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "FinalMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1]
  #   emb_size: [16]
  #   temperature: [10, 1]
  #   max_temperature: 10
  #   shared_emb_generator: True

  #   # OOD stuff
  #   certificate_loss_weight: [0]
  #   ood_dropout_prob: [0]
  #   global_ood_prob: [0.5]
  #   entire_global_prob: [0]
  #   pooling_mode: ['iss']
  #   selection_mode: ['max_class_confidence']
  #   hard_eval_selection: [null, 0.5]
  #   selection_sample: [False]
  #   calibration_epochs: [0, 30]
  #   mixed_probs: True
  #   contrastive_reg: 0
  #   calibrate_dynamic_logits: True
  #   calibrate_global_logits: True
  #   finetune_with_val: [True, False]
  #   init_dyn_temps: 1
  #   init_global_temps: 1
  #   global_temp_reg: 0
  #   inference_dyn_prob: False
  #   learnable_temps: False
  #   positive_calibration: False
  #   class_wise_temperature: False
  #   separate_calibration: False
  #   freeze_global_components: [False]
  #   global_epochs: [1, 5]
  #   print_eval_only: True
  #   counter_limit: 0

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]

  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - certificate_loss_weight
  #     - ood_dropout_prob
  #     - selection_mode
  #     - hard_eval_selection
  #     - selection_sample
  #     - global_ood_prob
  #     - entire_global_prob
  #     - temperature
  #     - finetune_with_val
  #     - calibration_epochs
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - freeze_global_components
  #     - global_epochs
  #   grid_search_mode: exhaustive




  #############################################################################################
  # UNCOMMENT ME TO TRY IT OUT!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #############################################################################################
  # # Let's see if with a globally pre-trained model, calibration, and random triggering of selection, we can get rid of the asymmetric temperature
  # - architecture: 'CertificateConceptEmbeddingModel'
  #   run_name: "NewMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_rs_{random_selection_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "NewMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_rs_{random_selection_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1]
  #   emb_size: [16]
  #   temperature: [1]
  #   max_temperature: 1
  #   shared_emb_generator: True

  #   # OOD stuff
  #   certificate_loss_weight: [0]
  #   ood_dropout_prob: [0]
  #   global_ood_prob: [0]
  #   entire_global_prob: [0]
  #   random_selection_prob: [0.5]
  #   pooling_mode: ['iss']
  #   selection_mode: ['max_class_confidence']
  #   hard_eval_selection: [null, 0.5]
  #   selection_sample: [False]
  #   calibration_epochs: [0, 30]
  #   mixed_probs: True
  #   contrastive_reg: 0
  #   calibrate_dynamic_logits: True
  #   calibrate_global_logits: True
  #   finetune_with_val: [True]
  #   init_dyn_temps: 1
  #   init_global_temps: 1
  #   global_temp_reg: 0
  #   inference_dyn_prob: False
  #   learnable_temps: False
  #   positive_calibration: False
  #   class_wise_temperature: False
  #   separate_calibration: False
  #   freeze_global_components: [False]
  #   global_epochs: [50, 0, 5, 30]
  #   print_eval_only: True
  #   counter_limit: 0

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]

  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - certificate_loss_weight
  #     - ood_dropout_prob
  #     - selection_mode
  #     - selection_sample
  #     - global_ood_prob
  #     - entire_global_prob
  #     - temperature
  #     - finetune_with_val
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - freeze_global_components
  #     - global_epochs
  #     - random_selection_prob
  #     - hard_eval_selection
  #     - calibration_epochs
  #   grid_search_mode: exhaustive

  # # Make the original temperature symmetric
  # - architecture: 'CertificateConceptEmbeddingModel'
  #   run_name: "FinalMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "FinalMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1]
  #   emb_size: [16]
  #   temperature: [10]
  #   max_temperature: 10
  #   shared_emb_generator: True

  #   # OOD stuff
  #   certificate_loss_weight: [0]
  #   ood_dropout_prob: [0]
  #   global_ood_prob: [0.5]
  #   entire_global_prob: [0]
  #   pooling_mode: ['iss']
  #   selection_mode: ['max_class_confidence']
  #   hard_eval_selection: [null, 0.5]
  #   selection_sample: [False]
  #   calibration_epochs: [30]
  #   mixed_probs: True
  #   contrastive_reg: 0
  #   calibrate_dynamic_logits: True
  #   calibrate_global_logits: True
  #   finetune_with_val: [True]
  #   init_dyn_temps: 1
  #   init_global_temps: 1
  #   global_temp_reg: 0
  #   inference_dyn_prob: False
  #   learnable_temps: False
  #   positive_calibration: False
  #   class_wise_temperature: False
  #   separate_calibration: False
  #   freeze_global_components: [False]
  #   global_epochs: 0
  #   print_eval_only: True
  #   counter_limit: 0

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]

  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - certificate_loss_weight
  #     - ood_dropout_prob
  #     - selection_mode
  #     - hard_eval_selection
  #     - selection_sample
  #     - global_ood_prob
  #     - entire_global_prob
  #     - temperature
  #     - finetune_with_val
  #     - calibration_epochs
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - freeze_global_components
  #   grid_search_mode: exhaustive



  # - architecture: 'CertificateConceptEmbeddingModel'
  #   run_name: "FinalMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "FinalMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1]
  #   emb_size: [16]
  #   temperature: [10, 1]
  #   max_temperature: 10
  #   shared_emb_generator: True

  #   # OOD stuff
  #   certificate_loss_weight: [0]
  #   ood_dropout_prob: [0]
  #   global_ood_prob: [0.5]
  #   entire_global_prob: [0]
  #   pooling_mode: ['iss']
  #   selection_mode: ['max_class_confidence']
  #   hard_eval_selection: [null, 0.5]
  #   selection_sample: [False]
  #   calibration_epochs: [0, 30]
  #   mixed_probs: True
  #   contrastive_reg: 0
  #   calibrate_dynamic_logits: True
  #   calibrate_global_logits: True
  #   finetune_with_val: [True]
  #   init_dyn_temps: 0.5
  #   init_global_temps: 1.5
  #   global_temp_reg: 0
  #   inference_dyn_prob: False
  #   learnable_temps: False
  #   positive_calibration: False
  #   class_wise_temperature: False
  #   separate_calibration: False
  #   freeze_global_components: [False]
  #   global_epochs: 0
  #   print_eval_only: True
  #   counter_limit: 0

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]

  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - certificate_loss_weight
  #     - ood_dropout_prob
  #     - selection_mode
  #     - hard_eval_selection
  #     - selection_sample
  #     - global_ood_prob
  #     - entire_global_prob
  #     - temperature
  #     - finetune_with_val
  #     - calibration_epochs
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - freeze_global_components
  #   grid_search_mode: exhaustive









#   - architecture: 'CertificateConceptEmbeddingModel'
#     run_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
#     load_path_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

#     concept_loss_weight: [1]
#     emb_size: [16]
#     temperature: [10, 1]
#     max_temperature: 10
#     shared_emb_generator: True

#     # OOD stuff
#     certificate_loss_weight: [0]
#     ood_dropout_prob: [0]
#     global_ood_prob: [0]
#     entire_global_prob: [0.5, 0.25, 0.75, 0]
#     pooling_mode: ['iss']
#     selection_mode: ['max_class_confidence']
#     hard_eval_selection: [null, 0.5]
#     selection_sample: [False]
#     calibration_epochs: [0, 30]
#     mixed_probs: True
#     contrastive_reg: 0
#     calibrate_dynamic_logits: True
#     calibrate_global_logits: True
#     finetune_with_val: [True]
#     init_dyn_temps: 1.5
#     init_global_temps: 0.5
#     global_temp_reg: 0
#     inference_dyn_prob: False
#     learnable_temps: False
#     positive_calibration: True
#     class_wise_temperature: False
#     separate_calibration: True
#     # patience: 2
#     # lr_scheduler_patience: 5
#     # check_val_every_n_epoch: 5
#     freeze_global_components: [False]
#     global_epochs: 0
#     max_epochs: 150
#     print_eval_only: True
#     counter_limit: 0

#     ###############################################################################################

#     # IntCEM stuff
#     embedding_activation: null
#     training_intervention_prob: [0.25]
#     intervention_task_discount: [1.1]
#     use_concept_groups: True
#     int_model_use_bn: True
#     int_model_layers: [128, 128, 64, 64]
#     max_horizon: 6
#     horizon_rate: 1.005
#     intervention_weight: [0.1]

#     grid_variables:
#       - training_intervention_prob
#       - intervention_task_discount
#       - emb_size
#       - certificate_loss_weight
#       - ood_dropout_prob
#       - selection_mode
#       - hard_eval_selection
#       - selection_sample
#       - global_ood_prob
#       - entire_global_prob
#       - temperature
#       - finetune_with_val
#       - calibration_epochs
#       - concept_loss_weight
#       - pooling_mode
#       - intervention_weight
#       - freeze_global_components
#     grid_search_mode: exhaustive


#   - architecture: 'CertificateConceptEmbeddingModel'
#     run_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
#     load_path_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

#     concept_loss_weight: [1]
#     emb_size: [16]
#     temperature: [10]
#     max_temperature: 10
#     shared_emb_generator: True

#     # OOD stuff
#     certificate_loss_weight: [0]
#     ood_dropout_prob: [0]
#     global_ood_prob: [0]
#     entire_global_prob: [0.5]
#     pooling_mode: ['iss']
#     selection_mode: ['max_class_confidence']
#     hard_eval_selection: [null, 0.5]
#     selection_sample: [False]
#     calibration_epochs: [0, 30]
#     mixed_probs: True
#     contrastive_reg: 0
#     calibrate_dynamic_logits: True
#     calibrate_global_logits: True
#     finetune_with_val: [True]
#     init_dyn_temps: 1.5
#     init_global_temps: 0.5
#     global_temp_reg: 0
#     inference_dyn_prob: False
#     learnable_temps: False
#     positive_calibration: True
#     class_wise_temperature: True
#     separate_calibration: False
#     freeze_global_components: [False]
#     global_epochs: 0
#     max_epochs: 150
#     print_eval_only: True
#     counter_limit: 0

#     ###############################################################################################

#     # IntCEM stuff
#     embedding_activation: null
#     training_intervention_prob: [0.25]
#     intervention_task_discount: [1.1]
#     use_concept_groups: True
#     int_model_use_bn: True
#     int_model_layers: [128, 128, 64, 64]
#     max_horizon: 6
#     horizon_rate: 1.005
#     intervention_weight: [0.1]

#     grid_variables:
#       - training_intervention_prob
#       - intervention_task_discount
#       - emb_size
#       - certificate_loss_weight
#       - ood_dropout_prob
#       - selection_mode
#       - hard_eval_selection
#       - selection_sample
#       - global_ood_prob
#       - entire_global_prob
#       - temperature
#       - finetune_with_val
#       - calibration_epochs
#       - concept_loss_weight
#       - pooling_mode
#       - intervention_weight
#       - freeze_global_components
#     grid_search_mode: exhaustive



#   # - architecture: 'CertificateConceptEmbeddingModel'
#   #   run_name: "MixIntCEM_hard_sel_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
#   #   load_path_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

#   #   concept_loss_weight: [1]
#   #   emb_size: [16]
#   #   temperature: [10]
#   #   max_temperature: 10
#   #   shared_emb_generator: True

#   #   # OOD stuff
#   #   certificate_loss_weight: [0]
#   #   ood_dropout_prob: [0]
#   #   global_ood_prob: [0]
#   #   entire_global_prob: [0.5]
#   #   pooling_mode: ['iss']
#   #   selection_mode: ['max_class_confidence']
#   #   hard_eval_selection: [null, 0.5]
#   #   selection_sample: [False]
#   #   calibration_epochs: [0, 30]
#   #   mixed_probs: True
#   #   contrastive_reg: 0
#   #   calibrate_dynamic_logits: True
#   #   calibrate_global_logits: True
#   #   finetune_with_val: [True]
#   #   init_dyn_temps: 1.5
#   #   init_global_temps: 0.5
#   #   global_temp_reg: 0
#   #   inference_dyn_prob: False
#   #   learnable_temps: False
#   #   positive_calibration: True
#   #   class_wise_temperature: True
#   #   separate_calibration: False
#   #   freeze_global_components: [False]
#   #   global_epochs: 0
#   #   max_epochs: 150
#   #   print_eval_only: True
#   #   counter_limit: 0
#   #   threshold_probs: null
#   #   hard_selection_value: 1 # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

#   #   ###############################################################################################

#   #   # IntCEM stuff
#   #   embedding_activation: null
#   #   training_intervention_prob: [0.25]
#   #   intervention_task_discount: [1.1]
#   #   use_concept_groups: True
#   #   int_model_use_bn: True
#   #   int_model_layers: [128, 128, 64, 64]
#   #   max_horizon: 6
#   #   horizon_rate: 1.005
#   #   intervention_weight: [0.1]

#   #   grid_variables:
#   #     - training_intervention_prob
#   #     - intervention_task_discount
#   #     - emb_size
#   #     - certificate_loss_weight
#   #     - ood_dropout_prob
#   #     - selection_mode
#   #     - hard_eval_selection
#   #     - selection_sample
#   #     - global_ood_prob
#   #     - entire_global_prob
#   #     - temperature
#   #     - finetune_with_val
#   #     - calibration_epochs
#   #     - concept_loss_weight
#   #     - pooling_mode
#   #     - intervention_weight
#   #     - freeze_global_components
#   #   grid_search_mode: exhaustive


#   - architecture: 'CertificateConceptEmbeddingModel'
#     run_name: "MixIntCEM_thresh_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
#     load_path_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

#     concept_loss_weight: [1]
#     emb_size: [16]
#     temperature: [10]
#     max_temperature: 10
#     shared_emb_generator: True

#     # OOD stuff
#     certificate_loss_weight: [0]
#     ood_dropout_prob: [0]
#     global_ood_prob: [0]
#     entire_global_prob: [0.5]
#     pooling_mode: ['iss']
#     selection_mode: ['max_class_confidence']
#     hard_eval_selection: [null, 0.5]
#     selection_sample: [False]
#     calibration_epochs: [0, 30]
#     mixed_probs: True
#     contrastive_reg: 0
#     calibrate_dynamic_logits: True
#     calibrate_global_logits: True
#     finetune_with_val: [True]
#     init_dyn_temps: 1.5
#     init_global_temps: 0.5
#     global_temp_reg: 0
#     inference_dyn_prob: False
#     learnable_temps: False
#     positive_calibration: True
#     class_wise_temperature: True
#     separate_calibration: False
#     freeze_global_components: [False]
#     global_epochs: 0
#     max_epochs: 150
#     print_eval_only: True
#     counter_limit: 0
#     threshold_probs: 0.5 # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#     hard_selection_value: null

#     ###############################################################################################

#     # IntCEM stuff
#     embedding_activation: null
#     training_intervention_prob: [0.25]
#     intervention_task_discount: [1.1]
#     use_concept_groups: True
#     int_model_use_bn: True
#     int_model_layers: [128, 128, 64, 64]
#     max_horizon: 6
#     horizon_rate: 1.005
#     intervention_weight: [0.1]

#     grid_variables:
#       - training_intervention_prob
#       - intervention_task_discount
#       - emb_size
#       - certificate_loss_weight
#       - ood_dropout_prob
#       - selection_mode
#       - hard_eval_selection
#       - selection_sample
#       - global_ood_prob
#       - entire_global_prob
#       - temperature
#       - finetune_with_val
#       - calibration_epochs
#       - concept_loss_weight
#       - pooling_mode
#       - intervention_weight
#       - freeze_global_components
#     grid_search_mode: exhaustive



#   - architecture: 'CertificateConceptEmbeddingModel'
#     run_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
#     load_path_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

#     concept_loss_weight: [1]
#     emb_size: [16]
#     temperature: [10]
#     max_temperature: 10
#     shared_emb_generator: True

#     # OOD stuff
#     certificate_loss_weight: [0]
#     ood_dropout_prob: [0]
#     global_ood_prob: [0.5]
#     entire_global_prob: [0]
#     pooling_mode: ['iss']
#     selection_mode: ['max_class_confidence']
#     hard_eval_selection: [null, 0.5, 0.05, 0.95, 0.01, 0.99] #, 0.001]
#     selection_sample: [False]
#     calibration_epochs: [0, 30]
#     mixed_probs: True
#     contrastive_reg: 0
#     calibrate_dynamic_logits: True
#     calibrate_global_logits: True
#     finetune_with_val: [True]
#     init_dyn_temps: 1.5
#     init_global_temps: 0.5
#     global_temp_reg: 0
#     inference_dyn_prob: False
#     learnable_temps: False
#     positive_calibration: True
#     class_wise_temperature: True
#     separate_calibration: False
#     freeze_global_components: [False]
#     global_epochs: 0
#     max_epochs: 150
#     print_eval_only: True
#     counter_limit: 0

#     ###############################################################################################

#     # IntCEM stuff
#     embedding_activation: null
#     training_intervention_prob: [0.25]
#     intervention_task_discount: [1.1]
#     use_concept_groups: True
#     int_model_use_bn: True
#     int_model_layers: [128, 128, 64, 64]
#     max_horizon: 6
#     horizon_rate: 1.005
#     intervention_weight: [0.1]

#     grid_variables:
#       - training_intervention_prob
#       - intervention_task_discount
#       - emb_size
#       - certificate_loss_weight
#       - ood_dropout_prob
#       - selection_mode
#       - hard_eval_selection
#       - selection_sample
#       - global_ood_prob
#       - entire_global_prob
#       - temperature
#       - finetune_with_val
#       - calibration_epochs
#       - concept_loss_weight
#       - pooling_mode
#       - intervention_weight
#       - freeze_global_components
#     grid_search_mode: exhaustive



#   - architecture: 'CertificateConceptEmbeddingModel'
#     run_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_hts_{hard_train_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
#     load_path_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

#     concept_loss_weight: [1]
#     emb_size: [16]
#     temperature: [10]
#     max_temperature: 10
#     shared_emb_generator: True

#     # OOD stuff
#     certificate_loss_weight: [0]
#     ood_dropout_prob: [0]
#     global_ood_prob: [0.5]
#     entire_global_prob: [0]
#     pooling_mode: ['iss']
#     selection_mode: ['max_class_confidence']
#     hard_eval_selection: [null]
#     hard_train_selection: [0.01, 0.5, 0.05, 0.95, 0.99, 0.001] #, 0.999, 0.0001, 0.9999, 0.00001]
#     selection_sample: [False]
#     calibration_epochs: [30]
#     mixed_probs: True
#     contrastive_reg: 0
#     calibrate_dynamic_logits: True
#     calibrate_global_logits: True
#     finetune_with_val: [True]
#     init_dyn_temps: 1.5
#     init_global_temps: 0.5
#     global_temp_reg: 0
#     inference_dyn_prob: False
#     learnable_temps: False
#     positive_calibration: True
#     class_wise_temperature: True
#     separate_calibration: False
#     freeze_global_components: [False]
#     global_epochs: 0
#     max_epochs: 150
#     print_eval_only: True
#     counter_limit: 0

#     ###############################################################################################

#     # IntCEM stuff
#     embedding_activation: null
#     training_intervention_prob: [0.25]
#     intervention_task_discount: [1.1]
#     use_concept_groups: True
#     int_model_use_bn: True
#     int_model_layers: [128, 128, 64, 64]
#     max_horizon: 6
#     horizon_rate: 1.005
#     intervention_weight: [0.1]

#     grid_variables:
#       - training_intervention_prob
#       - intervention_task_discount
#       - emb_size
#       - certificate_loss_weight
#       - ood_dropout_prob
#       - selection_mode
#       - hard_eval_selection
#       - hard_train_selection
#       - selection_sample
#       - global_ood_prob
#       - entire_global_prob
#       - temperature
#       - finetune_with_val
#       - calibration_epochs
#       - concept_loss_weight
#       - pooling_mode
#       - intervention_weight
#       - freeze_global_components
#     grid_search_mode: exhaustive



#   - architecture: 'CertificateConceptEmbeddingModel'
#     run_name: "MixIntCEM_tt_{train_prob_thresh}_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_hts_{hard_train_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
#     load_path_name: "MixIntCEM_tt_{train_prob_thresh}_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_hts_None_ce_0_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

#     concept_loss_weight: [1]
#     emb_size: [16]
#     temperature: [10]
#     max_temperature: 10
#     shared_emb_generator: True

#     # OOD stuff
#     certificate_loss_weight: [0]
#     ood_dropout_prob: [0]
#     global_ood_prob: [0.5]
#     entire_global_prob: [0]
#     pooling_mode: ['iss']
#     selection_mode: ['max_class_confidence']
#     hard_eval_selection: [null]
#     hard_train_selection: [null]
#     train_prob_thresh: [0.01, 0.05, 0.5]
#     selection_sample: [False]
#     calibration_epochs: [0, 30]
#     mixed_probs: True
#     contrastive_reg: 0
#     calibrate_dynamic_logits: True
#     calibrate_global_logits: True
#     finetune_with_val: [True]
#     init_dyn_temps: 1.5
#     init_global_temps: 0.5
#     global_temp_reg: 0
#     inference_dyn_prob: False
#     learnable_temps: False
#     positive_calibration: True
#     class_wise_temperature: True
#     separate_calibration: False
#     freeze_global_components: [False]
#     global_epochs: 0
#     max_epochs: 150
#     print_eval_only: True
#     counter_limit: 0

#     ###############################################################################################

#     # IntCEM stuff
#     embedding_activation: null
#     training_intervention_prob: [0.25]
#     intervention_task_discount: [1.1]
#     use_concept_groups: True
#     int_model_use_bn: True
#     int_model_layers: [128, 128, 64, 64]
#     max_horizon: 6
#     horizon_rate: 1.005
#     intervention_weight: [0.1]

#     grid_variables:
#       - training_intervention_prob
#       - intervention_task_discount
#       - emb_size
#       - certificate_loss_weight
#       - ood_dropout_prob
#       - selection_mode
#       - hard_eval_selection
#       - hard_train_selection
#       - selection_sample
#       - global_ood_prob
#       - entire_global_prob
#       - temperature
#       - finetune_with_val
#       - concept_loss_weight
#       - pooling_mode
#       - intervention_weight
#       - freeze_global_components
#       - train_prob_thresh
#       - calibration_epochs
#     grid_search_mode: exhaustive




#   - architecture: 'CertificateConceptEmbeddingModel'
#     run_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
#     load_path_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_True_True_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

#     concept_loss_weight: [1]
#     emb_size: [16]
#     temperature: [10]
#     max_temperature: 10
#     shared_emb_generator: True

#     # OOD stuff
#     certificate_loss_weight: [0]
#     ood_dropout_prob: [0]
#     global_ood_prob: [0.5]
#     entire_global_prob: [0]
#     pooling_mode: ['iss']
#     selection_mode: ['max_class_confidence']
#     hard_eval_selection: [null]
#     selection_sample: [False]
#     calibration_epochs: [30]
#     mixed_probs: True
#     contrastive_reg: 0
#     calibrate_dynamic_logits: False # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#     calibrate_global_logits: True
#     finetune_with_val: [True]
#     init_dyn_temps: 1.5
#     init_global_temps: 0.5
#     global_temp_reg: 0
#     inference_dyn_prob: False
#     learnable_temps: False
#     positive_calibration: True
#     class_wise_temperature: True
#     separate_calibration: False
#     freeze_global_components: [False]
#     global_epochs: 0
#     max_epochs: 150
#     print_eval_only: True
#     counter_limit: 0

#     ###############################################################################################

#     # IntCEM stuff
#     embedding_activation: null
#     training_intervention_prob: [0.25]
#     intervention_task_discount: [1.1]
#     use_concept_groups: True
#     int_model_use_bn: True
#     int_model_layers: [128, 128, 64, 64]
#     max_horizon: 6
#     horizon_rate: 1.005
#     intervention_weight: [0.1]

#     grid_variables:
#       - training_intervention_prob
#       - intervention_task_discount
#       - emb_size
#       - certificate_loss_weight
#       - ood_dropout_prob
#       - selection_mode
#       - hard_eval_selection
#       - selection_sample
#       - global_ood_prob
#       - entire_global_prob
#       - temperature
#       - finetune_with_val
#       - calibration_epochs
#       - concept_loss_weight
#       - pooling_mode
#       - intervention_weight
#       - freeze_global_components
#     grid_search_mode: exhaustive


#   - architecture: 'CertificateConceptEmbeddingModel'
#     run_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
#     load_path_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_True_True_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

#     concept_loss_weight: [1]
#     emb_size: [16]
#     temperature: [10]
#     max_temperature: 10
#     shared_emb_generator: True

#     # OOD stuff
#     certificate_loss_weight: [0]
#     ood_dropout_prob: [0]
#     global_ood_prob: [0.5]
#     entire_global_prob: [0]
#     pooling_mode: ['iss']
#     selection_mode: ['max_class_confidence']
#     hard_eval_selection: [null]
#     selection_sample: [False]
#     calibration_epochs: [30]
#     mixed_probs: True
#     contrastive_reg: 0
#     calibrate_dynamic_logits: True
#     calibrate_global_logits: False # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#     finetune_with_val: [True]
#     init_dyn_temps: 1.5
#     init_global_temps: 0.5
#     global_temp_reg: 0
#     inference_dyn_prob: False
#     learnable_temps: False
#     positive_calibration: True
#     class_wise_temperature: True
#     separate_calibration: False
#     freeze_global_components: [False]
#     global_epochs: 0
#     max_epochs: 150
#     print_eval_only: True
#     counter_limit: 0

#     ###############################################################################################

#     # IntCEM stuff
#     embedding_activation: null
#     training_intervention_prob: [0.25]
#     intervention_task_discount: [1.1]
#     use_concept_groups: True
#     int_model_use_bn: True
#     int_model_layers: [128, 128, 64, 64]
#     max_horizon: 6
#     horizon_rate: 1.005
#     intervention_weight: [0.1]

#     grid_variables:
#       - training_intervention_prob
#       - intervention_task_discount
#       - emb_size
#       - certificate_loss_weight
#       - ood_dropout_prob
#       - selection_mode
#       - hard_eval_selection
#       - selection_sample
#       - global_ood_prob
#       - entire_global_prob
#       - temperature
#       - finetune_with_val
#       - calibration_epochs
#       - concept_loss_weight
#       - pooling_mode
#       - intervention_weight
#       - freeze_global_components
#     grid_search_mode: exhaustive


#   - architecture: 'CertificateConceptEmbeddingModel'
#     run_name: "MixIntCEM_hard_sel_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
#     load_path_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

#     concept_loss_weight: [1]
#     emb_size: [16]
#     temperature: [10]
#     max_temperature: 10
#     shared_emb_generator: True

#     # OOD stuff
#     certificate_loss_weight: [0]
#     ood_dropout_prob: [0]
#     global_ood_prob: [0.5]
#     entire_global_prob: [0]
#     pooling_mode: ['iss']
#     selection_mode: ['max_class_confidence']
#     hard_eval_selection: [null, 0.5]
#     selection_sample: [False]
#     calibration_epochs: [0, 30]
#     mixed_probs: True
#     contrastive_reg: 0
#     calibrate_dynamic_logits: True
#     calibrate_global_logits: True
#     finetune_with_val: [True]
#     init_dyn_temps: 1.5
#     init_global_temps: 0.5
#     global_temp_reg: 0
#     inference_dyn_prob: False
#     learnable_temps: False
#     positive_calibration: True
#     class_wise_temperature: True
#     separate_calibration: False
#     freeze_global_components: [False]
#     global_epochs: 0
#     max_epochs: 150
#     print_eval_only: True
#     counter_limit: 0
#     threshold_probs: null
#     hard_selection_value: 1 # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

#     ###############################################################################################

#     # IntCEM stuff
#     embedding_activation: null
#     training_intervention_prob: [0.25]
#     intervention_task_discount: [1.1]
#     use_concept_groups: True
#     int_model_use_bn: True
#     int_model_layers: [128, 128, 64, 64]
#     max_horizon: 6
#     horizon_rate: 1.005
#     intervention_weight: [0.1]

#     grid_variables:
#       - training_intervention_prob
#       - intervention_task_discount
#       - emb_size
#       - certificate_loss_weight
#       - ood_dropout_prob
#       - selection_mode
#       - hard_eval_selection
#       - selection_sample
#       - global_ood_prob
#       - entire_global_prob
#       - temperature
#       - finetune_with_val
#       - calibration_epochs
#       - concept_loss_weight
#       - pooling_mode
#       - intervention_weight
#       - freeze_global_components
#     grid_search_mode: exhaustive


#   - architecture: 'CertificateConceptEmbeddingModel'
#     run_name: "MixIntCEM_loss_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_gpr_{global_prediction_reg}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
#     load_path_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_gpr_{global_prediction_reg}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

#     concept_loss_weight: [1]
#     global_prediction_reg: [1, 0.1, 10]  # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#     emb_size: [16]
#     temperature: [10]
#     max_temperature: 10
#     shared_emb_generator: True

#     # OOD stuff
#     certificate_loss_weight: [0]
#     ood_dropout_prob: [0]
#     global_ood_prob: [0]
#     entire_global_prob: [0]
#     pooling_mode: ['iss']
#     selection_mode: ['max_class_confidence']
#     hard_eval_selection: [null, 0.5]
#     selection_sample: [False]
#     calibration_epochs: [0, 30]
#     mixed_probs: True
#     contrastive_reg: 0
#     calibrate_dynamic_logits: True
#     calibrate_global_logits: True
#     finetune_with_val: [True]
#     init_dyn_temps: 1.5
#     init_global_temps: 0.5
#     global_temp_reg: 0
#     inference_dyn_prob: False
#     learnable_temps: False
#     positive_calibration: True
#     class_wise_temperature: True  # KEY!
#     separate_calibration: False
#     freeze_global_components: [False]
#     global_epochs: 0
#     max_epochs: 150
#     print_eval_only: True
#     counter_limit: 0

#     ###############################################################################################

#     # IntCEM stuff
#     embedding_activation: null
#     training_intervention_prob: [0.25]
#     intervention_task_discount: [1.1]
#     use_concept_groups: True
#     int_model_use_bn: True
#     int_model_layers: [128, 128, 64, 64]
#     max_horizon: 6
#     horizon_rate: 1.005
#     intervention_weight: [0.1]

#     grid_variables:
#       - training_intervention_prob
#       - intervention_task_discount
#       - emb_size
#       - certificate_loss_weight
#       - ood_dropout_prob
#       - selection_mode
#       - hard_eval_selection
#       - selection_sample
#       - global_ood_prob
#       - entire_global_prob
#       - temperature
#       - finetune_with_val
#       - calibration_epochs
#       - concept_loss_weight
#       - pooling_mode
#       - intervention_weight
#       - freeze_global_components
#       - global_prediction_reg
#     grid_search_mode: exhaustive




#   - architecture: 'CertificateConceptEmbeddingModel'
#     run_name: "MixIntCEM_thresh_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
#     load_path_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

#     concept_loss_weight: [1]
#     emb_size: [16]
#     temperature: [10]
#     max_temperature: 10
#     shared_emb_generator: True

#     # OOD stuff
#     certificate_loss_weight: [0]
#     ood_dropout_prob: [0]
#     global_ood_prob: [0.5]
#     entire_global_prob: [0]
#     pooling_mode: ['iss']
#     selection_mode: ['max_class_confidence']
#     hard_eval_selection: [null, 0.5]
#     selection_sample: [False]
#     calibration_epochs: [0, 30]
#     mixed_probs: True
#     contrastive_reg: 0
#     calibrate_dynamic_logits: True
#     calibrate_global_logits: True
#     finetune_with_val: [True]
#     init_dyn_temps: 1.5
#     init_global_temps: 0.5
#     global_temp_reg: 0
#     inference_dyn_prob: False
#     learnable_temps: False
#     positive_calibration: True
#     class_wise_temperature: True
#     separate_calibration: False
#     freeze_global_components: [False]
#     global_epochs: 0
#     max_epochs: 150
#     print_eval_only: True
#     counter_limit: 0
#     threshold_probs: 0.5
#     hard_selection_value: null

#     ###############################################################################################

#     # IntCEM stuff
#     embedding_activation: null
#     training_intervention_prob: [0.25]
#     intervention_task_discount: [1.1]
#     use_concept_groups: True
#     int_model_use_bn: True
#     int_model_layers: [128, 128, 64, 64]
#     max_horizon: 6
#     horizon_rate: 1.005
#     intervention_weight: [0.1]

#     grid_variables:
#       - training_intervention_prob
#       - intervention_task_discount
#       - emb_size
#       - certificate_loss_weight
#       - ood_dropout_prob
#       - selection_mode
#       - hard_eval_selection
#       - selection_sample
#       - global_ood_prob
#       - entire_global_prob
#       - temperature
#       - finetune_with_val
#       - calibration_epochs
#       - concept_loss_weight
#       - pooling_mode
#       - intervention_weight
#       - freeze_global_components
#     grid_search_mode: exhaustive


#   - architecture: 'CertificateConceptEmbeddingModel'
#     run_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
#     load_path_name: "MixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

#     concept_loss_weight: [1]
#     emb_size: [16]
#     temperature: [10, 1]
#     max_temperature: 10
#     shared_emb_generator: True

#     # OOD stuff
#     certificate_loss_weight: [0]
#     ood_dropout_prob: [0]
#     global_ood_prob: [0]
#     entire_global_prob: [0]
#     pooling_mode: ['iss']
#     selection_mode: ['max_class_confidence']
#     hard_eval_selection: [null, 0.5]
#     selection_sample: [False]
#     calibration_epochs: [0, 30]
#     mixed_probs: True
#     contrastive_reg: 0
#     calibrate_dynamic_logits: True
#     calibrate_global_logits: True
#     finetune_with_val: [True]
#     init_dyn_temps: 1.5
#     init_global_temps: 0.5
#     global_temp_reg: 0
#     inference_dyn_prob: False
#     learnable_temps: False
#     positive_calibration: True
#     class_wise_temperature: False
#     separate_calibration: True
#     # patience: 2
#     # lr_scheduler_patience: 5
#     # check_val_every_n_epoch: 5
#     freeze_global_components: [True, False]
#     global_epochs: 100
#     max_epochs: 100
#     print_eval_only: True
#     counter_limit: 0

#     ###############################################################################################

#     # IntCEM stuff
#     embedding_activation: null
#     training_intervention_prob: [0.25]
#     intervention_task_discount: [1.1]
#     use_concept_groups: True
#     int_model_use_bn: True
#     int_model_layers: [128, 128, 64, 64]
#     max_horizon: 6
#     horizon_rate: 1.005
#     intervention_weight: [0.1]

#     grid_variables:
#       - training_intervention_prob
#       - intervention_task_discount
#       - emb_size
#       - certificate_loss_weight
#       - ood_dropout_prob
#       - selection_mode
#       - hard_eval_selection
#       - selection_sample
#       - global_ood_prob
#       - entire_global_prob
#       - temperature
#       - finetune_with_val
#       - calibration_epochs
#       - concept_loss_weight
#       - pooling_mode
#       - intervention_weight
#       - freeze_global_components
#     grid_search_mode: exhaustive



#   - architecture: 'CertificateConceptEmbeddingModel'
#     run_name: "CertificateCEM_{selection_mode}_{pooling_mode}_t_{temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_gr_{global_temp_reg}_g_ood_{global_ood_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
#     load_path_name: "CertificateCEM_{selection_mode}_{pooling_mode}_t_{temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_gr_{global_temp_reg}_g_ood_{global_ood_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

#     concept_loss_weight: [1, 10]
#     emb_size: [16]
#     temperature: [10, 1]
#     shared_emb_generator: True

#     # OOD stuff
#     certificate_loss_weight: [0]
#     ood_dropout_prob: [0]
#     global_ood_prob: [0.5, 0]
#     pooling_mode: ['iss']
#     selection_mode: ['max_class_confidence']
#     hard_eval_selection: [null, 0.5, 0.05, 0.95]
#     selection_sample: [False]
#     calibration_epochs: [0, 30]
#     mixed_probs: True
#     contrastive_reg: 0
#     calibrate_dynamic_logits: True
#     calibrate_global_logits: True
#     init_dyn_temps: 1
#     init_global_temps: 1
#     global_temp_reg: 0
#     inference_dyn_prob: False
#     learnable_temps: False
#     finetune_with_val: True

#     ###############################################################################################

#     # IntCEM stuff
#     embedding_activation: null
#     training_intervention_prob: [0.25]
#     intervention_task_discount: [1.1]
#     use_concept_groups: True
#     int_model_use_bn: True
#     int_model_layers: [128, 128, 64, 64]
#     max_horizon: 6
#     horizon_rate: 1.005
#     gradient_clip_val: 100
#     intervention_weight: 0.1

#     grid_variables:
#       - concept_loss_weight
#       - training_intervention_prob
#       - intervention_task_discount
#       - emb_size
#       - certificate_loss_weight
#       - ood_dropout_prob
#       - selection_sample
#       - pooling_mode
#       - global_ood_prob
#       - temperature
#       - selection_mode
#       - calibration_epochs
#       - hard_eval_selection
#     grid_search_mode: exhaustive


#   # WORKING!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#   # - architecture: 'CertificateConceptEmbeddingModel'
#   #   run_name: "CertificateCEM_{selection_mode}_{pooling_mode}_{calibrate_global_logits}_{calibrate_dynamic_logits}_g_ood_{global_ood_prob}_emb_{emb_size}_ood_{ood_dropout_prob}_mp_{mixed_probs}_creg_{contrastive_reg}_hs_{hard_eval_selection}_cal_epochs_{calibration_epochs}_cw_{certificate_loss_weight}_ss_{selection_sample}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

#   #   concept_loss_weight: [1, 10]
#   #   emb_size: [16]
#   #   temperature: [10, 1]
#   #   shared_emb_generator: True

#   #   # OOD stuff
#   #   certificate_loss_weight: [0] #, 0.1]
#   #   ood_dropout_prob: [0] #, 0.75, 0.5, 0]
#   #   global_ood_prob: [0.5, 0.75, 0.25, 0] #, 0.75, 0.5, 0]
#   #   pooling_mode: ['individual_scores', 'iss']
#   #   selection_mode: ['max_class_confidence']
#   #   hard_eval_selection: [0.5, null, 0.05, 0.95] #0.1, 0.5, 0.05] #, -0.999, 0.999]
#   #   selection_sample: [False]
#   #   calibration_epochs: 30
#   #   mixed_probs: True
#   #   contrastive_reg: 0
#   #   calibrate_dynamic_logits: True
#   #   calibrate_global_logits: True

#   #   ###############################################################################################

#   #   # IntCEM stuff
#   #   embedding_activation: null
#   #   training_intervention_prob: [0.25]
#   #   intervention_task_discount: [1.1]
#   #   use_concept_groups: True
#   #   int_model_use_bn: True
#   #   int_model_layers: [128, 128, 64, 64]
#   #   max_horizon: 6
#   #   horizon_rate: 1.005
#   #   gradient_clip_val: 100
#   #   intervention_weight: 0.1


#   #   grid_variables:
#   #     - concept_loss_weight
#   #     - training_intervention_prob
#   #     - intervention_task_discount
#   #     - emb_size
#   #     - certificate_loss_weight
#   #     - ood_dropout_prob
#   #     - hard_eval_selection
#   #     - selection_sample
#   #     - pooling_mode
#   #     - global_ood_prob
#   #     - temperature
#   #     - selection_mode
#   #   grid_search_mode: exhaustive




#   # - architecture: 'CertificateConceptEmbeddingModel'
#   #   run_name: "CertificateCEM_{selection_mode}_{pooling_mode}_{calibrate_global_logits}_{calibrate_dynamic_logits}_g_ood_{global_ood_prob}_emb_{emb_size}_ood_{ood_dropout_prob}_mp_{mixed_probs}_creg_{contrastive_reg}_hs_{hard_eval_selection}_cal_epochs_{calibration_epochs}_cw_{certificate_loss_weight}_ss_{selection_sample}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

#   #   concept_loss_weight: [1]
#   #   emb_size: [16]
#   #   temperature: [1]
#   #   shared_emb_generator: True

#   #   # OOD stuff
#   #   certificate_loss_weight: [0] #, 0.1]
#   #   ood_dropout_prob: [0] #, 0.75, 0.5, 0]
#   #   global_ood_prob: [0.5, 0.75, 0.25] #, 0.75, 0.5, 0]
#   #   pooling_mode: ['individual_scores']
#   #   selection_mode: ['max_class_confidence']
#   #   hard_eval_selection: [0.5, null, 0.05, 0.95] #0.1, 0.5, 0.05] #, -0.999, 0.999]
#   #   selection_sample: [False]
#   #   calibration_epochs: 30
#   #   mixed_probs: False
#   #   contrastive_reg: [0.1, 1]
#   #   calibrate_dynamic_logits: True
#   #   calibrate_global_logits: True

#   #   ###############################################################################################

#   #   # IntCEM stuff
#   #   embedding_activation: null
#   #   training_intervention_prob: [0.25]
#   #   intervention_task_discount: [1.1]
#   #   use_concept_groups: True
#   #   int_model_use_bn: True
#   #   int_model_layers: [128, 128, 64, 64]
#   #   max_horizon: 6
#   #   horizon_rate: 1.005
#   #   gradient_clip_val: 100
#   #   intervention_weight: 0.1


#   #   grid_variables:
#   #     - concept_loss_weight
#   #     - training_intervention_prob
#   #     - intervention_task_discount
#   #     - temperature
#   #     - emb_size
#   #     - certificate_loss_weight
#   #     - ood_dropout_prob
#   #     - selection_mode
#   #     - hard_eval_selection
#   #     - selection_sample
#   #     - pooling_mode
#   #     - global_ood_prob
#   #     - contrastive_reg
#   #   grid_search_mode: exhaustive

# #   - architecture: 'CertificateConceptEmbeddingModel'
# #     run_name: "CertificateCEM_{selection_mode}_{pooling_mode}_emb_{emb_size}_ood_{ood_dropout_prob}_hs_{hard_eval_selection}_cw_{certificate_loss_weight}_ss_{selection_sample}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

# #     concept_loss_weight: [1]
# #     emb_size: [16]
# #     temperature: [1]
# #     shared_emb_generator: True

# #     # OOD stuff
# #     certificate_loss_weight: [0.01] #, 0.1]
# #     ood_dropout_prob: [0.5, 0]
# #     pooling_mode: 'individual_scores' #'concat'
# #     selection_mode: ['global']
# #     hard_eval_selection: [-0.001, 0.001] #, -0.999, 0.999]
# #     selection_sample: [True, False]

# #     ###############################################################################################

# #     # IntCEM stuff
# #     embedding_activation: null
# #     training_intervention_prob: [0.25]
# #     intervention_task_discount: [1.1]
# #     use_concept_groups: True
# #     int_model_use_bn: True
# #     int_model_layers: [128, 128, 64, 64]
# #     max_horizon: 6
# #     horizon_rate: 1.005
# #     gradient_clip_val: 100
# #     intervention_weight: 0.1


# #     grid_variables:
# #       - concept_loss_weight
# #       - training_intervention_prob
# #       - intervention_task_discount
# #       - emb_size
# #       - temperature
# #       - ood_dropout_prob
# #       - certificate_loss_weight
# #       - selection_mode
# #       - hard_eval_selection
# #       - selection_sample
# #     grid_search_mode: exhaustive





# #   - architecture: 'CertificateConceptEmbeddingModel'
# #     run_name: "CertificateCEM_{selection_mode}_{pooling_mode}_emb_{emb_size}_ood_{ood_dropout_prob}_hs_{hard_eval_selection}_cw_{certificate_loss_weight}_ss_{selection_sample}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

# #     concept_loss_weight: [1]
# #     emb_size: [16]
# #     temperature: [1]
# #     shared_emb_generator: True

# #     # OOD stuff
# #     certificate_loss_weight: [0.01, 0.001] #, 0.1]
# #     ood_dropout_prob: [0.5, 0.75]
# #     pooling_mode: ['individual_scores', 'concat']
# #     selection_mode: ['global', 'individual']
# #     hard_eval_selection: [0.001] #, -0.999, 0.999]
# #     selection_sample: [False]

# #     ###############################################################################################

# #     # IntCEM stuff
# #     embedding_activation: null
# #     training_intervention_prob: [0.25]
# #     intervention_task_discount: [1.1]
# #     use_concept_groups: True
# #     int_model_use_bn: True
# #     int_model_layers: [128, 128, 64, 64]
# #     max_horizon: 6
# #     horizon_rate: 1.005
# #     gradient_clip_val: 100
# #     intervention_weight: 0.1


# #     grid_variables:
# #       - concept_loss_weight
# #       - training_intervention_prob
# #       - intervention_task_discount
# #       - emb_size
# #       - temperature
# #       - ood_dropout_prob
# #       - certificate_loss_weight
# #       - selection_mode
# #       - hard_eval_selection
# #       - selection_sample
# #       - pooling_mode
# #     grid_search_mode: exhaustive





# #   - architecture: 'ProbConceptEmbeddingModel'
# #     run_name: "ProbCEM_{pooling_mode}_{selection_mode}_{separator_mode}_{attention_fn}_sam_{shared_attn_module}_emb_{emb_size}_n_vars_{n_concept_variants}_lce_{learnable_concept_embs}_ood_{ood_dropout_prob}_kl_weight_{kl_loss_weight}_threshold_{threshold}_bt_{box_temperature}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

# #     concept_loss_weight: [1]
# #     emb_size: [16]
# #     n_concept_variants: [10]
# #     temperature: [1]
# #     shared_emb_generator: True
# #     attention_fn: ['hard_gumbel', 'softmax'] #'softmax'
# #     pooling_mode: 'concat'

# #     # OOD stuff
# #     ood_dropout_prob: [0]
# #     selection_mode: 'z_score'
# #     separator_mode: 'softmax'
# #     kl_loss_weight: [0.1, 0.01, 1]
# #     threshold: [1.5]
# #     box_temperature: 50 #10
# #     learnable_concept_embs: False
# #     shared_attn_module: True
# #     global_above_thresh: True

# #     ###############################################################################################

# #     # IntCEM stuff
# #     embedding_activation: null
# #     training_intervention_prob: [0.25]
# #     # intervention_task_discount: [1] # [1.1] # Turned into a standard CEM for now!
# #     intervention_task_discount: [1.1]
# #     use_concept_groups: True
# #     int_model_use_bn: True
# #     int_model_layers: [128, 128, 64, 64]
# #     # max_horizon: 1 # 6 # Turned into a standard CEM for now!
# #     max_horizon: 6
# #     # horizon_rate: 1 # 1.005 # Turned into a standard CEM for now!
# #     horizon_rate: 1.005
# #     # gradient_clip_val: null # 100 # Turned into a standard CEM for now!
# #     gradient_clip_val: 100
# #     # intervention_weight: 0 # 0.1 # Turned into a standard CEM for now!
# #     intervention_weight: 0.1


# #     grid_variables:
# #       - concept_loss_weight
# #       - training_intervention_prob
# #       - intervention_task_discount
# #       - emb_size
# #       - n_concept_variants
# #       - temperature
# #       - ood_dropout_prob
# #       - threshold
# #       - kl_loss_weight
# #       - attention_fn
# #     grid_search_mode: exhaustive


# #   - architecture: 'SeparatorConceptEmbeddingModel'
# #     run_name: "SeparatorCEM_{pooling_mode}_{selection_mode}_{separator_mode}_pd_{projection_dim}_emb_{emb_size}_n_vars_{n_concept_variants}_ood_{ood_dropout_prob}_spl_{sep_loss_weight}_warm_{separator_warmup_steps}_ibv_{init_bound_val}_bt_{box_temperature}_blw_{bounds_loss_weight}_mlw_{margin_loss_weight}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

# #     concept_loss_weight: [1]
# #     emb_size: [16]
# #     n_concept_variants: [1]
# #     temperature: [1]
# #     shared_emb_generator: True
# #     attention_fn: 'softmax'
# #     separator_warmup_steps: 0
# #     pooling_mode: 'concat' #'individual_scores' # concat
# #     init_bound_val: 1 #5
# #     box_temperature: 1 #50

# #     # OOD stuff
# #     ood_dropout_prob: [0]
# #     margin_loss_weight: [0]
# #     bounds_loss_weight: [10, 0.1, 0.01, 0.001]
# #     selection_mode: 'distance'
# #     sep_loss_weight: [0.1, 0.01, 1]
# #     separator_mode: 'softmax'
# #     projection_dim: 16

# #     ###############################################################################################

# #     # IntCEM stuff
# #     embedding_activation: null
# #     training_intervention_prob: [0.25]
# #     # intervention_task_discount: [1] # [1.1] # Turned into a standard CEM for now!
# #     intervention_task_discount: [1.1]
# #     use_concept_groups: True
# #     int_model_use_bn: True
# #     int_model_layers: [128, 128, 64, 64]
# #     # max_horizon: 1 # 6 # Turned into a standard CEM for now!
# #     max_horizon: 6
# #     # horizon_rate: 1 # 1.005 # Turned into a standard CEM for now!
# #     horizon_rate: 1.005
# #     # gradient_clip_val: null # 100 # Turned into a standard CEM for now!
# #     gradient_clip_val: 100
# #     # intervention_weight: 0 # 0.1 # Turned into a standard CEM for now!
# #     intervention_weight: 0.1


# #     grid_variables:
# #       - concept_loss_weight
# #       - training_intervention_prob
# #       - intervention_task_discount
# #       - emb_size
# #       - n_concept_variants
# #       - temperature
# #       - margin_loss_weight
# #       - ood_dropout_prob
# #       - sep_loss_weight
# #       - bounds_loss_weight
# #     grid_search_mode: exhaustive


# #   - architecture: 'SeparatorConceptEmbeddingModel'
# #     run_name: "SeparatorCEM_{pooling_mode}_{selection_mode}_emb_{emb_size}_n_vars_{n_concept_variants}_ood_{ood_dropout_prob}_spl_{sep_loss_weight}_warm_{separator_warmup_steps}_ibv_{init_bound_val}_bt_{box_temperature}_blw_{bounds_loss_weight}_mlw_{margin_loss_weight}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

# #     concept_loss_weight: [1]
# #     emb_size: [16]
# #     n_concept_variants: [1, 5]
# #     temperature: [1]
# #     shared_emb_generator: True
# #     attention_fn: 'softmax'
# #     separator_warmup_steps: 0
# #     pooling_mode: 'concat' #'individual_scores'
# #     init_bound_val: 5
# #     box_temperature: 50

# #     # OOD stuff
# #     ood_dropout_prob: [0, 0.1]
# #     margin_loss_weight: [0]
# #     bounds_loss_weight: 0
# #     selection_mode: 'distance'
# #     sep_loss_weight: [0.1, 0.01, 1]
# #     separator_mode: 'individual'
# #     projection_dim: null

# #     ###############################################################################################

# #     # IntCEM stuff
# #     embedding_activation: null
# #     training_intervention_prob: [0.25]
# #     intervention_task_discount: [1.1]
# #     use_concept_groups: True
# #     int_model_use_bn: True
# #     int_model_layers: [128, 128, 64, 64]
# #     max_horizon: 6
# #     horizon_rate: 1.005
# #     gradient_clip_val: 100
# #     task_concept_loss: 1
# #     intervention_weight: 0.1


# #     grid_variables:
# #       - concept_loss_weight
# #       - training_intervention_prob
# #       - intervention_task_discount
# #       - emb_size
# #       - n_concept_variants
# #       - temperature
# #       - margin_loss_weight
# #       - ood_dropout_prob
# #       - sep_loss_weight
# #     grid_search_mode: exhaustive



# #   - architecture: 'SeparatorConceptEmbeddingModel'
# #     run_name: "SeparatorCEM_emb_{emb_size}_n_vars_{n_concept_variants}_ood_{ood_dropout_prob}_bt_{box_temperature}_blw_{bounds_loss_weight}_mlw_{margin_loss_weight}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

# #     concept_loss_weight: [1]
# #     emb_size: [16]
# #     n_concept_variants: [1]
# #     temperature: [1]
# #     shared_emb_generator: True
# #     attention_fn: 'softmax'
# #     separator_warmup_steps: 0
# #     pooling_mode: 'concat'
# #     init_bound_val: 5

# #     # OOD stuff
# #     ood_dropout_prob: [0, 0.1]
# #     margin_loss_weight: [0]
# #     box_temperature: 100
# #     bounds_loss_weight: 10

# #     ###############################################################################################

# #     # IntCEM stuff
# #     embedding_activation: null
# #     training_intervention_prob: [0.25]
# #     intervention_task_discount: [1.1]
# #     use_concept_groups: True
# #     int_model_use_bn: True
# #     int_model_layers: [128, 128, 64, 64]
# #     max_horizon: 6
# #     horizon_rate: 1.005
# #     gradient_clip_val: 100
# #     task_concept_loss: 1
# #     intervention_weight: 0.1


# #     grid_variables:
# #       - concept_loss_weight
# #       - training_intervention_prob
# #       - intervention_task_discount
# #       - emb_size
# #       - n_concept_variants
# #       - temperature
# #       - margin_loss_weight
# #       - ood_dropout_prob
# #     grid_search_mode: exhaustive



# #   - architecture: 'SeparatorConceptEmbeddingModel'
# #     run_name: "SeparatorCEM_{pooling_mode}_emb_{emb_size}_n_vars_{n_concept_variants}_ood_{ood_dropout_prob}_warm_{separator_warmup_steps}_ibv_{init_bound_val}_bt_{box_temperature}_blw_{bounds_loss_weight}_mlw_{margin_loss_weight}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

# #     concept_loss_weight: [1]
# #     emb_size: [16]
# #     n_concept_variants: [1]
# #     temperature: [1]
# #     shared_emb_generator: True
# #     attention_fn: 'softmax'
# #     separator_warmup_steps: 0
# #     pooling_mode: 'individual_scores'
# #     init_bound_val: 5

# #     # OOD stuff
# #     ood_dropout_prob: [0]
# #     margin_loss_weight: [0]
# #     box_temperature: 100
# #     bounds_loss_weight: 10

# #     ###############################################################################################

# #     # IntCEM stuff
# #     embedding_activation: null
# #     training_intervention_prob: [0.25]
# #     intervention_task_discount: [1.1]
# #     use_concept_groups: True
# #     int_model_use_bn: True
# #     int_model_layers: [128, 128, 64, 64]
# #     max_horizon: 6
# #     horizon_rate: 1.005
# #     gradient_clip_val: 100
# #     task_concept_loss: 1
# #     intervention_weight: 0.1


# #     grid_variables:
# #       - concept_loss_weight
# #       - training_intervention_prob
# #       - intervention_task_discount
# #       - emb_size
# #       - n_concept_variants
# #       - temperature
# #       - margin_loss_weight
# #       - ood_dropout_prob
# #     grid_search_mode: exhaustive



# #   - architecture: 'GlobalApproxConceptEmbeddingModel'
# #     run_name: "NewGlobalApproxCEM_joint_{joint_trained}_emb_{emb_size}_n_vars_{n_concept_variants}_itp_{init_threshold_percentile}_e_{dynamic_epochs}_{approx_epochs}_{finetuning_epochs}_{ood_epochs}_p_{use_dynamic_for_probs}_dl_{distance_l2_loss}_ood_{ood_dropout_prob}_{thresh_l2_loss}_l2_{l2_dist_loss_weight}_freeze_{freeze_underlying_model}_{ood_freeze_underlying_model}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

# #     concept_loss_weight: [1] #, 10]
# #     emb_size: [16]
# #     n_concept_variants: [5, 1, 2, 10] #, 20] #, 50]
# #     temperature: [5, 1]
# #     shared_emb_generator: True
# #     l2_dist_loss_weight: [0] #, 0.1, 0]
# #     attention_fn: 'softmax'
# #     init_threshold_percentile: [5, 10, 25, 50]
# #     approx_prediction_mode: 'new'
# #     mode: ood_beta #ood_same
# #     joint_trained: [True, False]
# #     # global_mixture_components: 10
# #     gmm_epochs: 50
# #     ood_loss_weight: 1


# #     # OOD stuff
# #     ood_dropout_prob: [0.25, 0.1, 0.5]
# #     thresh_l2_loss: [0] #0.0001]
# #     distance_l2_loss: [0]

# #     # Training:
# #     use_dynamic_for_probs: True
# #     freeze_centroids: False
# #     dynamic_epochs: 200
# #     prototypes_epochs: 0
# #     approx_epochs: 0 #100
# #     freeze_underlying_model: True
# #     finetuning_epochs: 0

# #     ood_epochs: 30 #150
# #     ood_freeze_underlying_model: True
# #     ood_freeze_label_predictor: False
# #     ood_freeze_concept_rank_model: False
# #     ood_freeze_global_concept_generators: False

# #     dynamic_model_path: "GlobalApproxCEM_BackBone_joint_fixed_{joint_trained}_n_concept_variants_{n_concept_variants}_epochs_{dynamic_epochs}_emb_size_{emb_size}_intervention_weight_{intervention_weight}_intervention_task_discount_{intervention_task_discount}_Baseline_cwl_{concept_loss_weight}_fold_{split}"

# #     ###############################################################################################

# #     # IntCEM stuff
# #     embedding_activation: null
# #     training_intervention_prob: [0.25]
# #     intervention_task_discount: [1.1]
# #     use_concept_groups: True
# #     int_model_use_bn: True
# #     int_model_layers: [128, 128, 64, 64]
# #     max_horizon: 6
# #     horizon_rate: 1.005
# #     gradient_clip_val: 100
# #     task_concept_loss: 1
# #     intervention_weight: 0.1


# #     grid_variables:
# #       - concept_loss_weight
# #       - training_intervention_prob
# #       - intervention_task_discount
# #       - emb_size
# #       - n_concept_variants
# #       - temperature
# #       - l2_dist_loss_weight
# #       - thresh_l2_loss
# #       - distance_l2_loss
# #       - init_threshold_percentile
# #       - ood_dropout_prob
# #       - joint_trained
# #     grid_search_mode: exhaustive





# #   - architecture: ''
# #     run_name: "GlobalApproxCEM_emb_{emb_size}_n_vars_{n_concept_variants}_itp_{init_threshold_percentile}_e_{dynamic_epochs}_{approx_epochs}_{finetuning_epochs}_{ood_epochs}_p_{use_dynamic_for_probs}_dl_{distance_l2_loss}_ood_{ood_dropout_prob}_{thresh_l2_loss}_l2_{l2_dist_loss_weight}_freeze_{freeze_underlying_model}_{ood_freeze_underlying_model}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

# #     concept_loss_weight: [1] #, 10]
# #     emb_size: [16]
# #     n_concept_variants: [5] #, 20] #, 50]
# #     temperature: [1, 5]
# #     shared_emb_generator: True
# #     l2_dist_loss_weight: [0] #, 0.1, 0]
# #     attention_fn: 'softmax'
# #     init_threshold_percentile: [25, 50, 75, 90]


# #     # OOD stuff
# #     ood_dropout_prob: [0.1, 0, 0.25, 0.5]
# #     thresh_l2_loss: [0] #0.0001]
# #     distance_l2_loss: [0]

# #     # Training:
# #     use_dynamic_for_probs: True
# #     freeze_centroids: False
# #     dynamic_epochs: 150
# #     prototypes_epochs: 0
# #     approx_epochs: 100
# #     freeze_underlying_model: True
# #     finetuning_epochs: 0

# #     ood_epochs: 150
# #     ood_freeze_underlying_model: True
# #     ood_freeze_label_predictor: False
# #     ood_freeze_concept_rank_model: False
# #     ood_freeze_global_concept_generators: False

# #     dynamic_model_path: "GlobalApproxCEM_BackBone_epochs_{dynamic_epochs}_emb_size_{emb_size}_intervention_weight_{intervention_weight}_intervention_task_discount_{intervention_task_discount}_Baseline_cwl_{concept_loss_weight}_fold_{split}"

# #     ###############################################################################################

# #     # IntCEM stuff
# #     embedding_activation: null
# #     training_intervention_prob: [0.25]
# #     intervention_task_discount: [1.1]
# #     use_concept_groups: True
# #     int_model_use_bn: True
# #     int_model_layers: [128, 128, 64, 64]
# #     max_horizon: 6
# #     horizon_rate: 1.005
# #     gradient_clip_val: 100
# #     task_concept_loss: 1
# #     intervention_weight: 0.1


# #     grid_variables:
# #       - concept_loss_weight
# #       - training_intervention_prob
# #       - intervention_task_discount
# #       - emb_size
# #       - temperature
# #       - n_concept_variants
# #       - l2_dist_loss_weight
# #       - thresh_l2_loss
# #       - distance_l2_loss
# #       - init_threshold_percentile
# #       - ood_dropout_prob
# #     grid_search_mode: exhaustive



# #   - architecture: ''
# #     run_name: "GlobalApproxCEM_emb_{emb_size}_n_vars_{n_concept_variants}_e_{dynamic_epochs}_{approx_epochs}_{finetuning_epochs}_{ood_epochs}_ood_{ood_dropout_prob}_{thresh_l2_loss}_l2_{l2_dist_loss_weight}_freeze_{freeze_underlying_model}_{ood_freeze_underlying_model}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

# #     concept_loss_weight: [1, 10]
# #     emb_size: [16]
# #     n_concept_variants: [10, 5, 2, 1]
# #     temperature: [5]
# #     shared_emb_generator: True
# #     l2_dist_loss_weight: [100] #, 0.1, 0]
# #     attention_fn: 'softmax'


# #     # OOD stuff
# #     ood_dropout_prob: [0.1, 0.25]
# #     thresh_l2_loss: [0.1]

# #     # Training:
# #     dynamic_epochs: 150
# #     freeze_underlying_model: True
# #     approx_epochs: 100
# #     finetuning_epochs: 0
# #     ood_epochs: 100
# #     ood_freeze_underlying_model: False
# #     dynamic_model_path: "IntCEM_emb_size_{emb_size}_intervention_weight_{intervention_weight}_intervention_task_discount_{intervention_task_discount}_Baseline_cwl_{concept_loss_weight}_fold_1"

# #     ###############################################################################################

# #     # IntCEM stuff
# #     embedding_activation: null
# #     training_intervention_prob: [0.25]
# #     intervention_task_discount: [1.1]
# #     use_concept_groups: True
# #     int_model_use_bn: True
# #     int_model_layers: [128, 128, 64, 64]
# #     max_horizon: 6
# #     horizon_rate: 1.005
# #     gradient_clip_val: 100
# #     task_concept_loss: 1
# #     intervention_weight: 0.1


# #     grid_variables:
# #       - concept_loss_weight
# #       - training_intervention_prob
# #       - intervention_task_discount
# #       - emb_size
# #       - temperature
# #       - n_concept_variants
# #       - l2_dist_loss_weight
# #       - thresh_l2_loss
# #       - ood_dropout_prob
# #     grid_search_mode: exhaustive



# # # THE NEXT CONFIG LED TO THIS
# # #   test c_acc: 94.96%, test y_acc: 71.04%, test c_auc: 89.62%, test y_auc: 98.92%
# # #         Intervening in GlobalApproxCEM_emb_16_n_vars_4_temp_5_l2_100_freeze_True_seg_True_tip_0.25_itd_1.1_iw_0.1_cwl_1 with dataset 'test', policy random_group_level_True_use_prior_False, and competence 1
# # # Global seed set to 42
# # #         Average intervention took 0.00150 seconds and construction took 0.00008 seconds.
# # # INFO:root:              test accuracy when intervening with 0 concept groups with claimed competence 1 and real competence same is 71.04% (avg int time is 0.00150s and construction time is 0.00008s).
# # # INFO:root:              test accuracy when intervening with 1 concept groups with claimed competence 1 and real competence same is 74.16% (avg int time is 0.00150s and construction time is 0.00008s).
# # # INFO:root:              test accuracy when intervening with 2 concept groups with claimed competence 1 and real competence same is 77.82% (avg int time is 0.00150s and construction time is 0.00008s).
# # # INFO:root:              test accuracy when intervening with 3 concept groups with claimed competence 1 and real competence same is 81.62% (avg int time is 0.00150s and construction time is 0.00008s).
# # # INFO:root:              test accuracy when intervening with 4 concept groups with claimed competence 1 and real competence same is 85.78% (avg int time is 0.00150s and construction time is 0.00008s).
# # # INFO:root:              test accuracy when intervening with 5 concept groups with claimed competence 1 and real competence same is 89.04% (avg int time is 0.00150s and construction time is 0.00008s).
# # # INFO:root:              test accuracy when intervening with 6 concept groups with claimed competence 1 and real competence same is 92.01% (avg int time is 0.00150s and construction time is 0.00008s).
# # # INFO:root:              test accuracy when intervening with 7 concept groups with claimed competence 1 and real competence same is 94.99% (avg int time is 0.00150s and construction time is 0.00008s).
# # # INFO:root:              test area under the intervention accuracy curve with policy random_group_level_True_use_prior_False, claimed competence 1, and real competence same is 666.47%.
# # # Global seed set to 42
# # #                 Selected concepts: [0, 1, 2, 3, 37, 38, 39, 40, 41, 42, 43, 44, 51, 52, 76, 77, 86, 87, 88, 99, 100, 101]
# # #                 Updated concept group map (with 7 groups):
# # #                         has_bill_shape -> [0, 1, 2, 3]
# # #                         has_head_pattern -> [4, 5]
# # #                         has_breast_color -> [6, 7, 8, 9, 10, 11]
# # #                         has_bill_length -> [12, 13]
# # #                         has_wing_shape -> [14, 15]
# # #                         has_tail_pattern -> [16, 17, 18]
# # #                         has_bill_color -> [19, 20, 21]
# # # Predicting DataLoader 0: 100%|| 91/91 [00:14<00:00,  6.36it/s]
# # # OOD_test c_acc: 71.67%, OOD_test y_acc: 0.67%, OOD_test c_auc: 50.04%, OOD_test y_auc: 47.45%
# # #         Intervening in GlobalApproxCEM_emb_16_n_vars_4_temp_5_l2_100_freeze_True_seg_True_tip_0.25_itd_1.1_iw_0.1_cwl_1 with dataset 'OOD_test', policy random_group_level_True_use_prior_False, and competence 1
# # # Global seed set to 42
# # #         Average intervention took 0.00150 seconds and construction took 0.00007 seconds.
# # # INFO:root:              OOD_test accuracy when intervening with 0 concept groups with claimed competence 1 and real competence same is 0.60% (avg int time is 0.00150s and construction time is 0.00007s).
# # # INFO:root:              OOD_test accuracy when intervening with 1 concept groups with claimed competence 1 and real competence same is 1.24% (avg int time is 0.00150s and construction time is 0.00007s).
# # # INFO:root:              OOD_test accuracy when intervening with 2 concept groups with claimed competence 1 and real competence same is 3.14% (avg int time is 0.00150s and construction time is 0.00007s).
# # # INFO:root:              OOD_test accuracy when intervening with 3 concept groups with claimed competence 1 and real competence same is 6.87% (avg int time is 0.00150s and construction time is 0.00007s).
# # # INFO:root:              OOD_test accuracy when intervening with 4 concept groups with claimed competence 1 and real competence same is 14.36% (avg int time is 0.00150s and construction time is 0.00007s).
# # # INFO:root:              OOD_test accuracy when intervening with 5 concept groups with claimed competence 1 and real competence same is 26.65% (avg int time is 0.00150s and construction time is 0.00007s).
# # # INFO:root:              OOD_test accuracy when intervening with 6 concept groups with claimed competence 1 and real competence same is 44.17% (avg int time is 0.00150s and construction time is 0.00007s).
# # # INFO:root:              OOD_test accuracy when intervening with 7 concept groups with claimed competence 1 and real competence same is 65.43% (avg int time is 0.00150s and construction time is 0.00007s).
# # # INFO:root:              OOD_test area under the intervention accuracy curve with policy random_group_level_True_use_prior_False, claimed competence 1, and real competence same is 162.46%.

# #   - architecture: ''
# #     run_name: "GlobalApproxCEM_emb_{emb_size}_n_vars_{n_concept_variants}_temp_{temperature}_l2_{l2_dist_loss_weight}_freeze_{freeze_underlying_model}_seg_{shared_emb_generator}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

# #     concept_loss_weight: [1, 10]
# #     emb_size: [16]
# #     n_concept_variants: [4, 20, 5]
# #     temperature: [5, 1]
# #     shared_emb_generator: True
# #     l2_dist_loss_weight: [100, 0.1, 0]
# #     attention_fn: 'sigmoid'

# #     # Training:
# #     dynamic_epochs: 150
# #     freeze_underlying_model: True
# #     approx_epochs: 50
# #     finetuning_epochs: 100
# #     dynamic_model_path: "IntCEM_emb_size_{emb_size}_intervention_weight_{intervention_weight}_intervention_task_discount_{intervention_task_discount}_Baseline_cwl_{concept_loss_weight}_fold_1"

# #     ###############################################################################################

# #     # IntCEM stuff
# #     embedding_activation: null
# #     training_intervention_prob: [0.25]
# #     intervention_task_discount: [1.1]
# #     use_concept_groups: True
# #     int_model_use_bn: True
# #     int_model_layers: [128, 128, 64, 64]
# #     max_horizon: 6
# #     horizon_rate: 1.005
# #     gradient_clip_val: 100
# #     task_concept_loss: 1
# #     intervention_weight: 0.1


# #     grid_variables:
# #       - concept_loss_weight
# #       - training_intervention_prob
# #       - intervention_task_discount
# #       - emb_size
# #       - temperature
# #       - n_concept_variants
# #       - l2_dist_loss_weight
# #     grid_search_mode: exhaustive









# #   - architecture: 'GlobalBankConceptEmbeddingModel'
# #     run_name: "BankMixCEM_{bottleneck_pooling}_{selection_mode}_ds_{distance_selection}_soft_{soft_select}_lp_{learnable_prob}_emb_{emb_size}_n_vars_{n_concept_variants}_ade_{add_dynamic_embedding}_seg_{shared_emb_generator}_rm_{remap_context}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

# #     bottleneck_pooling: ['concat']
# #     concept_loss_weight: [1, 10] #0.1, 1, 10]
# #     emb_size: [16] #, 16, 32, 512]
# #     n_concept_variants: [5] #10, 5, 1]
# #     temperature: [1] #, 10]
# #     selection_mode: ['min_distance'] #'hierarchical'] #'attention'] #, 'learnt'] #, 'max_distance']
# #     learnable_prob: [False] #[True] #, False] #True]
# #     distance_selection: 'stochastic'
# #     soft_select: [False] #, False]
# #     shared_emb_generator: True
# #     remap_context: False
# #     add_dynamic_embedding: False #True

# #     # Training:
# #     concept_epochs: 0

# #     ###############################################################################################

# #     # IntCEM stuff
# #     embedding_activation: null
# #     training_intervention_prob: [0.25]
# #     intervention_task_discount: [1.1]
# #     use_concept_groups: True
# #     int_model_use_bn: True
# #     int_model_layers: [128, 128, 64, 64]
# #     max_horizon: 6
# #     horizon_rate: 1.005
# #     gradient_clip_val: 100
# #     task_concept_loss: 1
# #     intervention_weight: 0.1


# #     grid_variables:
# #       - bottleneck_pooling
# #       - concept_loss_weight
# #       - training_intervention_prob
# #       - intervention_task_discount
# #       - emb_size
# #       - temperature
# #       - n_concept_variants
# #       - learnable_prob
# #       - soft_select
# #       - selection_mode
# #     grid_search_mode: exhaustive


# #   - architecture: 'GlobalBankConceptEmbeddingModel'
# #     run_name: "BankMixCEM_{bottleneck_pooling}_{selection_mode}_soft_{soft_select}_lp_{learnable_prob}_emb_{emb_size}_n_vars_{n_concept_variants}_temp_{temperature}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

# #     bottleneck_pooling: ['concat']
# #     concept_loss_weight: [1, 10] #0.1, 1, 10]
# #     emb_size: [512, 16, 32]
# #     n_concept_variants: [10, 5, 1]
# #     temperature: [1] #, 10]
# #     selection_mode: ['attention'] #, 'max_distance']
# #     learnable_prob: [False, True] #True]
# #     soft_select: [True] #, False]
# #     shared_emb_generator: True

# #     ###############################################################################################

# #     # IntCEM stuff
# #     embedding_activation: null
# #     training_intervention_prob: [0.25]
# #     intervention_task_discount: [1.1]
# #     use_concept_groups: True
# #     int_model_use_bn: True
# #     int_model_layers: [128, 128, 64, 64]
# #     max_horizon: 6
# #     horizon_rate: 1.005
# #     gradient_clip_val: 100
# #     task_concept_loss: 1
# #     intervention_weight: 0.1


# #     grid_variables:
# #       - bottleneck_pooling
# #       - concept_loss_weight
# #       - training_intervention_prob
# #       - intervention_task_discount
# #       - emb_size
# #       - temperature
# #       - n_concept_variants
# #       - selection_mode
# #       - learnable_prob
# #       - soft_select
# #     grid_search_mode: exhaustive





  # - architecture: 'ProjectionConceptEmbeddingModel'
  #   run_name: "ACEM_mr_{mix_residuals}_{residual_sep_loss}_{manual_residual_scale}_alw_{adversary_loss_weight}_wp_{warmup_period}_cr_{conditional_residual}_rpd_{residual_drop_prob}_ex_{extra_capacity}_tl_{use_triplet_loss}_ep_{extra_capacity_dropout_prob}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_{fix_concept_embeddings_for_res}_{fix_backbone_for_res}_{freeze_emb_generators_for_res}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_shared_{shared_emb_generator}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   ########################## IMPORTANT !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

  #   simplified_mode: False
  #   # learning_rate: 0.0001
  #   training_intervention_prob: [0.25]
  #   embedding_activation: null
  #   shared_emb_generator: True
  #   use_triplet_loss: True
  #   learnable_orthogonal_dir: 0
  #   single_residual_vector: True
  #   extra_capacity_dropout_prob: [0]
  #   intervention_task_discount: [1.1]
  #   concept_model_path: 'ProjCEM_concept_model_{emb_size}_{blackbox_warmup_epochs}_{fix_backbone_for_concept}_{freeze_emb_generators_for_concept}_{split}'
  #   adversary_loss_weight: [0]
  #   use_learnable_residual: True
  #   warmup_period: [0] #200]
  #   intervention_weight: 0.1
  #   emb_size: [16]
  #   concept_loss_weight: [1]
  #   bottleneck_pooling: ['concat'] #, 'per_class_mixing']
  #   extra_capacity: [0]
  #   sigmoidal_extra_capacity: False
  #   conditional_residual: False
  #   use_learnable_prob: False
  #   dyn_scaling: 100
  #   residual_weight_l2: 0 #0.01
  #   residual_drop_prob: [0, 0.25]
  #   mix_residuals: True
  #   residual_sep_loss: 1
  #   manual_residual_scale: 1 ############################################################ CHANGE TO 1 ################################


  #   ###############################################################################################

  #   # training_intervention_prob: [0.25]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   # embedding_activation: "leakyrelu"
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   gradient_clip_val: 100
  #   task_concept_loss: 1

  #   blackbox_warmup_epochs: 0

  #   concept_epochs: 0 #200 #150
  #   fix_backbone_for_concept: False
  #   freeze_emb_generators_for_concept: False

  #   no_residual_epochs: 300 #50
  #   fix_backbone_for_no_res: False
  #   fix_concept_embeddings_for_no_res: False
  #   use_ground_truth_mixing_for_no_res: False


  #   e2e_epochs: 0 #50
  #   fix_backbone_for_res: True
  #   freeze_emb_generators_for_res: True
  #   fix_concept_embeddings_for_res: True
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - extra_capacity_dropout_prob
  #     - extra_capacity
  #     - adversary_loss_weight
  #     - warmup_period
  #     - residual_drop_prob
  #   grid_search_mode: exhaustive




  # - architecture: 'ProjectionConceptEmbeddingModel'
  #   run_name: "ACEM_mr_{mix_residuals}_{residual_sep_loss}_{manual_residual_scale}_alw_{adversary_loss_weight}_wp_{warmup_period}_cr_{conditional_residual}_rpd_{residual_drop_prob}_ex_{extra_capacity}_tl_{use_triplet_loss}_ep_{extra_capacity_dropout_prob}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_{fix_concept_embeddings_for_res}_{fix_backbone_for_res}_{freeze_emb_generators_for_res}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_shared_{shared_emb_generator}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   ########################## IMPORTANT !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

  #   simplified_mode: False
  #   # learning_rate: 0.0001
  #   training_intervention_prob: [0.25]
  #   embedding_activation: null
  #   shared_emb_generator: True
  #   use_triplet_loss: True
  #   learnable_orthogonal_dir: 0
  #   single_residual_vector: True
  #   extra_capacity_dropout_prob: [0]
  #   intervention_task_discount: [1.1]
  #   concept_model_path: 'ProjCEM_concept_model_{emb_size}_{blackbox_warmup_epochs}_{fix_backbone_for_concept}_{freeze_emb_generators_for_concept}_{split}'
  #   adversary_loss_weight: [0]
  #   use_learnable_residual: True
  #   warmup_period: [0] #200]
  #   intervention_weight: 0.1
  #   emb_size: [16]
  #   concept_loss_weight: [1]
  #   bottleneck_pooling: ['concat'] #, 'per_class_mixing']
  #   extra_capacity: [128]
  #   sigmoidal_extra_capacity: False
  #   conditional_residual: False
  #   use_learnable_prob: False
  #   dyn_scaling: 100
  #   residual_weight_l2: 0 #0.01
  #   residual_drop_prob: [0, dyn_0.999_0.25, dyn_0.990_0.1, 0.1, 0.25] #[0.25]
  #   mix_residuals: True
  #   residual_sep_loss: 1
  #   manual_residual_scale: 1 ############################################################ CHANGE TO 1 ################################


  #   ###############################################################################################

  #   # training_intervention_prob: [0.25]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   # embedding_activation: "leakyrelu"
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   gradient_clip_val: 100
  #   task_concept_loss: 1

  #   blackbox_warmup_epochs: 0

  #   concept_epochs: 0 #200 #150
  #   fix_backbone_for_concept: False
  #   freeze_emb_generators_for_concept: False

  #   no_residual_epochs: 300 #50
  #   fix_backbone_for_no_res: False
  #   fix_concept_embeddings_for_no_res: False
  #   use_ground_truth_mixing_for_no_res: False


  #   e2e_epochs: 0 #50
  #   fix_backbone_for_res: True
  #   freeze_emb_generators_for_res: True
  #   fix_concept_embeddings_for_res: True
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - extra_capacity_dropout_prob
  #     - extra_capacity
  #     - adversary_loss_weight
  #     - warmup_period
  #     - residual_drop_prob
  #   grid_search_mode: exhaustive




  # - architecture: 'DeferConceptEmbeddingModel'
  #   run_name: "DCEM_bp_{bottleneck_pooling}_emb_{emb_size}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}_wr_{dynamic_weights_reg}_ar_{dynamic_activations_reg}_cpm_{conditional_pred_mixture}_{mixcem_concept_epochs}_{mixcem_epochs}_{dynamic_epochs}_{joint_epochs}"
  #   ########################## IMPORTANT !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

  #   simplified_mode: False
  #   training_intervention_prob: [0.25]
  #   embedding_activation: null
  #   intervention_task_discount: [1.1]
  #   intervention_weight: 0.1
  #   emb_size: [16]
  #   concept_loss_weight: [1]
  #   bottleneck_pooling: ['concat'] #, 'pcm']

  #   shared_emb_generator: True
  #   dynamic_ood_detection: 1
  #   dynamic_weights_reg: [0]
  #   dynamic_weights_reg_norm: 2
  #   dynamic_activations_reg: [0, 0.1, 1]
  #   dynamic_activations_reg_norm: 2
  #   dynamic_drop_prob: 0
  #   conditional_pred_mixture: ['none']


  #   ###############################################################################################

  #   # IntCEM stuff

  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   gradient_clip_val: 100
  #   task_concept_loss: 1

  #   ###############################################################################################

  #   # Training stuff

  #   mixcem_concept_model_path: 'DCEM_Concept_Trained_bp_{bottleneck_pooling}_emb_{emb_size}_cpm_{conditional_pred_mixture}_{mixcem_concept_epochs}_{split}'
  #   mixcem_concept_epochs: 0

  #   mixcem_entire_model_path: 'DCEM_Mixcem_Trained_bp_{bottleneck_pooling}_emb_{emb_size}_cpm_{conditional_pred_mixture}_{mixcem_concept_epochs}_{mixcem_epochs}_{mixcem_concept_epochs}_{fix_concept_embeddings_for_mixcem}_{fix_backbone_for_mixcem}_{freeze_emb_generators_for_mixcem}_{split}'
  #   mixcem_epochs: 0 #150
  #   fix_concept_embeddings_for_mixcem: False
  #   fix_backbone_for_mixcem: False
  #   freeze_emb_generators_for_mixcem: False


  #   dynamic_entire_model_path: 'DCEM_Dynamic_Trained_bp_{bottleneck_pooling}_emb_{emb_size}_cpm_{conditional_pred_mixture}_{mixcem_concept_epochs}_{mixcem_epochs}_{dynamic_epochs}_{fix_backbone_for_dynamic}_{freeze_emb_generators_for_dynamic}_{mixcem_concept_epochs}_{fix_concept_embeddings_for_mixcem}_{fix_backbone_for_mixcem}_{freeze_emb_generators_for_mixcem}_{split}'
  #   dynamic_epochs: 0 #150
  #   fix_backbone_for_dynamic: True
  #   freeze_emb_generators_for_dynamic: True


  #   joint_epochs: 300 #150
  #   fix_backbone_for_joint: False
  #   freeze_emb_generators_for_joint: False
  #   fix_concept_embeddings_for_joint: False
  #   fix_dynamic_prob_generators_for_joint: False
  #   fix_mixcem_label_predictor_for_joint: False
  #   fix_dynamic_label_predictor_for_joint: False


  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - dynamic_weights_reg
  #     - dynamic_activations_reg
  #     - conditional_pred_mixture
  #   grid_search_mode: exhaustive



  # - architecture: 'DeferConceptEmbeddingModel'
  #   run_name: "DCEM_frozen_bp_{bottleneck_pooling}_emb_{emb_size}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}_wr_{dynamic_weights_reg}_ar_{dynamic_activations_reg}_cpm_{conditional_pred_mixture}_{mixcem_concept_epochs}_{mixcem_epochs}_{dynamic_epochs}_{joint_epochs}"
  #   ########################## IMPORTANT !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

  #   simplified_mode: False
  #   training_intervention_prob: [0.25]
  #   embedding_activation: null
  #   intervention_task_discount: [1.1]
  #   intervention_weight: 0.1
  #   emb_size: [16]
  #   concept_loss_weight: [1]
  #   bottleneck_pooling: ['concat'] #, 'pcm']

  #   shared_emb_generator: True
  #   dynamic_ood_detection: 1
  #   dynamic_weights_reg: [0]
  #   dynamic_weights_reg_norm: 2
  #   dynamic_activations_reg: [0, 0.1, 1]
  #   dynamic_activations_reg_norm: 2
  #   dynamic_drop_prob: 0
  #   conditional_pred_mixture: ['none']


  #   ###############################################################################################

  #   # IntCEM stuff

  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   gradient_clip_val: 100
  #   task_concept_loss: 1

  #   ###############################################################################################

  #   # Training stuff

  #   mixcem_concept_model_path: 'DCEM_Concept_Trained_bp_{bottleneck_pooling}_emb_{emb_size}_cpm_{conditional_pred_mixture}_{mixcem_concept_epochs}_{split}'
  #   mixcem_concept_epochs: 0

  #   mixcem_entire_model_path: 'DCEM_Mixcem_Trained_bp_{bottleneck_pooling}_emb_{emb_size}_cpm_{conditional_pred_mixture}_{mixcem_concept_epochs}_{mixcem_epochs}_{mixcem_concept_epochs}_{fix_concept_embeddings_for_mixcem}_{fix_backbone_for_mixcem}_{freeze_emb_generators_for_mixcem}_{split}'
  #   mixcem_epochs: 150
  #   fix_concept_embeddings_for_mixcem: False
  #   fix_backbone_for_mixcem: False
  #   freeze_emb_generators_for_mixcem: False


  #   dynamic_entire_model_path: 'DCEM_Dynamic_Trained_bp_{bottleneck_pooling}_emb_{emb_size}_cpm_{conditional_pred_mixture}_{mixcem_concept_epochs}_{mixcem_epochs}_{dynamic_epochs}_{fix_backbone_for_dynamic}_{freeze_emb_generators_for_dynamic}_{mixcem_concept_epochs}_{fix_concept_embeddings_for_mixcem}_{fix_backbone_for_mixcem}_{freeze_emb_generators_for_mixcem}_{split}'
  #   dynamic_epochs: 150
  #   fix_backbone_for_dynamic: True
  #   freeze_emb_generators_for_dynamic: True


  #   joint_epochs: 150
  #   fix_backbone_for_joint: True
  #   freeze_emb_generators_for_joint: True
  #   fix_concept_embeddings_for_joint: True
  #   fix_dynamic_prob_generators_for_joint: True
  #   fix_mixcem_label_predictor_for_joint: True
  #   fix_dynamic_label_predictor_for_joint: True


  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - dynamic_weights_reg
  #     - dynamic_activations_reg
  #     - conditional_pred_mixture
  #   grid_search_mode: exhaustive


################################## COMMENT FOR NOW!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

  # - architecture: 'ProjectionConceptEmbeddingModel'
  #   run_name: "ACEM_dis_mr_{mix_residuals}_{residual_sep_loss}_{manual_residual_scale}_alw_{adversary_loss_weight}_wp_{warmup_period}_cr_{conditional_residual}_rpd_{residual_drop_prob}_ex_{extra_capacity}_tl_{use_triplet_loss}_ep_{extra_capacity_dropout_prob}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_{fix_concept_embeddings_for_res}_{fix_backbone_for_res}_{freeze_emb_generators_for_res}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_shared_{shared_emb_generator}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   ########################## IMPORTANT !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

  #   simplified_mode: False
  #   # learning_rate: 0.0001
  #   training_intervention_prob: [0.25]
  #   embedding_activation: null
  #   shared_emb_generator: True
  #   use_triplet_loss: True
  #   learnable_orthogonal_dir: 0
  #   single_residual_vector: True
  #   extra_capacity_dropout_prob: [0]
  #   intervention_task_discount: [1.1]
  #   concept_model_path: 'ProjCEM_concept_model_{emb_size}_{blackbox_warmup_epochs}_{fix_backbone_for_concept}_{freeze_emb_generators_for_concept}_{split}'
  #   adversary_loss_weight: [0]
  #   use_learnable_residual: False # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   warmup_period: [0] #200]
  #   intervention_weight: 0.1
  #   emb_size: [16]
  #   concept_loss_weight: [1]
  #   bottleneck_pooling: ['concat', 'pcm'] # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   extra_capacity: [128]
  #   sigmoidal_extra_capacity: False
  #   conditional_residual: False
  #   use_learnable_prob: False
  #   dyn_scaling: 100
  #   residual_weight_l2: 0 #0.01
  #   residual_drop_prob: [0, dyn_0.999_0.25, dyn_0.990_0.1, 0.1, 0.25] #[0.25]
  #   mix_residuals: True
  #   residual_sep_loss: 1
  #   manual_residual_scale: 1 ############################################################ CHANGE TO 1 ################################


  #   ###############################################################################################

  #   # training_intervention_prob: [0.25]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   # embedding_activation: "leakyrelu"
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   gradient_clip_val: 100
  #   task_concept_loss: 1

  #   blackbox_warmup_epochs: 0

  #   concept_epochs: 0 #200 #150
  #   fix_backbone_for_concept: False
  #   freeze_emb_generators_for_concept: False

  #   no_residual_epochs: 300 #50
  #   fix_backbone_for_no_res: False
  #   fix_concept_embeddings_for_no_res: False
  #   use_ground_truth_mixing_for_no_res: False


  #   e2e_epochs: 0 #50
  #   fix_backbone_for_res: True
  #   freeze_emb_generators_for_res: True
  #   fix_concept_embeddings_for_res: True
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - extra_capacity_dropout_prob
  #     - extra_capacity
  #     - adversary_loss_weight
  #     - warmup_period
  #     - residual_drop_prob
  #   grid_search_mode: exhaustive




  # - architecture: 'ProjectionConceptEmbeddingModel'
  #   run_name: "ACEM_sig_mr_{mix_residuals}_{residual_sep_loss}_{manual_residual_scale}_alw_{adversary_loss_weight}_wp_{warmup_period}_cr_{conditional_residual}_rpd_{residual_drop_prob}_ex_{extra_capacity}_tl_{use_triplet_loss}_ep_{extra_capacity_dropout_prob}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_{fix_concept_embeddings_for_res}_{fix_backbone_for_res}_{freeze_emb_generators_for_res}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_shared_{shared_emb_generator}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   ########################## IMPORTANT !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

  #   simplified_mode: False
  #   # learning_rate: 0.0001
  #   training_intervention_prob: [0.25]
  #   embedding_activation: null
  #   shared_emb_generator: True
  #   use_triplet_loss: True
  #   learnable_orthogonal_dir: 0
  #   single_residual_vector: True
  #   extra_capacity_dropout_prob: [0]
  #   intervention_task_discount: [1.1]
  #   concept_model_path: 'ProjCEM_concept_model_{emb_size}_{blackbox_warmup_epochs}_{fix_backbone_for_concept}_{freeze_emb_generators_for_concept}_{split}'
  #   adversary_loss_weight: [0]
  #   use_learnable_residual: True
  #   warmup_period: [0] #200]
  #   intervention_weight: 0.1
  #   emb_size: [16]
  #   concept_loss_weight: [1]
  #   bottleneck_pooling: ['concat'] #, 'per_class_mixing']
  #   extra_capacity: [128]
  #   sigmoidal_extra_capacity: True # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   conditional_residual: False
  #   use_learnable_prob: False
  #   dyn_scaling: 100
  #   residual_weight_l2: 0 #0.01
  #   residual_drop_prob: [0, dyn_0.999_0.25, dyn_0.990_0.1, 0.1, 0.25] #[0.25]
  #   mix_residuals: True
  #   residual_sep_loss: 1
  #   manual_residual_scale: 1 ############################################################ CHANGE TO 1 ################################


  #   ###############################################################################################

  #   # training_intervention_prob: [0.25]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   # embedding_activation: "leakyrelu"
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   gradient_clip_val: 100
  #   task_concept_loss: 1

  #   blackbox_warmup_epochs: 0

  #   concept_epochs: 0 #200 #150
  #   fix_backbone_for_concept: False
  #   freeze_emb_generators_for_concept: False

  #   no_residual_epochs: 300 #50
  #   fix_backbone_for_no_res: False
  #   fix_concept_embeddings_for_no_res: False
  #   use_ground_truth_mixing_for_no_res: False


  #   e2e_epochs: 0 #50
  #   fix_backbone_for_res: True
  #   freeze_emb_generators_for_res: True
  #   fix_concept_embeddings_for_res: True
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - extra_capacity_dropout_prob
  #     - extra_capacity
  #     - adversary_loss_weight
  #     - warmup_period
  #     - residual_drop_prob
  #   grid_search_mode: exhaustive





  # - architecture: 'ProjectionConceptEmbeddingModel'
  #   run_name: "AdversarialCEM_alw_{adversary_loss_weight}_wp_{warmup_period}_lod_{learnable_orthogonal_dir}_ex_{extra_capacity}_tl_{use_triplet_loss}_s_{simplified_mode}_ep_{extra_capacity_dropout_prob}_{blackbox_warmup_epochs}_{concept_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_shared_{shared_emb_generator}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   ########################## IMPORTANT !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

  #   simplified_mode: False
  #   training_intervention_prob: [0.25]
  #   embedding_activation: null
  #   shared_emb_generator: True
  #   use_triplet_loss: True
  #   learnable_orthogonal_dir: 0
  #   single_residual_vector: True
  #   extra_capacity_dropout_prob: [0]
  #   intervention_task_discount: [1.1]
  #   concept_model_path: 'ProjCEM_concept_model_{emb_size}_{blackbox_warmup_epochs}_{fix_backbone_for_concept}_{freeze_emb_generators_for_concept}_{split}'
  #   adversary_loss_weight: ["dyn"]
  #   use_learnable_residual: True
  #   warmup_period: [0]
  #   intervention_weight: 0.1
  #   emb_size: [16]
  #   concept_loss_weight: [1]
  #   bottleneck_pooling: ['concat']
  #   extra_capacity: [100]
  #   sigmoidal_extra_capacity: False

  #   ###############################################################################################

  #   # training_intervention_prob: [0.25]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   # embedding_activation: "leakyrelu"
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   gradient_clip_val: 100
  #   task_concept_loss: 1

  #   blackbox_warmup_epochs: 0

  #   concept_epochs: 150
  #   fix_backbone_for_concept: False
  #   freeze_emb_generators_for_concept: False

  #   e2e_epochs: 300
  #   fix_backbone_for_res: False
  #   freeze_emb_generators_for_res: False
  #   fix_concept_embeddings_for_res: False
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - extra_capacity_dropout_prob
  #     - extra_capacity
  #     - adversary_loss_weight
  #     - warmup_period
  #   grid_search_mode: exhaustive



  # - architecture: 'ProjectionConceptEmbeddingModel'
  #   run_name: "AdversarialCEM_{adversary_loss_weight}_{warmup_period}_{learnable_orthogonal_dir}_ex_{extra_capacity}_tl_{use_triplet_loss}_s_{simplified_mode}_ep_{extra_capacity_dropout_prob}_{blackbox_warmup_epochs}_{concept_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_shared_{shared_emb_generator}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   ########################## IMPORTANT !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

  #   simplified_mode: False
  #   training_intervention_prob: [0.25] #[0.25, 0.5, 0.75, 1]]
  #   embedding_activation: null
  #   shared_emb_generator: True
  #   use_triplet_loss: True
  #   learnable_orthogonal_dir: 0
  #   single_residual_vector: True
  #   extra_capacity_dropout_prob: [0, 0.25, 0.75]
  #   intervention_task_discount: [1.1] #, 1.1] #, 1.01] #, 1.05]
  #   concept_model_path: 'ProjCEM_concept_model_{emb_size}_{blackbox_warmup_epochs}_{fix_backbone_for_concept}_{freeze_emb_generators_for_concept}_{split}'
  #   adversary_loss_weight: [0, 1, 0.5, 0.1, 0.01]
  #   use_learnable_residual: True
  #   warmup_period: [0, 100]
  #   intervention_weight: 0.1
  #   emb_size: [16] #, 64, 128]
  #   concept_loss_weight: [1]

  #   ###############################################################################################

  #   # training_intervention_prob: [0.25]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   # embedding_activation: "leakyrelu"
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   gradient_clip_val: 100

  #   normalize_embs: False
  #   task_concept_loss: 1
  #   use_cosine_similarity: [True]
  #   early_stopping_best_model: False
  #   dynamic_residual: False
  #   conditional_residual: [False]
  #   c2y_layers: []
  #   residual_layers: []
  #   bottleneck_pooling: ['concat']
  #   per_concept_residual: [False]
  #   sigmoidal_residual: [False]
  #   residual_deviation: [0]
  #   shared_per_concept_residual: [False]
  #   intermediate_task_concept_loss: 0
  #   residual_scale: 0
  #   learnable_residual_scale: False
  #   sigmoidal_residual_scale: False
  #   learn_residual_embeddings: False


  #   learnable_distance_metric: False
  #   learnable_prob_model: False
  #   use_latent_space: False
  #   use_residual: False
  #   residual_model_weight_l2_reg: 0
  #   residual_norm_loss: [0]
  #   residual_scale_reg: [0]
  #   residual_norm_metric: 2
  #   residual_scale_norm_metric: 1

  #   extra_capacity: [0]
  #   orthogonal_extra_capacity: False

  #   blackbox_warmup_epochs: 0

  #   concept_epochs: 150
  #   fix_backbone_for_concept: False
  #   freeze_emb_generators_for_concept: False

  #   e2e_epochs: 300
  #   fix_backbone_for_res: False
  #   freeze_emb_generators_for_res: False
  #   fix_concept_embeddings_for_res: False
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #     - emb_size
  #     - residual_scale_reg
  #     - extra_capacity_dropout_prob
  #     - extra_capacity
  #     - use_cosine_similarity
  #     - adversary_loss_weight
  #     - warmup_period
  #   grid_search_mode: exhaustive



  # - architecture: 'ProjectionConceptEmbeddingModel'
  #   run_name: "ProjCEM_{single_residual_vector}_{learnable_orthogonal_dir}_ex_{extra_capacity}_tl_{use_triplet_loss}_s_{simplified_mode}_ep_{extra_capacity_dropout_prob}_{blackbox_warmup_epochs}_{concept_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_shared_{shared_emb_generator}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   ########################## IMPORTANT !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   simplified_mode: False
  #   training_intervention_prob: [0.25] #[0.25, 0.5, 0.75, 1]]
  #   embedding_activation: null
  #   shared_emb_generator: True
  #   use_triplet_loss: True
  #   learnable_orthogonal_dir: 0
  #   single_residual_vector: True
  #   intervention_weight: 0.1
  #   extra_capacity_dropout_prob: [0.9, 0.75]
  #   intervention_task_discount: [1.1] #, 1.1] #, 1.01] #, 1.05]
  #   concept_model_path: 'ProjCEM_concept_model_{emb_size}_{blackbox_warmup_epochs}_{fix_backbone_for_concept}_{freeze_emb_generators_for_concept}_{split}'

  #   ###############################################################################################

  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   gradient_clip_val: 100

  #   emb_size: [16, 256]
  #   concept_loss_weight: [1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   use_cosine_similarity: [True]
  #   early_stopping_best_model: False
  #   dynamic_residual: False
  #   conditional_residual: [False]
  #   c2y_layers: []
  #   residual_layers: []
  #   bottleneck_pooling: ['concat'] #, 'per_class_mixing']
  #   per_concept_residual: [False]
  #   sigmoidal_residual: [False]
  #   residual_deviation: [0]
  #   shared_per_concept_residual: [False]
  #   intermediate_task_concept_loss: 0
  #   residual_scale: 0
  #   learnable_residual_scale: False
  #   sigmoidal_residual_scale: False
  #   learn_residual_embeddings: False


  #   learnable_distance_metric: False
  #   learnable_prob_model: False
  #   use_latent_space: False
  #   use_residual: False
  #   residual_model_weight_l2_reg: 0
  #   residual_norm_loss: [0]
  #   residual_scale_reg: [0]
  #   residual_norm_metric: 2
  #   residual_scale_norm_metric: 1

  #   extra_capacity: [0]
  #   orthogonal_extra_capacity: False

  #   blackbox_warmup_epochs: 0

  #   concept_epochs: 150
  #   fix_backbone_for_concept: False
  #   freeze_emb_generators_for_concept: False

  #   e2e_epochs: 300
  #   fix_backbone_for_res: False
  #   freeze_emb_generators_for_res: False
  #   fix_concept_embeddings_for_res: False
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #     - emb_size
  #     - residual_scale_reg
  #     - extra_capacity_dropout_prob
  #     - extra_capacity
  #     - use_cosine_similarity
  #   grid_search_mode: exhaustive


  # - architecture: 'MixingConceptEmbeddingModel'
  #   run_name: "MixCEM_n_extra_{n_discovered_concepts}_entr_{discovered_probs_entropy}_dis_{intervention_task_discount}_ip_{training_intervention_prob}_dip_{dyn_training_intervention_prob}_cl_{contrastive_loss_weight}_mix_{mix_ground_truth_embs}_shared_{shared_emb_generator}_emb_size_{emb_size}_ml_{intermediate_task_concept_loss}_Baseline_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [32] #, 32]
  #   embedding_activation: null
  #   n_discovered_concepts: [100, 200, 300] #, 25]
  #   concept_loss_weight: [1]
  #   contrastive_loss_weight: [0]
  #   mix_ground_truth_embs: True
  #   shared_emb_generator: [True]
  #   normalize_embs: False
  #   sample_probs: False
  #   cond_discovery: False
  #   intermediate_task_concept_loss: [0]
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.01] #, 1.05] #, 1.1, 1.5]
  #   discovered_probs_entropy: 0
  #   dyn_training_intervention_prob: [0.1, 0.25]
  #   grid_variables:
  #     - concept_loss_weight
  #     - n_discovered_concepts
  #     - contrastive_loss_weight
  #     - shared_emb_generator
  #     - emb_size
  #     - intermediate_task_concept_loss
  #     - intervention_task_discount
  #     - training_intervention_prob
  #     - dyn_training_intervention_prob
  #   grid_search_mode: exhaustive


  # - architecture: 'NewMixingConceptEmbeddingModel'
  #   run_name: "Try_MixCEM_proj_{orthogonal_extra_capacity}_ex_{extra_capacity}_exp_p_{extra_capacity_dropout_prob}_cos_{use_cosine_similarity}_{use_latent_space}_{residual_model_weight_l2_reg}_{residual_scale_reg}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_rnl_{residual_norm_loss}_pcr_{per_concept_residual}_shared_{shared_per_concept_residual}_itl_{intermediate_task_concept_loss}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   # training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   training_intervention_prob: [0.25]
  #   intervention_weight: 5
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   embedding_activation: "leakyrelu"
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   gradient_clip_val: 100

  #   emb_size: [16]
  #   # embedding_activation: null
  #   concept_loss_weight: [1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.1, 1.5] #, 1.1] #, 1.01] #, 1.05]
  #   use_cosine_similarity: [False, False]
  #   early_stopping_best_model: False
  #   dynamic_residual: False
  #   conditional_residual: [False]
  #   c2y_layers: []
  #   residual_layers: []
  #   bottleneck_pooling: ['concat']
  #   per_concept_residual: [False]
  #   sigmoidal_residual: [False]
  #   residual_deviation: [0]
  #   shared_per_concept_residual: [False]
  #   intermediate_task_concept_loss: 0
  #   residual_scale: 0
  #   learnable_residual_scale: False
  #   sigmoidal_residual_scale: False
  #   learn_residual_embeddings: False


  #   learnable_distance_metric: False
  #   learnable_prob_model: False
  #   use_latent_space: False
  #   use_residual: True
  #   residual_model_weight_l2_reg: 0
  #   residual_norm_loss: [0]
  #   residual_scale_reg: [0]
  #   residual_norm_metric: 2
  #   residual_scale_norm_metric: 1

  #   extra_capacity: [0]
  #   orthogonal_extra_capacity: True
  #   extra_capacity_dropout_prob: [0]

  #   blackbox_warmup_epochs: 0

  #   concept_epochs: 75
  #   fix_backbone_for_concept: False
  #   freeze_emb_generators_for_concept: False

  #   no_residual_epochs: 0
  #   fix_backbone_for_no_res: False
  #   fix_concept_embeddings_for_no_res: False
  #   use_ground_truth_mixing_for_no_res: False

  #   e2e_epochs: 300
  #   fix_backbone_for_res: False
  #   freeze_emb_generators_for_res: False
  #   fix_concept_embeddings_for_res: False
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #     - emb_size
  #     - residual_scale_reg
  #     - extra_capacity_dropout_prob
  #     - extra_capacity
  #     - use_cosine_similarity
  #   grid_search_mode: exhaustive
  #   dataset_config:
  #     dataset: "cub"
  #     num_workers: 8
  #     batch_size: 256 #64

  #     # DATASET VARIABLES
  #     root_dir: /homes/me466/data/CUB200/
  #     sampling_percent: 0.25  # [IMPORTANT] Select only a quarter of all concepts!
  #     sampling_groups: True
  #     test_subsampling: 1
  #     weight_loss: True



  # - architecture: 'NewMixingConceptEmbeddingModel'
  #   run_name: "Try_MixCEM_no_res_ex_{extra_capacity}_exp_p_{extra_capacity_dropout_prob}_{use_latent_space}_{residual_model_weight_l2_reg}_{residual_scale_reg}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_rnl_{residual_norm_loss}_pcr_{per_concept_residual}_shared_{shared_per_concept_residual}_itl_{intermediate_task_concept_loss}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [16]
  #   embedding_activation: null
  #   concept_loss_weight: [1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.1, 1.5] #, 1.1] #, 1.01] #, 1.05]
  #   use_cosine_similarity: False
  #   early_stopping_best_model: False
  #   dynamic_residual: False
  #   conditional_residual: [False]
  #   c2y_layers: []
  #   residual_layers: []
  #   bottleneck_pooling: ['concat']
  #   per_concept_residual: [False]
  #   sigmoidal_residual: [False]
  #   residual_deviation: [0]
  #   shared_per_concept_residual: [False]
  #   intermediate_task_concept_loss: 0
  #   residual_scale: 0
  #   learnable_residual_scale: False
  #   sigmoidal_residual_scale: False
  #   learn_residual_embeddings: False


  #   learnable_distance_metric: False
  #   learnable_prob_model: False
  #   use_latent_space: False
  #   use_residual: False ##!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   residual_model_weight_l2_reg: 0
  #   residual_norm_loss: [0]
  #   residual_scale_reg: [0]
  #   residual_norm_metric: 2
  #   residual_scale_norm_metric: 1

  #   extra_capacity: [50]
  #   extra_capacity_dropout_prob: [0.1]

  #   blackbox_warmup_epochs: 5

  #   concept_epochs: 75
  #   fix_backbone_for_concept: False
  #   freeze_emb_generators_for_concept: False

  #   no_residual_epochs: 0
  #   fix_backbone_for_no_res: False
  #   fix_concept_embeddings_for_no_res: False
  #   use_ground_truth_mixing_for_no_res: False

  #   e2e_epochs: 300
  #   fix_backbone_for_res: False
  #   freeze_emb_generators_for_res: False
  #   fix_concept_embeddings_for_res: False
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #     - emb_size
  #     - residual_scale_reg
  #     - extra_capacity_dropout_prob
  #     - extra_capacity
  #   grid_search_mode: exhaustive



  # - architecture: "IntAwareMixCEM"
  #   run_name: "IntAwareMixCEM_cwl_{concept_loss_weight}_scale_reg_{residual_scale_reg}_emb_size_{emb_size}_norm_res_{normalize_residual}_intervention_weight_{intervention_weight}_intervention_task_discount_{intervention_task_discount}"
  #   training_intervention_prob: 0.25
  #   intervention_weight: [0.1]
  #   emb_size: 16
  #   intervention_task_discount: [1.1]
  #   concept_loss_weight: [1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   embedding_activation: "leakyrelu"
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   gradient_clip_val: 100

  #   residual_scale_reg: [0, 0.1, 0.001]
  #   residual_scale_norm_metric: 1
  #   normalize_residual: False
  #   sigmoidal_residual_scale: False

  #   grid_variables:
  #       - concept_loss_weight
  #       - intervention_task_discount
  #       - intervention_weight
  #       - residual_scale_reg
  #   grid_search_mode: exhaustive








  # - architecture: 'NewMixingConceptEmbeddingModel'
  #   run_name: "Try_MixCEM_noise_{residual_scale_reg}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_rnl_{residual_norm_loss}_pcr_{per_concept_residual}_shared_{shared_per_concept_residual}_itl_{intermediate_task_concept_loss}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [32] #[512, 1024]
  #   embedding_activation: null
  #   concept_loss_weight: [1] #, 1] #, 0.1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.05] #, 1.1] #, 1.01] #, 1.05]
  #   use_cosine_similarity: False
  #   early_stopping_best_model: False
  #   conditional_residual: [True] # BIG CHANGE: [True] #False, True]
  #   # CHANGE: c2y_layers: [256, 128, 64]
  #   c2y_layers: [256, 128] #[256, 128, 64]
  #   residual_layers: [256, 128] #[256, 128, 64]
  #   bottleneck_pooling: ['concat'] #, 'concat']
  #   per_concept_residual: [True] # BIG CHANGE: [True]
  #   sigmoidal_residual: [False]
  #   residual_deviation: [0] #, 2]
  #   shared_per_concept_residual: [False] # Change: [True]
  #   residual_norm_loss: [0] # CHANGE: [0]
  #   intermediate_task_concept_loss: 0
  #   # CHANGE: gradient_clip_val: 100
  #   residual_scale: null #1 # CHANGE
  #   learnable_residual_scale: False # CHANGE: True
  #   sigmoidal_residual_scale: False # CHANGE: True
  #   learn_residual_embeddings: True # CHANGE
  #   noise_residual_embedings:  True # CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   residual_scale_reg: [0] #CHANGE: 0.1
  #   concept_model_path: 'Try_MixCEM_concept_model_{emb_size}_{blackbox_warmup_epochs}_{split}'
  #   warmup_model_path: 'Try_MixCEM_warmup_model_{emb_size}_{concept_epochs}_{fix_backbone_for_concept}_{split}'

  #   blackbox_warmup_epochs: 75 # CHANGE: 100

  #   concept_epochs: 100 # CHANGE: 100
  #   fix_backbone_for_concept: True #CHANGE: True

  #   no_residual_epochs: 100 # CHANGE: 100
  #   fix_backbone_for_no_res: True # CHANGE: True
  #   fix_concept_embeddings_for_no_res: False
  #   use_ground_truth_mixing_for_no_res: False # CHANGEEEEEEEEEE

  #   e2e_epochs: 100 # CHANGE: 200
  #   fix_backbone_for_res: True
  #   fix_concept_embeddings_for_res: True
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #     - emb_size
  #     - residual_scale_reg
  #   grid_search_mode: exhaustive

  #   dataset_config:
  #     dataset: "cub"
  #     num_workers: 8
  #     batch_size: 128 #64

  #     # DATASET VARIABLES
  #     root_dir: /homes/me466/data/CUB200/
  #     sampling_percent: 0.25  # [IMPORTANT] Select only a quarter of all concepts!
  #     sampling_groups: True
  #     test_subsampling: 1
  #     weight_loss: True






  # - architecture: 'NewMixingConceptEmbeddingModel'
  #   run_name: "Try_MixCEM_baseline_{use_latent_space}_{residual_model_weight_l2_reg}_{residual_scale_reg}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_rnl_{residual_norm_loss}_pcr_{per_concept_residual}_shared_{shared_per_concept_residual}_itl_{intermediate_task_concept_loss}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [32]
  #   embedding_activation: null
  #   concept_loss_weight: [10] #, 1] #, 0.1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.1, 1.5] #, 1.1] #, 1.01] #, 1.05]
  #   use_cosine_similarity: False
  #   early_stopping_best_model: False
  #   dynamic_residual: False
  #   conditional_residual: [True] # KEY
  #   c2y_layers: []
  #   residual_layers: []
  #   bottleneck_pooling: ['concat']
  #   per_concept_residual: [False] # KEY
  #   sigmoidal_residual: [False] # KEY
  #   residual_deviation: [0]
  #   shared_per_concept_residual: [False] # KEY
  #   intermediate_task_concept_loss: 0
  #   residual_scale: null
  #   learnable_residual_scale: True # KEY
  #   sigmoidal_residual_scale: True # KEY
  #   learn_residual_embeddings: False


  #   learnable_distance_metric: False
  #   learnable_prob_model: False
  #   use_latent_space: False

  #   residual_model_weight_l2_reg: 0
  #   residual_norm_loss: [0]
  #   residual_scale_reg: [0.1]
  #   residual_norm_metric: 2
  #   residual_scale_norm_metric: 1

  #   blackbox_warmup_epochs: 0

  #   concept_epochs: 0
  #   fix_backbone_for_concept: False
  #   freeze_emb_generators_for_concept: False

  #   no_residual_epochs: 300
  #   fix_backbone_for_no_res: False
  #   fix_concept_embeddings_for_no_res: False
  #   use_ground_truth_mixing_for_no_res: False

  #   e2e_epochs: 300
  #   fix_backbone_for_res: False
  #   freeze_emb_generators_for_res: False
  #   fix_concept_embeddings_for_res: False
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #     - emb_size
  #     - residual_scale_reg
  #   grid_search_mode: exhaustive


  # - architecture: 'NewMixingConceptEmbeddingModel'
  #   run_name: "Try_MixCEM_dyn_from_scratch_no_dist_{residual_scale_reg}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_rnl_{residual_norm_loss}_pcr_{per_concept_residual}_shared_{shared_per_concept_residual}_itl_{intermediate_task_concept_loss}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [0.25, [0.25, 0.5, 0.75, 1]]
  #   emb_size: [32] #[512, 1024]
  #   embedding_activation: null
  #   concept_loss_weight: [1] #, 1] #, 0.1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.1, 1.5] #, 1.1] #, 1.01] #, 1.05]
  #   use_cosine_similarity: False
  #   early_stopping_best_model: False
  #   dynamic_residual: True # CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   conditional_residual: [False] # BIG CHANGE: [True] #False, True]
  #   # CHANGE: c2y_layers: [256, 128, 64]
  #   c2y_layers: [] #[256, 128, 64]
  #   residual_layers: [] #[256, 128, 64]
  #   bottleneck_pooling: ['concat'] #, 'concat']
  #   per_concept_residual: [True] # BIG CHANGE: [True]
  #   sigmoidal_residual: [False]
  #   residual_deviation: [0] #, 2]
  #   shared_per_concept_residual: [True] # Change: [True]
  #   residual_norm_loss: [0, 0.1, 1] # CHANGE: [0]
  #   intermediate_task_concept_loss: 0
  #   # CHANGE: gradient_clip_val: 100
  #   residual_scale: null #1 # CHANGE
  #   learnable_residual_scale: True
  #   sigmoidal_residual_scale: False #True
  #   learn_residual_embeddings: False #True # CHANGE
  #   residual_scale_reg: [0, 0.1, 1] #CHANGE: 0.1
  #   warmup_model_path: 'Try_MixCEM_warmup_model_{emb_size}_{blackbox_warmup_epochs}_{split}'
  #   concept_model_path: 'Try_MixCEM_dyn_concept_model_{emb_size}_{concept_epochs}_{blackbox_warmup_epochs}_{fix_backbone_for_concept}_{freeze_emb_generators_for_concept}_{split}'
  #   residual_norm_metric: 2
  #   residual_scale_norm_metric: 1

  #   learnable_distance_metric: False
  #   learnable_prob_model: True
  #   use_latent_space: True

  #   blackbox_warmup_epochs: 0

  #   concept_epochs: 0
  #   fix_backbone_for_concept: False
  #   freeze_emb_generators_for_concept: False

  #   no_residual_epochs: 0
  #   fix_backbone_for_no_res: False
  #   fix_concept_embeddings_for_no_res: False
  #   use_ground_truth_mixing_for_no_res: False

  #   e2e_epochs: 300
  #   fix_backbone_for_res: False
  #   freeze_emb_generators_for_res: False
  #   fix_concept_embeddings_for_res: False
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #     - emb_size
  #     - residual_scale_reg
  #   grid_search_mode: exhaustive

  #   dataset_config:
  #     dataset: "cub"
  #     num_workers: 8
  #     batch_size: 256 #64

  #     # DATASET VARIABLES
  #     root_dir: /homes/me466/data/CUB200/
  #     sampling_percent: 0.25  # [IMPORTANT] Select only a quarter of all concepts!
  #     sampling_groups: True
  #     test_subsampling: 1
  #     weight_loss: True







  # WORKS OK!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  # - architecture: 'NewMixingConceptEmbeddingModel'
  #   run_name: "Try_MixCEM_dyn_res_from_scratch_{residual_scale_reg}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_rnl_{residual_norm_loss}_pcr_{per_concept_residual}_shared_{shared_per_concept_residual}_itl_{intermediate_task_concept_loss}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [1024] #[512, 1024]
  #   embedding_activation: null
  #   concept_loss_weight: [1] #, 1] #, 0.1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.1, 1.5] #, 1.1] #, 1.01] #, 1.05]
  #   use_cosine_similarity: False
  #   early_stopping_best_model: False
  #   dynamic_residual: True # CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   conditional_residual: [False] # BIG CHANGE: [True] #False, True]
  #   # CHANGE: c2y_layers: [256, 128, 64]
  #   c2y_layers: [] #[256, 128, 64]
  #   residual_layers: [] #[256, 128, 64]
  #   bottleneck_pooling: ['concat'] #, 'concat']
  #   per_concept_residual: [True] # BIG CHANGE: [True]
  #   sigmoidal_residual: [False]
  #   residual_deviation: [0] #, 2]
  #   shared_per_concept_residual: [True] # Change: [True]
  #   residual_norm_loss: [0.1, 1] # CHANGE: [0]
  #   intermediate_task_concept_loss: 0
  #   # CHANGE: gradient_clip_val: 100
  #   residual_scale: null #1 # CHANGE
  #   learnable_residual_scale: True
  #   sigmoidal_residual_scale: False #True
  #   learn_residual_embeddings: False #True # CHANGE
  #   residual_scale_reg: [0.1, 1] #CHANGE: 0.1
  #   warmup_model_path: 'Try_MixCEM_warmup_model_{emb_size}_{blackbox_warmup_epochs}_{split}'
  #   concept_model_path: 'Try_MixCEM_dyn_concept_model_{emb_size}_{concept_epochs}_{blackbox_warmup_epochs}_{fix_backbone_for_concept}_{freeze_emb_generators_for_concept}_{split}'
  #   residual_norm_metric: 2
  #   residual_scale_norm_metric: 1

  #   blackbox_warmup_epochs: 0

  #   concept_epochs: 0
  #   fix_backbone_for_concept: False
  #   freeze_emb_generators_for_concept: False

  #   no_residual_epochs: 0
  #   fix_backbone_for_no_res: False
  #   fix_concept_embeddings_for_no_res: False
  #   use_ground_truth_mixing_for_no_res: False

  #   e2e_epochs: 300
  #   fix_backbone_for_res: False
  #   freeze_emb_generators_for_res: False
  #   fix_concept_embeddings_for_res: False
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #     - emb_size
  #     - residual_scale_reg
  #   grid_search_mode: exhaustive

  #   dataset_config:
  #     dataset: "cub"
  #     num_workers: 8
  #     batch_size: 256 #64

  #     # DATASET VARIABLES
  #     root_dir: /homes/me466/data/CUB200/
  #     sampling_percent: 0.25  # [IMPORTANT] Select only a quarter of all concepts!
  #     sampling_groups: True
  #     test_subsampling: 1
  #     weight_loss: True


  # - architecture: 'NewMixingConceptEmbeddingModel'
  #   run_name: "Try_MixCEM_dyn_res_fixed_{residual_scale_reg}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_rnl_{residual_norm_loss}_pcr_{per_concept_residual}_shared_{shared_per_concept_residual}_itl_{intermediate_task_concept_loss}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [128] #[512, 1024]
  #   embedding_activation: null
  #   concept_loss_weight: [1] #, 1] #, 0.1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.1] #, 1.1] #, 1.01] #, 1.05]
  #   use_cosine_similarity: False
  #   early_stopping_best_model: False
  #   dynamic_residual: True # CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   conditional_residual: [False] # BIG CHANGE: [True] #False, True]
  #   # CHANGE: c2y_layers: [256, 128, 64]
  #   c2y_layers: [] #[256, 128, 64]
  #   residual_layers: [] #[256, 128, 64]
  #   bottleneck_pooling: ['concat'] #, 'concat']
  #   per_concept_residual: [True] # BIG CHANGE: [True]
  #   sigmoidal_residual: [False]
  #   residual_deviation: [0] #, 2]
  #   shared_per_concept_residual: [True] # Change: [True]
  #   residual_norm_loss: [0.1, 1] # CHANGE: [0]
  #   intermediate_task_concept_loss: 0
  #   # CHANGE: gradient_clip_val: 100
  #   residual_scale: null #1 # CHANGE
  #   learnable_residual_scale: True
  #   sigmoidal_residual_scale: False #True
  #   learn_residual_embeddings: False #True # CHANGE
  #   residual_scale_reg: [0.1, 1] #CHANGE: 0.1
  #   warmup_model_path: 'Try_MixCEM_warmup_model_{emb_size}_{blackbox_warmup_epochs}_{split}'
  #   concept_model_path: 'Try_MixCEM_dyn_concept_model_{emb_size}_{concept_epochs}_{blackbox_warmup_epochs}_{fix_backbone_for_concept}_{freeze_emb_generators_for_concept}_{split}'
  #   residual_norm_metric: 2
  #   residual_scale_norm_metric: 1

  #   blackbox_warmup_epochs: 150

  #   concept_epochs: 150
  #   fix_backbone_for_concept: True
  #   freeze_emb_generators_for_concept: False

  #   no_residual_epochs: 0
  #   fix_backbone_for_no_res: False
  #   fix_concept_embeddings_for_no_res: False
  #   use_ground_truth_mixing_for_no_res: False

  #   e2e_epochs: 150
  #   fix_backbone_for_res: True
  #   freeze_emb_generators_for_res: False
  #   fix_concept_embeddings_for_res: False
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #     - emb_size
  #     - residual_scale_reg
  #   grid_search_mode: exhaustive

  #   dataset_config:
  #     dataset: "cub"
  #     num_workers: 8
  #     batch_size: 256 #64

  #     # DATASET VARIABLES
  #     root_dir: /homes/me466/data/CUB200/
  #     sampling_percent: 0.25  # [IMPORTANT] Select only a quarter of all concepts!
  #     sampling_groups: True
  #     test_subsampling: 1
  #     weight_loss: True


  # - architecture: 'NewMixingConceptEmbeddingModel'
  #   run_name: "Try_MixCEM_dyn_res_{residual_scale_reg}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_rnl_{residual_norm_loss}_pcr_{per_concept_residual}_shared_{shared_per_concept_residual}_itl_{intermediate_task_concept_loss}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [32, 128, 512, 1024] #[512, 1024]
  #   embedding_activation: null
  #   concept_loss_weight: [1] #, 1] #, 0.1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.01, 1.05] #, 1.1] #, 1.01] #, 1.05]
  #   use_cosine_similarity: False
  #   early_stopping_best_model: False
  #   dynamic_residual: True # CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   conditional_residual: [False] # BIG CHANGE: [True] #False, True]
  #   # CHANGE: c2y_layers: [256, 128, 64]
  #   c2y_layers: [] #[256, 128, 64]
  #   residual_layers: [] #[256, 128, 64]
  #   bottleneck_pooling: ['concat'] #, 'concat']
  #   per_concept_residual: [True] # BIG CHANGE: [True]
  #   sigmoidal_residual: [False]
  #   residual_deviation: [0] #, 2]
  #   shared_per_concept_residual: [True] # Change: [True]
  #   residual_norm_loss: [0.1, 1] # CHANGE: [0]
  #   intermediate_task_concept_loss: 0
  #   # CHANGE: gradient_clip_val: 100
  #   residual_scale: null #1 # CHANGE
  #   learnable_residual_scale: True
  #   sigmoidal_residual_scale: False #True
  #   learn_residual_embeddings: False #True # CHANGE
  #   residual_scale_reg: [0.1, 1] #CHANGE: 0.1
  #   concept_model_path: 'Try_MixCEM_dyn_concept_model_{emb_size}_{blackbox_warmup_epochs}_{split}'
  #   # warmup_model_path: 'Try_MixCEM_warmup_model_{emb_size}_{concept_epochs}_{fix_backbone_for_concept}_{split}'
  #   residual_norm_metric: 2
  #   residual_scale_norm_metric: 1

  #   blackbox_warmup_epochs: 0

  #   concept_epochs: 75
  #   fix_backbone_for_concept: False

  #   no_residual_epochs: 0
  #   fix_backbone_for_no_res: False
  #   fix_concept_embeddings_for_no_res: False
  #   use_ground_truth_mixing_for_no_res: False

  #   e2e_epochs: 300
  #   fix_backbone_for_res: False
  #   fix_concept_embeddings_for_res: False
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #     - emb_size
  #     - residual_scale_reg
  #   grid_search_mode: exhaustive

  #   dataset_config:
  #     dataset: "cub"
  #     num_workers: 8
  #     batch_size: 128 #64

  #     # DATASET VARIABLES
  #     root_dir: /homes/me466/data/CUB200/
  #     sampling_percent: 0.25  # [IMPORTANT] Select only a quarter of all concepts!
  #     sampling_groups: True
  #     test_subsampling: 1
  #     weight_loss: True



  # - architecture: 'NewMixingConceptEmbeddingModel'
  #   run_name: "Try_MixCEM_ind_{residual_scale_reg}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_rnl_{residual_norm_loss}_pcr_{per_concept_residual}_shared_{shared_per_concept_residual}_itl_{intermediate_task_concept_loss}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [1024, 512] #[512, 1024]
  #   embedding_activation: null
  #   concept_loss_weight: [1] #, 1] #, 0.1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.01, 1.05] #, 1.1] #, 1.01] #, 1.05]
  #   use_cosine_similarity: False
  #   early_stopping_best_model: False
  #   conditional_residual: [True] # BIG CHANGE: [True] #False, True]
  #   # CHANGE: c2y_layers: [256, 128, 64]
  #   c2y_layers: [256, 128] #[256, 128, 64]
  #   residual_layers: [256, 128] #[256, 128, 64]
  #   bottleneck_pooling: ['concat'] #, 'concat']
  #   per_concept_residual: [True] # BIG CHANGE: [True]
  #   sigmoidal_residual: [False]
  #   residual_deviation: [0] #, 2]
  #   shared_per_concept_residual: [True] # Change: [True]
  #   residual_norm_loss: [0.1, 5, 10] # CHANGE: [0]
  #   intermediate_task_concept_loss: 0
  #   # CHANGE: gradient_clip_val: 100
  #   residual_scale: null #1 # CHANGE
  #   learnable_residual_scale: True
  #   sigmoidal_residual_scale: False #True
  #   learn_residual_embeddings: False #True # CHANGE
  #   residual_scale_reg: [0.1] #CHANGE: 0.1
  #   # concept_model_path: 'Try_MixCEM_concept_model_{emb_size}_{blackbox_warmup_epochs}_{split}'
  #   # warmup_model_path: 'Try_MixCEM_warmup_model_{emb_size}_{concept_epochs}_{fix_backbone_for_concept}_{split}'
  #   residual_norm_metric: 2 # DIFFERENCE!!!!!!!!!!!!!!!!!
  #   residual_scale_norm_metric: 1

  #   blackbox_warmup_epochs: 0 # CHANGE: 100

  #   concept_epochs: 0 # CHANGE: 100
  #   fix_backbone_for_concept: True #CHANGE: True

  #   no_residual_epochs: 0 # CHANGE: 100
  #   fix_backbone_for_no_res: True # CHANGE: True
  #   fix_concept_embeddings_for_no_res: True
  #   use_ground_truth_mixing_for_no_res: True # CHANGEEEEEEEEEE

  #   e2e_epochs: 300 # CHANGE: 200
  #   fix_backbone_for_res: False
  #   fix_concept_embeddings_for_res: False
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #     - emb_size
  #     - residual_scale_reg
  #   grid_search_mode: exhaustive

  #   dataset_config:
  #     dataset: "cub"
  #     num_workers: 8
  #     batch_size: 128 #64

  #     # DATASET VARIABLES
  #     root_dir: /homes/me466/data/CUB200/
  #     sampling_percent: 0.25  # [IMPORTANT] Select only a quarter of all concepts!
  #     sampling_groups: True
  #     test_subsampling: 1
  #     weight_loss: True


  # - architecture: 'NewMixingConceptEmbeddingModel'
  #   run_name: "Try_MixCEM_{residual_scale_reg}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_rnl_{residual_norm_loss}_pcr_{per_concept_residual}_shared_{shared_per_concept_residual}_itl_{intermediate_task_concept_loss}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [256, 1024] #[512, 1024]
  #   embedding_activation: null
  #   concept_loss_weight: [1, 5] #, 1] #, 0.1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.05] #, 1.1] #, 1.01] #, 1.05]
  #   use_cosine_similarity: False
  #   early_stopping_best_model: False
  #   conditional_residual: [True] # BIG CHANGE: [True] #False, True]
  #   # CHANGE: c2y_layers: [256, 128, 64]
  #   c2y_layers: [256] #[256, 128, 64]
  #   residual_layers: [256] #[256, 128, 64]
  #   bottleneck_pooling: ['per_class_mixing', 'per_class_mixing_shared', 'concat'] #, 'concat']
  #   per_concept_residual: [True] # BIG CHANGE: [True]
  #   sigmoidal_residual: [True]
  #   residual_deviation: [0] #, 2]
  #   shared_per_concept_residual: [False] # Change: [True]
  #   residual_norm_loss: [0, 10] # CHANGE: [0]
  #   intermediate_task_concept_loss: 0
  #   # CHANGE: gradient_clip_val: 100
  #   residual_scale: null #1 # CHANGE
  #   learnable_residual_scale: True
  #   sigmoidal_residual_scale: True
  #   learn_residual_embeddings: True # CHANGE
  #   residual_scale_reg: [0] #CHANGE: 0.1
  #   concept_model_path: 'Try_MixCEM_concept_model_{emb_size}_{concept_epochs}_{fix_backbone_for_concept}_{split}'

  #   blackbox_warmup_epochs: 0 # CHANGE: 100

  #   concept_epochs: 100 # CHANGE: 100
  #   fix_backbone_for_concept: False #CHANGE: True

  #   no_residual_epochs: 0 # CHANGE: 100
  #   fix_backbone_for_no_res: False # CHANGE: True
  #   fix_concept_embeddings_for_no_res: True

  #   e2e_epochs: 200 # CHANGE: 100
  #   fix_backbone_for_res: False
  #   fix_concept_embeddings_for_res: True
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #     - emb_size
  #     - residual_scale_reg
  #   grid_search_mode: exhaustive

  #   dataset_config:
  #     dataset: "cub"
  #     num_workers: 8
  #     batch_size: 128 #64

  #     # DATASET VARIABLES
  #     root_dir: /homes/me466/data/CUB200/
  #     sampling_percent: 0.25  # [IMPORTANT] Select only a quarter of all concepts!
  #     sampling_groups: True
  #     test_subsampling: 1
  #     weight_loss: True


  # - architecture: 'NewMixingConceptEmbeddingModel'
  #   run_name: "Try_MixCEM_attempt_5_{residual_scale_reg}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_rnl_{residual_norm_loss}_pcr_{per_concept_residual}_shared_{shared_per_concept_residual}_itl_{intermediate_task_concept_loss}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [32] #[512, 1024]
  #   embedding_activation: null
  #   concept_loss_weight: [1] #, 5] #, 0.1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.05] #, 1.01] #, 1.05]
  #   use_cosine_similarity: False
  #   early_stopping_best_model: False
  #   conditional_residual: [True] #False, True]
  #   c2y_layers: [512]
  #   residual_layers: [512]
  #   bottleneck_pooling: ['per_class_mixing_shared'] #, 'concat']
  #   per_concept_residual: [True]
  #   sigmoidal_residual: [False]
  #   residual_deviation: [0] #, 2]
  #   shared_per_concept_residual: [False] # Change: [True]
  #   residual_norm_loss: [0] # CHANGE: [0.1]
  #   intermediate_task_concept_loss: 0
  #   # CHANGE: gradient_clip_val: 100
  #   learnable_residual_scale: True  # CHANGE: False
  #   sigmoidal_residual_scale: False
  #   residual_scale_reg: [0.1] #CHANGE: 0

  #   # patience: 2 #15
  #   # check_val_every_n_epoch: 4 #5
  #   # patience: 5 # 10

  #   residual_scale: null #1 # CHANGE
  #   blackbox_warmup_epochs: 0 # CHANGE: 100

  #   concept_epochs: 100 # CHANGE: 100
  #   fix_backbone_for_concept: False #CHANGE: True

  #   no_residual_epochs: 0 # CHANGE: 100
  #   fix_backbone_for_no_res: False # CHANGE: True
  #   fix_concept_embeddings_for_no_res: False

  #   e2e_epochs: 300 # CHANGE: 100
  #   fix_backbone_for_res: True # CHANGE: True
  #   fix_concept_embeddings_for_res: True
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #     - emb_size
  #     - residual_scale_reg
  #   grid_search_mode: exhaustive


  #   dataset_config:
  #     dataset: "cub"
  #     num_workers: 8
  #     batch_size: 256 #64

  #     # DATASET VARIABLES
  #     root_dir: /homes/me466/data/CUB200/
  #     sampling_percent: 0.25  # [IMPORTANT] Select only a quarter of all concepts!
  #     sampling_groups: True
  #     test_subsampling: 1
  #     weight_loss: True


  # - architecture: 'NewMixingConceptEmbeddingModel'
  #   run_name: "Try_MixCEM_attempt_4_{residual_scale_reg}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_rnl_{residual_norm_loss}_pcr_{per_concept_residual}_shared_{shared_per_concept_residual}_itl_{intermediate_task_concept_loss}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [1024, 512] #[512, 1024]
  #   embedding_activation: null
  #   concept_loss_weight: [1, 5] #, 0.1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.05, 1.01] #, 1.05]
  #   use_cosine_similarity: False
  #   early_stopping_best_model: False
  #   conditional_residual: [True] #False, True]
  #   # CHANGE: c2y_layers: [32, 16, 8]
  #   residual_layers: [32, 16, 8]
  #   bottleneck_pooling: ['per_class_mixing_shared', 'concat']
  #   per_concept_residual: [True]
  #   sigmoidal_residual: [False]
  #   residual_deviation: [0] #, 2]
  #   shared_per_concept_residual: [False] # Change: [True]
  #   residual_norm_loss: [0] # CHANGE: [0.1]
  #   intermediate_task_concept_loss: 0
  #   # CHANGE: gradient_clip_val: 100
  #   learnable_residual_scale: True  # CHANGE: False
  #   sigmoidal_residual_scale: False
  #   residual_scale_reg: [0.01, 0.1] #CHANGE: 0

  #   # patience: 2 #15
  #   # check_val_every_n_epoch: 4 #5
  #   # patience: 5 # 10

  #   residual_scale: null #1 # CHANGE
  #   blackbox_warmup_epochs: 0 # CHANGE: 100

  #   concept_epochs: 50 # CHANGE: 100
  #   fix_backbone_for_concept: False #CHANGE: True

  #   no_residual_epochs: 0 # CHANGE: 100
  #   fix_backbone_for_no_res: False # CHANGE: True
  #   fix_concept_embeddings_for_no_res: False

  #   e2e_epochs: 300 # CHANGE: 100
  #   fix_backbone_for_res: False # CHANGE: True
  #   fix_concept_embeddings_for_res: False
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - bottleneck_pooling
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #     - emb_size
  #     - residual_scale_reg
  #   grid_search_mode: exhaustive

  # - architecture: 'NewMixingConceptEmbeddingModel'
  #   run_name: "Try_MixCEM_attempt_3_{residual_scale_reg}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_rnl_{residual_norm_loss}_pcr_{per_concept_residual}_shared_{shared_per_concept_residual}_itl_{intermediate_task_concept_loss}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [32] #[512, 1024]
  #   embedding_activation: null
  #   concept_loss_weight: [1, 5] #, 0.1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.01, 1.05]
  #   use_cosine_similarity: False
  #   early_stopping_best_model: False
  #   conditional_residual: [True] #False, True]
  #   # CHANGE: c2y_layers: [32, 16, 8]
  #   residual_layers: [32, 16, 8]
  #   bottleneck_pooling: 'concat'
  #   per_concept_residual: [True]
  #   sigmoidal_residual: [False]
  #   residual_deviation: [0] #, 2]
  #   shared_per_concept_residual: [False] # Change: [True]
  #   residual_norm_loss: [0] # CHANGE: [0.1]
  #   intermediate_task_concept_loss: 0
  #   # CHANGE: gradient_clip_val: 100
  #   learnable_residual_scale: True  # CHANGE: False
  #   sigmoidal_residual_scale: False
  #   residual_scale_reg: 0.1 #CHANGE: 0

  #   # patience: 2 #15
  #   # check_val_every_n_epoch: 4 #5
  #   # patience: 5 # 10

  #   residual_scale: null #1 # CHANGE
  #   blackbox_warmup_epochs: 0 # CHANGE: 100

  #   concept_epochs: 0 # CHANGE: 100
  #   fix_backbone_for_concept: False #CHANGE: True

  #   no_residual_epochs: 0 # CHANGE: 100
  #   fix_backbone_for_no_res: False # CHANGE: True
  #   fix_concept_embeddings_for_no_res: False

  #   e2e_epochs: 300 # CHANGE: 100
  #   fix_backbone_for_res: False # CHANGE: True
  #   fix_concept_embeddings_for_res: False
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #     - emb_size
  #   grid_search_mode: exhaustive


  # - architecture: 'NewMixingConceptEmbeddingModel'
  #   run_name: "Try_MixCEM_attempt_2_{residual_scale_reg}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_rnl_{residual_norm_loss}_pcr_{per_concept_residual}_shared_{shared_per_concept_residual}_itl_{intermediate_task_concept_loss}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [1024] #[512, 1024]
  #   embedding_activation: null
  #   concept_loss_weight: [1, 5] #, 0.1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.01, 1.05]
  #   use_cosine_similarity: False
  #   early_stopping_best_model: False
  #   conditional_residual: [True] #False, True]
  #   c2y_layers: [32, 16, 8]
  #   residual_layers: [32, 16, 8]
  #   bottleneck_pooling: 'concat'
  #   per_concept_residual: [True]
  #   sigmoidal_residual: [False]
  #   residual_deviation: [0] #, 2]
  #   shared_per_concept_residual: [True]
  #   residual_norm_loss: [0] # CHANGE: [0.1]
  #   intermediate_task_concept_loss: 0
  #   gradient_clip_val: 100
  #   learnable_residual_scale: True  #False # CHANGE
  #   sigmoidal_residual_scale: False
  #   residual_scale_reg: 0.1 #0 CHANGE

  #   # patience: 2 #15
  #   # check_val_every_n_epoch: 4 #5
  #   # patience: 5 # 10

  #   residual_scale: null #1 # CHANGE
  #   blackbox_warmup_epochs: 0 # CHANGE: 100

  #   concept_epochs: 100 # CHANGE: 100
  #   fix_backbone_for_concept: False #CHANGE: True

  #   no_residual_epochs: 0 # CHANGE: 100
  #   fix_backbone_for_no_res: False # CHANGE: True
  #   fix_concept_embeddings_for_no_res: False

  #   e2e_epochs: 300 # CHANGE: 100
  #   fix_backbone_for_res: False # CHANGE: True
  #   fix_concept_embeddings_for_res: False
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #     - emb_size
  #   grid_search_mode: exhaustive

  #   dataset_config:
  #     dataset: "cub"
  #     num_workers: 8
  #     batch_size: 128 #512

  #     # DATASET VARIABLES
  #     root_dir: /homes/me466/data/CUB200/
  #     sampling_percent: 0.25  # [IMPORTANT] Select only a quarter of all concepts!
  #     sampling_groups: True
  #     test_subsampling: 1
  #     weight_loss: True

  #   # Intervention Parameters
  #   intervention_config:
  #     competence_levels: [1]
  #     intervention_freq: 1
  #     intervention_batch_size: 256
  #     val_intervention_policies:
  #       - policy: "random"
  #         group_level: True
  #         use_prior: False
  #     intervention_policies:
  #       - policy: "random"
  #         group_level: True
  #         use_prior: False

  # - architecture: 'NewMixingConceptEmbeddingModel'
  #   run_name: "Try_MixCEM_attempt_{residual_scale_reg}_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_rnl_{residual_norm_loss}_pcr_{per_concept_residual}_shared_{shared_per_concept_residual}_itl_{intermediate_task_concept_loss}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [2048, 1024, 64] #[512, 1024]
  #   embedding_activation: null
  #   concept_loss_weight: [1, 5] #, 0.1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.01, 1.05]
  #   use_cosine_similarity: False
  #   early_stopping_best_model: False
  #   conditional_residual: [True] #False, True]
  #   c2y_layers: [32, 16, 8]
  #   residual_layers: [32, 16, 8]
  #   bottleneck_pooling: 'per_class_mixing_shared' #'concat'  # CHANGE
  #   per_concept_residual: [True]
  #   sigmoidal_residual: [False]
  #   residual_deviation: [0] #, 2]
  #   shared_per_concept_residual: [True]
  #   residual_norm_loss: [0] # CHANGE: [0.1]
  #   intermediate_task_concept_loss: 0
  #   gradient_clip_val: 100
  #   learnable_residual_scale: True  #False # CHANGE
  #   residual_scale_reg: 0.1 #0 CHANGE

  #   # patience: 2 #15
  #   # check_val_every_n_epoch: 4 #5
  #   # patience: 5 # 10

  #   residual_scale: null #1 # CHANGE
  #   blackbox_warmup_epochs: 0 # CHANGE: 100

  #   concept_epochs: 100 # CHANGE: 100
  #   fix_backbone_for_concept: False #CHANGE: True

  #   no_residual_epochs: 0 # CHANGE: 100
  #   fix_backbone_for_no_res: False # CHANGE: True
  #   fix_concept_embeddings_for_no_res: False

  #   e2e_epochs: 300 # CHANGE: 100
  #   fix_backbone_for_res: False # CHANGE: True
  #   fix_concept_embeddings_for_res: False
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #     - emb_size
  #   grid_search_mode: exhaustive

  #   dataset_config:
  #     dataset: "cub"
  #     num_workers: 8
  #     batch_size: 128 #512

  #     # DATASET VARIABLES
  #     root_dir: /homes/me466/data/CUB200/
  #     sampling_percent: 0.25  # [IMPORTANT] Select only a quarter of all concepts!
  #     sampling_groups: True
  #     test_subsampling: 1
  #     weight_loss: True

  #   # Intervention Parameters
  #   intervention_config:
  #     competence_levels: [1]
  #     intervention_freq: 1
  #     intervention_batch_size: 256
  #     val_intervention_policies:
  #       - policy: "random"
  #         group_level: True
  #         use_prior: False
  #     intervention_policies:
  #       - policy: "random"
  #         group_level: True
  #         use_prior: False

  # - architecture: 'NewMixingConceptEmbeddingModel'
  #   run_name: "Try_MixCEM_posthoc_embs_{blackbox_warmup_epochs}_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_rnl_{residual_norm_loss}_pcr_{per_concept_residual}_shared_{shared_per_concept_residual}_itl_{intermediate_task_concept_loss}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [1024] #[512, 1024]
  #   embedding_activation: null
  #   concept_loss_weight: [1] #, 0.1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.01, 1.05]
  #   use_cosine_similarity: False
  #   early_stopping_best_model: False
  #   conditional_residual: [True] #False, True]
  #   residual_layers: [32, 16, 8]
  #   bottleneck_pooling: 'concat'
  #   per_concept_residual: [True]
  #   sigmoidal_residual: [False]
  #   residual_deviation: [0] #, 2]
  #   shared_per_concept_residual: [True]
  #   residual_norm_loss: [0.1] #, 0]
  #   intermediate_task_concept_loss: 0
  #   gradient_clip_val: 100

  #   patience: 2 #15
  #   check_val_every_n_epoch: 4 #5
  #   patience: 5 # 10

  #   residual_scale: 1
  #   blackbox_warmup_epochs: 100

  #   concept_epochs: 100
  #   fix_backbone_for_concept: True

  #   no_residual_epochs: 100
  #   fix_backbone_for_no_res: True
  #   fix_concept_embeddings_for_no_res: True

  #   e2e_epochs: 100
  #   fix_backbone_for_res: True
  #   fix_concept_embeddings_for_res: True
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - concept_loss_weight
  #     - emb_size
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #   grid_search_mode: exhaustive

  # - architecture: 'NewMixingConceptEmbeddingModel'
  #   run_name: "Try_MixCEM_fixed_embs_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_rnl_{residual_norm_loss}_pcr_{per_concept_residual}_shared_{shared_per_concept_residual}_itl_{intermediate_task_concept_loss}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [32, 512] #[512, 1024]
  #   embedding_activation: null
  #   concept_loss_weight: [5] #, 0.1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.01, 1.05]
  #   use_cosine_similarity: False
  #   early_stopping_best_model: False
  #   conditional_residual: [True] #False, True]
  #   residual_layers: [32, 16, 8]
  #   bottleneck_pooling: 'concat'
  #   per_concept_residual: [True]
  #   sigmoidal_residual: [False]
  #   residual_deviation: [0] #, 2]
  #   shared_per_concept_residual: [True]
  #   residual_norm_loss: [0.1] #, 0]
  #   intermediate_task_concept_loss: 0
  #   gradient_clip_val: 100

  #   residual_scale: 1
  #   blackbox_warmup_epochs: 0

  #   concept_epochs: 50
  #   fix_backbone_for_concept: False

  #   no_residual_epochs: 50
  #   fix_backbone_for_no_res: False
  #   fix_concept_embeddings_for_no_res: True

  #   e2e_epochs: 150
  #   fix_backbone_for_res: False
  #   fix_concept_embeddings_for_res: True
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - concept_loss_weight
  #     - emb_size
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #   grid_search_mode: exhaustive

  # - architecture: 'NewMixingConceptEmbeddingModel'
  #   run_name: "Try_MixCEM_staged_{concept_epochs}_{no_residual_epochs}_{e2e_epochs}_ip_{training_intervention_prob}_emb_size_{emb_size}_dis_{intervention_task_discount}_rnl_{residual_norm_loss}_pcr_{per_concept_residual}_shared_{shared_per_concept_residual}_itl_{intermediate_task_concept_loss}_bp_{bottleneck_pooling}_cwl_{concept_loss_weight}"
  #   training_intervention_prob: [[0.25, 0.5, 0.75, 1]]
  #   emb_size: [32, 512] #[512, 1024]
  #   embedding_activation: null
  #   concept_loss_weight: [5] #, 0.1]
  #   normalize_embs: False
  #   task_concept_loss: 1
  #   intervention_task_discount: [1.01, 1.05]
  #   use_cosine_similarity: False
  #   early_stopping_best_model: False
  #   conditional_residual: [True] #False, True]
  #   residual_layers: [32, 16, 8]
  #   bottleneck_pooling: 'concat'
  #   per_concept_residual: [True]
  #   sigmoidal_residual: [False]
  #   residual_deviation: [0] #, 2]
  #   shared_per_concept_residual: [True]
  #   residual_norm_loss: [0.1] #, 0]
  #   intermediate_task_concept_loss: 0
  #   gradient_clip_val: 100

  #   residual_scale: 1
  #   blackbox_warmup_epochs: 0

  #   concept_epochs: 50
  #   fix_backbone_for_concept: False

  #   no_residual_epochs: 50
  #   fix_backbone_for_no_res: False
  #   fix_concept_embeddings_for_no_res: False

  #   e2e_epochs: 150
  #   fix_backbone_for_res: False
  #   fix_concept_embeddings_for_res: False
  #   fix_label_predictor_for_res: False
  #   use_ground_truth_mixing_for_res: False

  #   grid_variables:
  #     - concept_loss_weight
  #     - emb_size
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - conditional_residual
  #     - per_concept_residual
  #     - sigmoidal_residual
  #     - residual_deviation
  #     - shared_per_concept_residual
  #     - residual_norm_loss
  #   grid_search_mode: exhaustive







# WORKS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  # - architecture: 'CertificateConceptEmbeddingModel'
  #   run_name: "CertificateCEM_{selection_mode}_{pooling_mode}_t_{temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_gr_{global_temp_reg}_g_ood_{global_ood_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "CertificateCEM_{selection_mode}_{pooling_mode}_t_{temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_gr_{global_temp_reg}_g_ood_{global_ood_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1, 10]
  #   emb_size: [16]
  #   temperature: [10, 1]
  #   shared_emb_generator: True

  #   # OOD stuff
  #   certificate_loss_weight: [0]
  #   ood_dropout_prob: [0]
  #   global_ood_prob: [0.5, 0]
  #   pooling_mode: ['iss']
  #   selection_mode: ['max_class_confidence']
  #   hard_eval_selection: [null, 0.5, 0.05, 0.95]
  #   selection_sample: [False]
  #   calibration_epochs: [0, 30]
  #   mixed_probs: True
  #   contrastive_reg: 0
  #   calibrate_dynamic_logits: True
  #   calibrate_global_logits: True
  #   init_dyn_temps: 1.5
  #   init_global_temps: 0.5
  #   global_temp_reg: 0
  #   inference_dyn_prob: False
  #   learnable_temps: False
  #   finetune_with_val: True

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   gradient_clip_val: 100
  #   intervention_weight: 0.1

  #   grid_variables:
  #     - concept_loss_weight
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - certificate_loss_weight
  #     - ood_dropout_prob
  #     - selection_sample
  #     - pooling_mode
  #     - global_ood_prob
  #     - temperature
  #     - selection_mode
  #     - calibration_epochs
  #     - hard_eval_selection
  #   grid_search_mode: exhaustive


  # # WORKS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  # - architecture: 'CertificateConceptEmbeddingModel'
  #   run_name: "FinalMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "FinalMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1]
  #   emb_size: [16]
  #   temperature: [10, 1]
  #   max_temperature: 10
  #   shared_emb_generator: True

  #   # OOD stuff
  #   certificate_loss_weight: [0]
  #   ood_dropout_prob: [0]
  #   global_ood_prob: [0.5]
  #   entire_global_prob: [0]
  #   pooling_mode: ['iss']
  #   selection_mode: ['max_class_confidence']
  #   hard_eval_selection: [null, 0.5]
  #   selection_sample: [False]
  #   calibration_epochs: [0, 30]
  #   mixed_probs: True
  #   contrastive_reg: 0
  #   calibrate_dynamic_logits: True
  #   calibrate_global_logits: True
  #   finetune_with_val: [True, False]
  #   init_dyn_temps: 1.5
  #   init_global_temps: 0.5
  #   global_temp_reg: 0
  #   inference_dyn_prob: False
  #   learnable_temps: False
  #   positive_calibration: False
  #   class_wise_temperature: False
  #   separate_calibration: False
  #   freeze_global_components: [False]
  #   global_epochs: 0
  #   print_eval_only: True
  #   counter_limit: 0

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]

  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - certificate_loss_weight
  #     - ood_dropout_prob
  #     - selection_mode
  #     - hard_eval_selection
  #     - selection_sample
  #     - global_ood_prob
  #     - entire_global_prob
  #     - temperature
  #     - finetune_with_val
  #     - calibration_epochs
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - freeze_global_components
  #   grid_search_mode: exhaustive



  # # Make the original temperature symmetric but warm up the global model
  # - architecture: 'CertificateConceptEmbeddingModel'
  #   run_name: "FinalMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "FinalMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1]
  #   emb_size: [16]
  #   temperature: [10]
  #   max_temperature: 10
  #   shared_emb_generator: True

  #   # OOD stuff
  #   certificate_loss_weight: [0]
  #   ood_dropout_prob: [0]
  #   global_ood_prob: [0.5]
  #   entire_global_prob: [0]
  #   pooling_mode: ['iss']
  #   selection_mode: ['max_class_confidence']
  #   hard_eval_selection: [null, 0.5]
  #   selection_sample: [False]
  #   calibration_epochs: [0, 30]
  #   mixed_probs: True
  #   contrastive_reg: 0
  #   calibrate_dynamic_logits: True
  #   calibrate_global_logits: True
  #   finetune_with_val: [True]
  #   init_dyn_temps: 1
  #   init_global_temps: 1
  #   global_temp_reg: 0
  #   inference_dyn_prob: False
  #   learnable_temps: False
  #   positive_calibration: False
  #   class_wise_temperature: False
  #   separate_calibration: False
  #   freeze_global_components: [False]
  #   global_epochs: 30
  #   print_eval_only: True
  #   counter_limit: 0

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]

  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - certificate_loss_weight
  #     - ood_dropout_prob
  #     - selection_mode
  #     - hard_eval_selection
  #     - selection_sample
  #     - global_ood_prob
  #     - entire_global_prob
  #     - temperature
  #     - finetune_with_val
  #     - calibration_epochs
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - freeze_global_components
  #   grid_search_mode: exhaustive


  # # Make the original temperature symmetric but warm up the global model but with temperature 1
  # - architecture: 'CertificateConceptEmbeddingModel'
  #   run_name: "FinalMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "FinalMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1]
  #   emb_size: [16]
  #   temperature: [1]
  #   max_temperature: 1
  #   shared_emb_generator: True

  #   # OOD stuff
  #   certificate_loss_weight: [0]
  #   ood_dropout_prob: [0]
  #   global_ood_prob: [0.5]
  #   entire_global_prob: [0]
  #   pooling_mode: ['iss']
  #   selection_mode: ['max_class_confidence']
  #   hard_eval_selection: [null, 0.5]
  #   selection_sample: [False]
  #   calibration_epochs: [0, 30]
  #   mixed_probs: True
  #   contrastive_reg: 0
  #   calibrate_dynamic_logits: True
  #   calibrate_global_logits: True
  #   finetune_with_val: [True]
  #   init_dyn_temps: 1
  #   init_global_temps: 1
  #   global_temp_reg: 0
  #   inference_dyn_prob: False
  #   learnable_temps: False
  #   positive_calibration: False
  #   class_wise_temperature: False
  #   separate_calibration: False
  #   freeze_global_components: [False]
  #   global_epochs: 30
  #   print_eval_only: True
  #   counter_limit: 0

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]

  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - certificate_loss_weight
  #     - ood_dropout_prob
  #     - selection_mode
  #     - hard_eval_selection
  #     - selection_sample
  #     - global_ood_prob
  #     - entire_global_prob
  #     - temperature
  #     - finetune_with_val
  #     - calibration_epochs
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - freeze_global_components
  #   grid_search_mode: exhaustive



  # - architecture: 'CertificateConceptEmbeddingModel'
  #   run_name: "ConceptMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "ConceptMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_True_True_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1]
  #   emb_size: [16]
  #   temperature: [1]
  #   max_temperature: 1
  #   shared_emb_generator: True

  #   # OOD stuff
  #   certificate_loss_weight: [0]
  #   ood_dropout_prob: [0]
  #   global_ood_prob: [0.5]
  #   entire_global_prob: [0]
  #   pooling_mode: ['iss']
  #   selection_mode: ['max_concept_confidence']
  #   hard_eval_selection: [null] #, 0.5]
  #   selection_sample: [False]
  #   calibration_epochs: [0, 30]
  #   mixed_probs: True
  #   contrastive_reg: 0
  #   calibrate_dynamic_logits: True
  #   calibrate_global_logits: True
  #   finetune_with_val: [True]
  #   init_dyn_temps: 1
  #   init_global_temps: 1
  #   global_temp_reg: 0
  #   inference_dyn_prob: False
  #   learnable_temps: False
  #   positive_calibration: False
  #   class_wise_temperature: False
  #   separate_calibration: False
  #   freeze_global_components: [False]
  #   global_epochs: [5, 0]
  #   print_eval_only: True
  #   counter_limit: 0

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]

  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - certificate_loss_weight
  #     - ood_dropout_prob
  #     - selection_mode
  #     - hard_eval_selection
  #     - selection_sample
  #     - global_ood_prob
  #     - entire_global_prob
  #     - temperature
  #     - finetune_with_val
  #     - calibration_epochs
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - freeze_global_components
  #     - global_epochs
  #   grid_search_mode: exhaustive



  # # Calibrating only the contextual probabilities
  # - architecture: 'CertificateConceptEmbeddingModel'
  #   run_name: "ConceptMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "ConceptMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_True_True_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1]
  #   emb_size: [16]
  #   temperature: [1]
  #   max_temperature: 1
  #   shared_emb_generator: True

  #   # OOD stuff
  #   certificate_loss_weight: [0]
  #   ood_dropout_prob: [0]
  #   global_ood_prob: [0.5]
  #   entire_global_prob: [0]
  #   pooling_mode: ['iss']
  #   selection_mode: ['max_concept_confidence']
  #   hard_eval_selection: [null] #, 0.5]
  #   selection_sample: [False]
  #   calibration_epochs: [30]
  #   mixed_probs: True
  #   contrastive_reg: 0
  #   calibrate_dynamic_logits: True
  #   calibrate_global_logits: False  # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   finetune_with_val: [True]
  #   init_dyn_temps: 1
  #   init_global_temps: 1
  #   global_temp_reg: 0
  #   inference_dyn_prob: False
  #   learnable_temps: False
  #   positive_calibration: False
  #   class_wise_temperature: False
  #   separate_calibration: False
  #   freeze_global_components: [False]
  #   global_epochs: [5, 0]
  #   print_eval_only: True
  #   counter_limit: 0

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]

  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - certificate_loss_weight
  #     - ood_dropout_prob
  #     - selection_mode
  #     - hard_eval_selection
  #     - selection_sample
  #     - global_ood_prob
  #     - entire_global_prob
  #     - temperature
  #     - finetune_with_val
  #     - calibration_epochs
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - freeze_global_components
  #     - global_epochs
  #   grid_search_mode: exhaustive




  # # Calibrating only the global probabilities
  # - architecture: 'CertificateConceptEmbeddingModel'
  #   run_name: "ConceptMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "ConceptMixIntCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_True_True_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1]
  #   emb_size: [16]
  #   temperature: [1]
  #   max_temperature: 1
  #   shared_emb_generator: True

  #   # OOD stuff
  #   certificate_loss_weight: [0]
  #   ood_dropout_prob: [0]
  #   global_ood_prob: [0.5]
  #   entire_global_prob: [0]
  #   pooling_mode: ['iss']
  #   selection_mode: ['max_concept_confidence']
  #   hard_eval_selection: [null] #, 0.5]
  #   selection_sample: [False]
  #   calibration_epochs: [30]
  #   mixed_probs: True
  #   contrastive_reg: 0
  #   calibrate_dynamic_logits: False   # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   calibrate_global_logits: True
  #   finetune_with_val: [True]
  #   init_dyn_temps: 1
  #   init_global_temps: 1
  #   global_temp_reg: 0
  #   inference_dyn_prob: False
  #   learnable_temps: False
  #   positive_calibration: False
  #   class_wise_temperature: False
  #   separate_calibration: False
  #   freeze_global_components: [False]
  #   global_epochs: [5, 0]
  #   print_eval_only: True
  #   counter_limit: 0

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]

  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - certificate_loss_weight
  #     - ood_dropout_prob
  #     - selection_mode
  #     - hard_eval_selection
  #     - selection_sample
  #     - global_ood_prob
  #     - entire_global_prob
  #     - temperature
  #     - finetune_with_val
  #     - calibration_epochs
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - freeze_global_components
  #     - global_epochs
  #   grid_search_mode: exhaustive


  # # Mixing approach for vanilla CEMs
  # - architecture: 'CertificateConceptEmbeddingModel'
  #   run_name: "ConceptMixCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_{calibrate_global_logits}_{calibrate_dynamic_logits}_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_{hard_eval_selection}_ce_{calibration_epochs}_{finetune_with_val}_tip_{training_intervention_prob}_cwl_{concept_loss_weight}"
  #   load_path_name: "ConceptMixCEM_{global_epochs}_{max_epochs}_{freeze_global_components}_t_{temperature}_mt_{max_temperature}_pc_{positive_calibration}_cwc_{class_wise_temperature}_True_True_{init_dyn_temps}_{init_global_temps}_g_ood_{global_ood_prob}_{entire_global_prob}_emb_{emb_size}_idp_{inference_dyn_prob}_hs_None_ce_0_{finetune_with_val}_tip_{training_intervention_prob}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1]
  #   emb_size: [16]
  #   temperature: [1]
  #   max_temperature: 1
  #   shared_emb_generator: True

  #   # OOD stuff
  #   certificate_loss_weight: [0]
  #   ood_dropout_prob: [0]
  #   global_ood_prob: [0.5]
  #   entire_global_prob: [0]
  #   pooling_mode: ['iss']
  #   selection_mode: ['max_concept_confidence']
  #   hard_eval_selection: [null]
  #   selection_sample: [False]
  #   calibration_epochs: [0, 30]
  #   mixed_probs: True
  #   contrastive_reg: 0
  #   calibrate_dynamic_logits: True
  #   calibrate_global_logits: True
  #   finetune_with_val: [True]
  #   init_dyn_temps: 1
  #   init_global_temps: 1
  #   global_temp_reg: 0
  #   inference_dyn_prob: False
  #   learnable_temps: False
  #   positive_calibration: False
  #   class_wise_temperature: False
  #   separate_calibration: False
  #   freeze_global_components: [False]
  #   global_epochs: [5, 0]
  #   print_eval_only: True
  #   counter_limit: 0

  #   ###############################################################################################

  #   # IntCEM stuff cancelled so that it behaves as a CEM
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 1
  #   horizon_rate: 1
  #   intervention_weight: [0]

  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - emb_size
  #     - certificate_loss_weight
  #     - ood_dropout_prob
  #     - selection_mode
  #     - hard_eval_selection
  #     - selection_sample
  #     - global_ood_prob
  #     - entire_global_prob
  #     - temperature
  #     - finetune_with_val
  #     - calibration_epochs
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - freeze_global_components
  #     - global_epochs
  #   grid_search_mode: exhaustive

  # - architecture: 'MCIntCEM'
  #   run_name: "MCMixIntCEM_freeze_all_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "MCMixIntCEM_freeze_all_r_{all_intervened_loss_weight}_{pooling_mode}_50_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   global_model_path: "g_MCMixIntCEM_freeze_all_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1] #, 10]
  #   emb_size: [16]
  #   shared_emb_generator: True
  #   montecarlo_train_tries: [1]
  #   montecarlo_test_tries: [50]
  #   ood_dropout_prob: [0, 0.01] #, 0.1, 0.5]
  #   pooling_mode: ['concat']
  #   calibration_epochs: [0]
  #   finetune_with_val: [True]
  #   learnable_temps: False
  #   class_wise_temperature: False
  #   global_epochs: [0]
  #   global_mixed_probs_coeff: 0 # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   freeze_global_embeddings: False  # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   max_epochs: 150
  #   uncerainty_threshold_percentile: [50, 5, 95]
  #   mixed_probs_coeff: [1]
  #   inference_threshold: True
  #   all_intervened_loss_weight: [0, 0.01] #, 1]
  #   # lr_scheduler_patience: 15 # 10 CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   # patience: 10  # 5 CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   global_cem_only_pass: True # CHANGE FOR GLOBAL!

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]
  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - ood_dropout_prob
  #     - emb_size
  #     - finetune_with_val
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - global_epochs
  #     - montecarlo_train_tries
  #     - montecarlo_test_tries
  #     - mixed_probs_coeff
  #     - all_intervened_loss_weight
  #     - calibration_epochs
  #     - uncerainty_threshold_percentile
  #   grid_search_mode: exhaustive


  # - architecture: 'MCIntCEM'
  #   run_name: "MCMixIntCEM_freeze_all_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   load_path_name: "MCMixIntCEM_freeze_all_r_{all_intervened_loss_weight}_{pooling_mode}_1_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_0_g_ood_{ood_dropout_prob}_emb_{emb_size}_True_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"
  #   global_model_path: "g_MCMixIntCEM_freeze_all_r_{all_intervened_loss_weight}_{pooling_mode}_{uncerainty_threshold_percentile}_{global_epochs}_mpc_{mixed_probs_coeff}_tmc_{montecarlo_train_tries}_emc_{montecarlo_test_tries}_cwc_{class_wise_temperature}_ce_{calibration_epochs}_g_ood_{ood_dropout_prob}_emb_{emb_size}_{finetune_with_val}_tip_{training_intervention_prob}_itd_{intervention_task_discount}_iw_{intervention_weight}_cwl_{concept_loss_weight}"

  #   concept_loss_weight: [1] #, 10]
  #   emb_size: [16]
  #   shared_emb_generator: True
  #   montecarlo_train_tries: [1]
  #   montecarlo_test_tries: [50]
  #   ood_dropout_prob: [0.1] #, 0.1, 0.5]
  #   pooling_mode: ['concat']
  #   calibration_epochs: [30]
  #   finetune_with_val: [True]
  #   learnable_temps: False
  #   class_wise_temperature: False
  #   global_epochs: [0]
  #   global_mixed_probs_coeff: 0 # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   freeze_global_embeddings: False  # DIFFERENCE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   max_epochs: 150
  #   uncerainty_threshold_percentile: [1, 5, 10, 25, 50, 99]
  #   mixed_probs_coeff: [1]
  #   inference_threshold: True
  #   all_intervened_loss_weight: [0.01] #, 1]
  #   # lr_scheduler_patience: 15 # 10 CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   # patience: 10  # 5 CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #   global_cem_only_pass: True # CHANGE FOR GLOBAL!

  #   ###############################################################################################

  #   # IntCEM stuff
  #   embedding_activation: null
  #   training_intervention_prob: [0.25]
  #   intervention_task_discount: [1.1]
  #   use_concept_groups: True
  #   int_model_use_bn: True
  #   int_model_layers: [128, 128, 64, 64]
  #   max_horizon: 6
  #   horizon_rate: 1.005
  #   intervention_weight: [0.1]
  #   grid_variables:
  #     - training_intervention_prob
  #     - intervention_task_discount
  #     - ood_dropout_prob
  #     - emb_size
  #     - finetune_with_val
  #     - concept_loss_weight
  #     - pooling_mode
  #     - intervention_weight
  #     - global_epochs
  #     - montecarlo_train_tries
  #     - montecarlo_test_tries
  #     - mixed_probs_coeff
  #     - all_intervened_loss_weight
  #     - calibration_epochs
  #     - uncerainty_threshold_percentile
  #   grid_search_mode: exhaustive
